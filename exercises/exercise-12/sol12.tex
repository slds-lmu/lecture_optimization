\documentclass[a4paper]{article}
\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R




\usepackage[utf8]{inputenc}
%\usepackage[ngerman]{babel}
\usepackage{a4wide,paralist}
\usepackage{amsmath, amssymb, xfrac, amsthm}
\usepackage{dsfont}
%\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{multirow}
\usepackage{bytefield}
\usepackage{csquotes}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bm}


\input{../../style/common}

\tcbset{enhanced}

\DeclareRobustCommand{\mybox}[2][gray!20]{%
	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
	\fi
}

\DeclareRobustCommand{\myboxshow}[2][gray!20]{%
%	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
%	\fi
}


%exercise numbering
\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand\labelenumi{\theenumi}


\font \sfbold=cmssbx10

\setlength{\oddsidemargin}{0cm} \setlength{\textwidth}{16cm}


\sloppy
\parindent0em
\parskip0.5em
\topmargin-2.3 cm
\textheight25cm
\textwidth17.5cm
\oddsidemargin-0.8cm
\pagestyle{empty}

\newcommand{\kopf}[1]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Optimization in Machine Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/website_optimization/} \hfill WS 2024/2025}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfic}[1]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Optimization in Machine Learning \hfill Live Session #1\\
	 \url{https://slds-lmu.github.io/website_optimization/} \hfill WS 2024/2025}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfsl}[1]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Optimization in Machine Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/website_optimization/} \hfill WS 2024/2025}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newenvironment{allgemein}
	{\noindent}{\vspace{1cm}}

\newcounter{aufg}
\newenvironment{aufgabe}[1]
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}: #1}\\ \noindent}
	{\vspace{0.5cm}}

\newcounter{loes}
\newenvironment{loesung}[1]
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}: #1}\\ \noindent}
	{\bigskip}
	
\newenvironment{bonusaufgabe}
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}*\footnote{This
	is a bonus exercise.}:}\\ \noindent}
	{\vspace{0.5cm}}

\newenvironment{bonusloesung}
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}*:}\\\noindent}
	{\bigskip}

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\begin{document}
% !Rnw weave = knitr

\kopfsl{12}{Derivative Free Optimization and Evolutionary Strategies}

\loesung{Coordinate Descent I}{

\begin{equation*}
\begin{split}
\mathcal{R}_\text{emp}(\bm{\theta}) &= \frac{1}{2} \Vert\mathbf{X}\bm{\theta} - \mathbf{y}\Vert^2_2 + \frac{\lambda}{2} \Vert\bm{\theta}\Vert^2_2 = \frac{1}{2}\mathbf{y}^\top\mathbf{y}-\mathbf{y}^\top\mathbf{X}\bm{\theta} + \frac{1}{2}\bm{\theta}^\top\bm{\theta} + \frac{\lambda}{2}\bm{\theta}^\top\bm{\theta} \\
&=\frac{1}{2}\mathbf{y}^\top\mathbf{y}-\sum^d_{j=1}\mathbf{y}^\top\mathbf{x}_j\theta_j + \frac{1}{2}(1 + \lambda)\bm{\theta}^\top\bm{\theta} \\
\frac{\partial \mathcal{R}_\text{emp}}{\theta_j} &= (1 + \lambda)\theta_j -\mathbf{y}^\top\mathbf{x}_j \overset{!}{=} 0 \\
&\Rightarrow \theta_j^* = \frac{\mathbf{y}^\top\mathbf{x}_j}{1 + \lambda}
\end{split}
\end{equation*}

}

\loesung{Coordinate Descent II}{
\begin{enumerate}
  \item Update $x_{1}$ whle fixing $x_{2}$:
    We fix $x_{2} = c$ (constant).
    The function then states as $$g(x_{1}, c) = |x_{1} - c| + 0.1 (x_{1} + c).$$
    We want to choose $x_{1}$ to minimize this.
    Due to the absolute value, there are two cases.
    \begin{enumerate}
      \item Case 1: $x_{1} \ge c$:
        Then $|x_{1} - c| = x_{1} - c$.
        So $g(x_{1}, c) = (x_{1} - c) + 0.1 x_{1} + 0.1 c = 1.1 x_{1} - 0.9 c$.
        As a function of $x_{1}$ this is strictly increasing (derivative of $1.1 > 0$).
        Therefore, the minimizer given $x_{1} \ge c$ is at the left boundary, i.e., $x_{1} = c$.
      \item Case 2: $x_{1} < c$:
        Then $|x_{1} - c| = c - x_{1}$.
        So $g(x_{1}, c) = (c - x_{1}) + 0.1 x_{1} + 0.1 c = 1.1 c - 0.9 x_{1}$.
        As a function of $x_{1}$ this is strictly decreasing (derivative of $-0.9 < 0$).
        Thereore, the minimizer given $x_{1} < c$ is at the right boundary, i.e., $x_{1} = c$.
    \end{enumerate}
    In both cases, the best choice is $x_{1}^{*} = c$.
    Note that $x_{2}$ was fixed to be $c$, i.e., the function is minimized exactly when $x_{1} = x_{2}$.\\
    After updating $x_{1}$ while holding $x_{2}$ constant, we arrive at $(x_{1}^{[1]}, x_{2}^{[0]}) = (x_{2}^{[0]}, x_{2}^{[0]})$.\\

    Update $x_{2}$ while fixing $x_{1}$:
    We now fix $x_{1} = c$ (constant).
    The function then states as $$g(c, x_{2}) = |c - x_{2}| + 0.1 (c + x_{2}).$$
    Note that $g$ is symmetric in its arguments, therefore based on the first analysis, we conclude that again $x_{2}^{*} = c$.\\
    After updating $x_{2}$ while holding $x_{1}$ constant, we arrive at $(x_{1}^{[1]}, x_{2}^{[1]}) = (x_{1}^{[1]}, x_{1}^{[1]}) = (x_{2}^{[0]}, x_{2}^{[0]})$.\\

    We observe that coordinate updates will set the respective coordinate to the value of the other constant held coordinate value and once the algorithm arrives at $x_{1} = x_{2} = c$, neither coordinate update will move the point.

  \item Along the diagonal $x_{1} = x_{2} = t$, the function simplifies to $$g(t, t) = |t - t| + 0.1 (t + t) = 0.2 t.$$
    As $t \rightarrow - \infty$, $0.2 t \rightarrow - \infty$, hence the infimum of $g$ is $- \infty$.
    No finite $(x_{1}, x_{2})$ can achieve that inifimum, i.e., there is no global minimizer, but the values of $g$ can be made arbitrarly negative by letting $x_{1}$, $x_{2}$ be arbitrarly negative.

\end{enumerate}
}

\loesung{CMA-ES}{
Pick $\mu = 3$ parents with highest fitness values, i.e.,
$\text{Id} = 1,2,5$ which we denote with $\mathbf{x}_{1:\mu}$
and respective weights $w_i = \frac{f_i}{\sum^\mu_{i=1}f_i} \approx (0.432, 0.265, 0.303)$. \\
\begin{equation*}
\begin{split}
\mathbf{m}^{[1]} &= \mathbf{m}^{[0]} + 0.5 \sum^3_{i=1} w_i(\mathbf{x}_i - \mathbf{m}^{[0]}) \approx (1.140, 0.515)^\top \\
\mathbf{C}_\mu &= \frac{1}{3-1} \sum^3_{i=1}(\mathbf{x}_i - \mathbf{m}^{[0]}))(\mathbf{x}_i - \mathbf{m}^{[0]}))^\top  \\
& \approx \begin{pmatrix} 0.187 & -0.617 \\ -0.617 & 2.139 \end{pmatrix} \\
\mathbf{C}^{[1]} &= 0.9\cdot\mathbf{I}_d + 0.1\cdot\mathbf{C}_\mu \\
&\approx \begin{pmatrix} 0.919 & -0.062 \\ -0.062 & 1.114 \end{pmatrix}
\end{split}
\end{equation*}
}
\end{document}
