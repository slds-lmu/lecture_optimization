\documentclass[a4paper]{article}
\usepackage[]{graphicx}\usepackage[]{xcolor}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\newcommand{\SweaveOpts}[1]{}  % do not interfere with LaTeX
\newcommand{\SweaveInput}[1]{} % because they are not real TeX commands
\newcommand{\Sexpr}[1]{}       % will only be parsed by R




\usepackage[utf8]{inputenc}
%\usepackage[ngerman]{babel}
\usepackage{a4wide,paralist}
\usepackage{amsmath, amssymb, xfrac, amsthm}
\usepackage{dsfont}
%\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{framed}
\usepackage{multirow}
\usepackage{bytefield}
\usepackage{csquotes}
\usepackage[breakable, theorems, skins]{tcolorbox}
\usepackage{hyperref}
\usepackage{cancel}
\usepackage{bm}


\input{../../style/common}

\tcbset{enhanced}

\DeclareRobustCommand{\mybox}[2][gray!20]{%
	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
	\fi
}

\DeclareRobustCommand{\myboxshow}[2][gray!20]{%
%	\iffalse
	\begin{tcolorbox}[   %% Adjust the following parameters at will.
		breakable,
		left=0pt,
		right=0pt,
		top=0pt,
		bottom=0pt,
		colback=#1,
		colframe=#1,
		width=\dimexpr\linewidth\relax,
		enlarge left by=0mm,
		boxsep=5pt,
		arc=0pt,outer arc=0pt,
		]
		#2
	\end{tcolorbox}
%	\fi
}


%exercise numbering
\renewcommand{\theenumi}{(\alph{enumi})}
\renewcommand{\theenumii}{\roman{enumii}}
\renewcommand\labelenumi{\theenumi}


\font \sfbold=cmssbx10

\setlength{\oddsidemargin}{0cm} \setlength{\textwidth}{16cm}


\sloppy
\parindent0em
\parskip0.5em
\topmargin-2.3 cm
\textheight25cm
\textwidth17.5cm
\oddsidemargin-0.8cm
\pagestyle{empty}

\newcommand{\kopf}[1]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Optimization in Machine Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/website_optimization/} \hfill WS 2024/2025}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfic}[1]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Optimization in Machine Learning \hfill Live Session #1\\
	 \url{https://slds-lmu.github.io/website_optimization/} \hfill WS 2024/2025}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newcommand{\kopfsl}[1]{
\hrule
\vspace{.15cm}
\begin{minipage}{\textwidth}
%akwardly i had to put \" here to make it compile correctly
	{\sf\bf Optimization in Machine Learning \hfill Exercise sheet #1\\
	 \url{https://slds-lmu.github.io/website_optimization/} \hfill WS 2024/2025}
\end{minipage}
\vspace{.05cm}
\hrule
\vspace{1cm}}

\newenvironment{allgemein}
	{\noindent}{\vspace{1cm}}

\newcounter{aufg}
\newenvironment{aufgabe}[1]
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}: #1}\\ \noindent}
	{\vspace{0.5cm}}

\newcounter{loes}
\newenvironment{loesung}
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}:}\\\noindent}
	{\bigskip}
	
\newenvironment{bonusaufgabe}
	{\refstepcounter{aufg}\textbf{Exercise \arabic{aufg}*\footnote{This
	is a bonus exercise.}:}\\ \noindent}
	{\vspace{0.5cm}}

\newenvironment{bonusloesung}
	{\refstepcounter{loes}\textbf{Solution \arabic{loes}*:}\\\noindent}
	{\bigskip}

\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\begin{document}
% !Rnw weave = knitr

\kopfsl{1}{Mathematical Concepts 1}

\loesung{Gradient}{

%
\begin{enumerate}
	%
  \item The gradient $\nabla f(\mathbf{x}) = (2x_1 + x_2, x_2 + x_1)^\top$ is continuous $\Rightarrow f \in \mathcal{C}^1$.
	\item The direction of greatest increase is given by the gradient, i.e., $\nabla f(1,1) = (3,2)^\top.$
	\item Let $\mathbf{v} \in \R^2$ be a direction with fixed length $\Vert \mathbf{v}\Vert_2 = r > 0$. \newline The directional derivative $D_{\mathbf{v}}f(\mathbf{x}) = \nabla f(\mathbf{x})^\top \mathbf{v} = \left\Vert \nabla f(\mathbf{x}) \right\Vert_2 \left\Vert \mathbf{v} \right\Vert_2 \cos (\theta) = \left\Vert \nabla f(\mathbf{x}) \right\Vert_2 r \cos (\theta).$ This becomes minimal if $\theta = \pi$, i.e., if $\mathbf{v}$ points in the opposite direction of $\nabla f \Rightarrow \mathbf{v} = -\nabla f(\mathbf{x})$ if $r = \left\Vert \nabla f (\mathbf{x}) \right\Vert_2.$ 
	Here, the direction of greatest decrease is given by $-\nabla f(1,1) = (-3, -2)^\top$.
	\item $D_{\mathbf{v}}f(\mathbf{x}) = \nabla f(1, 1)^\top \mathbf{v} \overset{!}{=} 0 \Rightarrow (3,2) \cdot \mathbf{v} = 0 \iff \mathbf{v} = \alpha \cdot (-2, 3)^\top$ with $\alpha \in \R$ and $\alpha \neq 0.$
	
	\item When we differentiate both sides of the equation $f(\tilde{\mathbf{x}}(t)) = f(1,1)$  w.r.t. $t$ we arrive at $\frac{\partial f(\tilde{\mathbf{x}}(t))}{\partial t}  = 0$.
	Via the chain rule it follows that $\underbrace{\frac{\partial f}{\partial \tilde{\mathbf{x}}}}_{=\nabla f(\tilde{\mathbf{x}})^\top} \frac{\partial\tilde{\mathbf{x}}}{\partial t} = 0.$
	\item The gradient is orthogonal to the tangent line of the level curves.
	  
\end{enumerate}
}

\loesung{Convexity}{

\begin{enumerate}
\item Let $x,y \in \R$ and $t \in [0, 1]$ then it holds that
\begin{align*}
(f + g)(x + t(y-x)) &= f(x + t(y-x)) + g(x + t(y-x)) \\
&\leq f(x) + t(f(y)-f(x)) + g(x) + t(g(y)-g(x)) & \text{($f,g$ are convex)} \\
& = f(x) + g(x) + t(f(y) + g(y) -(f(x) + g(x))) \\
& = (f + g)(x) + t((f + g)(y) - (f + g)(x)).
\end{align*}
\item Let $x,y \in \R$ and $t \in [0, 1]$ then it holds that
\begin{align*}
(g \circ f)(x + t(y-x)) &= g(f(x + t(y-x))) \\
&\leq g(f(x) + t(f(y)-f(x))) &\text{($g$ is non-decreasing, $f$ is convex)}\\
&\leq g(f(x)) + t(g(f(y)) - g(f(x)))) & \text{($g$ is convex)} \\
&= (g\circ f)(x) + t((g \circ f)(y) - (g \circ f)(x)).
\end{align*}

\end{enumerate}
}

\loesung{Convexity}{

Consider the bivariate function $f: \R^2 \to \R, (x_1, x_2) \mapsto \exp(\pi \cdot x_1) - \sin(\pi \cdot x_2) + \pi \cdot x_1 \cdot x_2$

\begin{enumerate}
\item	$\nabla f(\mathbf{x}) = \pi \cdot (\exp(\pi x_1) +   x_2,  -\cos(\pi x_2) +  x_1)^\top$
	\item $\nabla^2 f(\mathbf{x}) = \pi \cdot \begin{pmatrix}\pi \exp(\pi x_1) &  1 \\
	 1 & \pi \sin(\pi x_2)  \end{pmatrix}$
	\item  $T_{1,\mathbf{a}}(\mathbf{x}) = f(\mathbf{a}) + \nabla f(\mathbf{a})^\top(\mathbf{x}-\mathbf{a}) = 1 + \pi \cdot (2, 1) \cdot (x_1, x_2 - 1)^\top = 1 - \pi + 2\pi x_1 + \pi x_2$
	\item \begin{align*}T_{2,\mathbf{a}}(\mathbf{x}) &= T_{1,\mathbf{a}}(\mathbf{x}) + \frac{1}{2} (\mathbf{x} - \mathbf{a})^\top\nabla^2 f(\mathbf{a})(\mathbf{x} - \mathbf{a}) \\
	&= T_{1,\mathbf{a}}(\mathbf{x}) + \frac{1}{2} \mathbf{x}^\top\nabla^2 f(\mathbf{a})\mathbf{x} + \mathbf{x}^\top\nabla^2 f(\mathbf{a})\mathbf{a} + \frac{1}{2}\mathbf{a}^\top\nabla^2 f(\mathbf{a})\mathbf{a}\end{align*}
	With $\nabla^2 f(\mathbf{a}) = \begin{pmatrix}\pi^2  &  \pi \\
	\pi & 0 \end{pmatrix}$ we get that 
	\begin{align*}
	T_{2,\mathbf{a}}(\mathbf{x}) = T_{1,\mathbf{a}}(\mathbf{x}) &+ 0.5\pi^2x_1^2 \\
	  &+ \pi x_1 x_2 - \pi x_1 \\
	  &+ 0.
	\end{align*}.
	\item $T_{2,\mathbf{a}}(\mathbf{x})$ is multivariate polynomial of degree 2 which means its Hessian is constant and we can directly see that $\mathbf{H}:= \nabla^2 T_{2,\mathbf{a}}(\mathbf{x}) = \nabla^2 f(\mathbf{a}).$
	For the eigenvalues of the Hessian it must hold that
	\begin{align*}
	&\det (\mathbf{H} - \lambda\mathbf{I}) &= 0 \\
	\iff& \det \begin{pmatrix}\pi^2 - \lambda & \pi \\ 
	\pi & -\lambda\end{pmatrix} &= 0 
 \\
		\iff& (\pi^2 - \lambda) \cdot (-\lambda) - \pi^2 &= 0 \\
		\iff& \lambda^2 -\pi^2\lambda - \pi^2 &= 0.
		\end{align*}
	From which it follows that
	$\lambda_{1,2} = \frac{\pi^2 \pm \sqrt{\pi^4 + 4\pi^2}}{2} \Rightarrow \lambda_1 \approx 10.785, \lambda_2 \approx -0.915.$ Since $\lambda_2 < 0\;T_{2,\mathbf{a}}$  is not convex.
\end{enumerate}
}
\end{document}
