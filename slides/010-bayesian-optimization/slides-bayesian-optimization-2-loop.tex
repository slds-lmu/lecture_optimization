\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-mbo}

\newcommand{\titlefigure}{figure_man/loop_2.png}
\newcommand{\learninggoals}{
\item Initial design
\item Surrogate modeling
\item Basic loop
}

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Bayesian Optimization:\\ Basic Loop and Surrogate Modeling}
\lecture{Optimization in Machine Learning}

\begin{vbframe}{Optimization via Surrogate Modeling}

\textbf{Starting point:}
\begin{itemize}
\item We do not know the objective function $f: \mathcal{S} \to \R$
\item But we can evaluate $f$ for a few different inputs $\xv \in \mathcal{S}$
\item For now we assume that those evaluations are noise-free

\item \textbf{Idea:}
  Use the data $\Dt = \{(\xvsi, \ysi)\}_{i = 1, \ldots t}$, $\ysi := f(\xvsi)$, to derive properties about the unknown function $f$
\end{itemize}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/loop_1.png}
\end{center}

\end{vbframe}

\begin{vbframe}{Initial Design}

\begin{itemize}
\item Should cover / explore input space sufficiently:
\begin{itemize}
  \item Random design
  \item Latin hypercube sampling
  \item Sobol sampling
\end{itemize}
\item Type of design usually has not the largest effect
%and unequal distances between points could even be beneficial
\item A more important choice is the \textbf{size} of the initial design
\begin{itemize}
  \item Should neither be too small (bad initial fit) nor too large (spending too much budget without doing \enquote{intelligent} optimization)
  \item Rule of thumb: $4d$
\end{itemize}
\end{itemize}
\end{vbframe}

\begin{frame}{Latin Hypercube Sampling}

\begin{columns}[T]
\begin{column}{0.5\textwidth}
\begin{itemize}
\item LHS partitions the search space into bins of equal probability
\item Goal is to attain a more even distribution of a sample points than random sampling
\item Allow at most one sample per bin
\end{itemize}
\end{column}

\begin{column}{0.5\textwidth}
\includegraphics[width = 0.75\textwidth]{slides/010-bayesian-optimization/figure_man/init_0.png}
\includegraphics[width = 0.75\textwidth]{slides/010-bayesian-optimization/figure_man/init_1.png}
\end{column}
\end{columns}

\end{frame}

\begin{frame}{Latin Hypercube Sampling}

Actual sampling of points, e.g., constructed via \textbf{Maximin}:\\
\begin{itemize}
\item The minimum distance between any two points in $\D$ is $2q = \min_{\xv \in \D, \xv' \in \D} \rho(\xv, \xv')$ ($\rho$ any metric, e.g., Euclidean distance)
\item $q$ is the packing radius - the radius of the largest ball that can be placed around every design point such that no two balls overlap
\item Goal: Find $\D$ that maximizes $2q$: $\max_{\D} \min_{\xv \in \D, \xv' \in \D} \rho(\xv, \xv')$
\item Ensures that the design points in $\D$ are as far apart from each other as possible
\end{itemize}

\end{frame}

\begin{frame}{Surrogate Modeling}

Our running example will be to minimize the following \enquote{black-box} function:

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/loop_0.png}
\end{center}

\end{frame}



\begin{vbframe}{Surrogate Modeling}
\begin{enumerate}
\item \textbf{Fit} a \textbf{regression model} $\fh: \Dt \rightarrow \R$ (blue) to extract maximum information from the design points (black) and learn properties of $f$
\vspace{+.05cm}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/loop_2.png}
\end{center}

\vspace{-.1cm}
As we can evaluate $f$ without noise, we fit an interpolating regression model

\framebreak 

\item Instead of the expensive $f$, we optimize the cheap 
 surrogate $\fh$ (blue) to \textbf{propose} a new point (red) for evaluation 
\vspace{+.05cm}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/loop_3.png}
\end{center}

%\vspace{-.1cm}
%In the context of BO, model is called \textbf{surrogate}, because it is a cheap approximation of $f$, which is used in its place.

\framebreak 

\item We finally evaluate the newly proposed point
\vspace{+.45cm}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/loop_4.png}
\end{center}

\end{enumerate}

\framebreak 

\begin{itemize}

\item After evaluation of the new point, we \textbf{adjust} the model on the expanded dataset via (slow) refitting or a (cheaper) online update
\vspace{+.45cm}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/loop_5.png}
\end{center}

\framebreak 

\item We again obtain a new candidate point (red) by optimizing the cheap surrogate model function (blue) ...
\vspace{+.45cm}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/loop_6.png}
\end{center}

\framebreak

\item ... and evaluate that candidate
\vspace{+.45cm}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/loop_7.png}
\end{center}

\framebreak

\item We repeat: (i) \textbf{fit} the model
\vspace{+.45cm}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/loop_8.png}
\end{center}

\framebreak

\item (ii) \textbf{propose} a new point
\vspace{+.45cm}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/loop_9.png}
\end{center}

\framebreak

\item (iii) \textbf{evaluate} that point
\vspace{+.45cm}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/loop_10.png}
\end{center}

\item We observe that the algorithm converged

\end{itemize}

\end{vbframe}

\begin{vbframe}{Basic Loop}

The basic loop of our sequential optimization procedure is:
  \begin{enumerate}
    \item Fit surrogate model $\fh$ on previous evaluations $\Dts$
    \item Optimize the surrogate model $\fh$ to obtain a new point $\xvsi[t+1] \coloneqq \argmax_{\xv \in \mathcal{S}} \fh(\xv)$
    \item Evaluate $\xvsi[t+1]$ and update data $\Dt[t+1] = \Dt \cup \{(\xvsi[t+1], f(\xvsi[t+1]))\}$
  \end{enumerate}

\framebreak

\begin{itemize}
  \item More sensible options exist for proposing new points
  \item We will define so-called acquisition functions based on the prediction of the surrogate model which we then optimize
  \item Optimizing the surrogate model directly corresponds to optimizing the mean prediction as acquisition function
  \item This results in high exploitation but low exploration
\end{itemize}

\end{vbframe}

\begin{vbframe}{Exploration vs. Exploitation}

The black line is the \enquote{unknown} black-box function the sequential optimization procedure has been applied to

\vspace*{0.2cm} 
We see: We ran into a local minimum. We did not \enquote{explore} the most crucial areas and \textbf{missed} the global minimum

\vspace{+.45cm}

\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/loop_11.png}
\end{center}


\end{vbframe} 

\endlecture

\end{document}
