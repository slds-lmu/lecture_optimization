\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{}{ 
Coordinate descent
}{
figure_man/Coordinate_descent.png
}{
\item Axes as descent direction 
\item CD on linear model and Lasso
\item Soft thresholding 
}

\begin{framei}{coordinate descent}
\item Assumption: objective function not differentiable
\item Idea: instead of gradient, use coordinate directions for descent
\item First: Select starting point $\bm{x}^{[0]} = (x^{[0]}_1, \ldots, x^{[0]}_d)$
\item Step $t$: Minimize $f$ along $x_i$ for each dimension $i$ for fixed $x^{[t]}_1, \ldots,x^{[t]}_{i-1}$ and $x^{[t-1]}_{i+1}, \ldots,x^{[t-1]}_d$:
\imageC[0.42][https://en.wikipedia.org/wiki/Coordinate_descent]{figure_man/Coordinate_descent.png}
\end{framei}

\begin{frame2}{coordinate descent}
\begin{itemize}
\item Minimum is determined with (exact/inexact) line search
\item Order of dimensions can be any permutation of $\left\{1,2,\ldots,d\right\}$
\item Convergence:
\begin{itemize}
\item $f$ convex differentiable
\item $f$ sum of convex differentiable and \textit{convex separable} function:
\begin{equation*}
f(\xv) = g(\xv) + \sum_{i=1}^d h_i(x_i),
\end{equation*}
\item where $g$ convex differentiable and $h_i$ convex
\end{itemize}
\end{itemize}
\end{frame2}

\begin{frame2}{coordinate descent}
No convergence in general for convex functions, counterexample:
\imageC[0.6][https://en.wikipedia.org/wiki/Coordinate_descent]{figure_man/Nonsmooth_coordinate_descent.png}
\end{frame2}

\begin{frame2}{Example: linear regression}
Minimize LM with L2 loss via CD
$$\min g(\thetav) = \min_{\thetav} \frac{1}{2}\sumin \left(y^{(i)} - \thetav^\top \xi\right)^2 = \min_{\thetav} \frac{1}{2}\|\yv - \Xmat \thetav\|^2 $$
where $\yv \in \R^n$, $\Xmat \in \R^{n \times d}$ with columns $\xv_1, \ldots, \xv_d \in \R^n$\\
\lz
Assume orthogonal design for intuition, i.e., $\Xmat^\top \Xmat = I_d$:
\begin{align*}
g(\thetav) &= \frac{1}{2}\yv^\top \yv + \frac{1}{2}\thetav^\top \thetav - \yv^\top \Xmat \thetav  \\
&\overset{(*)}{=} \frac{1}{2}\yv^\top \yv + \frac{1}{2}\thetav^\top \thetav - \yv^\top \sum_{k = 1}^d \xv_k \theta_k 
\end{align*}
$^{(*)}$ $\Xmat \thetav = \xv_1 \theta_1 + \xv_2 \theta_2 + \cdots + \xv_d \theta_d = \sum_{k = 1}^d \xv_k \theta_k$
\end{frame2}

\begin{framei}{example: linear regression}
\item Exact CD update in direction $j$:
$$\frac{\partial{g}(\thetav)}{\partial \theta_j} = \theta_j - \yv^\top \xv_j$$
\item By solving $\frac{\partial{g}(\thetav)}{\partial \theta_j} = 0$ we get $$\theta_j^\ast = \yv^\top \xv_j$$
\item Repeat this update for all $\theta_j$
\end{framei}

\begin{frame2}{Soft thresholding}
Minimize LM with L2 loss and L1 regularization via CD
$$\min_{\thetav} h(\thetav) = \min_{\thetav} \frac{1}{2}\|\yv - \Xmat \thetav\|^2 + \lambda \|\thetav\|_1$$
Note that $h(\thetav) = \frac{1}{2}\yv^\top \yv + \frac{1}{2}\thetav^\top \thetav  - \sum_{k = 1}^d (\yv^\top \xv_k \theta_k + \lambda|\theta_k|)$\\
Again assume $\Xmat^\top \Xmat = I_d$. Since $|\cdot|$ not differentiable, distinguish cases:
\lz
{\footnotesize
\begin{itemize}
\item \textbf{Case 1:} $\theta_j > 0$.
Then $|\theta_j| = \theta_j$ and 
$$0 = \frac{\partial{h}(\thetav)}{\partial \theta_j} = \theta_j - \yv^\top \xv_j + \lambda \qquad \Leftrightarrow \qquad \theta^\ast_{j, \text{Lasso}} = \theta^\ast_j - \lambda$$
\item \textbf{Case 2:} $\theta_j < 0$.
Then $|\theta_j| = - \theta_j$ and 
$$0 = \frac{\partial{h}(\thetav)}{\partial \theta_j} = \theta_j - \yv^\top \xv_j - \lambda \qquad \Leftrightarrow \qquad \theta^\ast_{j, \text{Lasso}} = \theta^\ast_j + \lambda$$
\item \textbf{Case 3:} $\theta_j = 0$
\end{itemize}}
\end{frame2}

\begin{frame2}{soft thresholding}
We can write the solution as: 
$$\theta^\ast_{j, \text{Lasso}} =
\begin{cases}
\theta^\ast_j - \lambda & \text{ if } \theta^\ast_j > \lambda \\
\theta^\ast_j + \lambda & \text{ if } \theta^\ast_j < - \lambda \\
0 &  \text{ if }  \theta^\ast_j \in [- \lambda, \lambda],
\end{cases}$$
This operation is called soft thresholding\\
\lz
Coefs for which solution to unregularized problem is smaller than threshold ($|\thetav^\ast_j| < \lambda$) are shrunk to zero
\lz
NB: Derivation of soft thresholding operator not trivial (subgradients)
\end{frame2}


\begin{frame2}{CD for statistics and ML}
Why is it being used?
\begin{itemizeL}
\item Easy to implement
\item Scalable: no storage/operations on large objects,
just current point \\
$\Rightarrow$ Good implementation can achieve state-of-the-art performance
\item Applicable for non-differentiable (but convex separable) objectives
\end{itemizeL}
\lz
Examples:
\begin{itemizeL}
\item Lasso regression, Lasso GLM, graphical Lasso
\item Support Vector Machines
\item Regression with non-convex penalties
\end{itemizeL}
\end{frame2}

\endlecture
\end{document}
