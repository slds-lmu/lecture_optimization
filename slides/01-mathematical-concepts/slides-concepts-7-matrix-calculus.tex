\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}
{figure_man/1920px-Greek_lc_delta.svg.png} 
\newcommand{\learninggoals}{
\item Rules of matrix calculus 
\item Connection of gradient, Jacobian and Hessian
}

\title{Optimization in Machine Learning}
\date{}

\begin{document}

\lecturechapter{Mathematical Concepts: \\Matrix Calculus}
\lecture{Optimization in Machine Learning}
\sloppy

% ------------------------------------------------------------------------------


\begin{vbframe}{Scope}
\begin{itemize}
    \setlength{\itemsep}{0.5\baselineskip}
    \item $\mathcal{X}$/$\mathcal{Y}$ denote space of \textbf{independent}/\textbf{dependent} variables
    \item Identify dependent variable with a \textbf{function} $y: \mathcal{X} \to \mathcal{Y}, x\mapsto y(x)$
    \item Assume $y$ sufficiently smooth
    \item In matrix calculus, $x$ and $y$ can be \textbf{scalars}, \textbf{vectors}, or \textbf{matrices}:
        \vspace{0.5\baselineskip}
        \begin{table}
            \centering
            \begin{tabular}{c||c|c|c}
                 Type & scalar $x$ & vector $\xv$ & matrix $\mathbf{X}$ \\ \hline\hline
                 scalar $y$ & $\partial y / \partial x$ & $\partial y / \partial\xv$ & $\partial y / \partial\mathbf{X}$ \\ \hline
                 vector $\yv$ & $\partial\yv / \partial x$ & $\partial\yv / \partial\xv$ & -- \\ \hline
                 matrix $\mathbf{Y}$ & $\partial\mathbf{Y} / \partial x$ & -- & --
            \end{tabular}
        \end{table}
    \item We denote vectors/matrices in \textbf{bold} lowercase/uppercase letters
\end{itemize}
\end{vbframe}

\begin{vbframe}{Numerator layout}

\begin{itemize}
    \item \textbf{Matrix calculus:} collect derivative of each component of dependent variable w.r.t. each component of independent variable
    \item We use so-called \textbf{numerator layout} convention:
        \begin{align*}
            \frac{\partial y}{\partial\xv} &= \left(\frac{\partial y}{\partial x_1}, \cdots, \frac{\partial y}{\partial x_d}\right) = \nabla y^T \in \R^{1\times d} \\
            \frac{\partial\yv}{\partial x} &= \left(\frac{\partial y_1}{\partial x}, \cdots, \frac{\partial y_m}{\partial x}\right)^T \in \R^{m} \\
            \frac{\partial\yv}{\partial\xv} =
            \begin{pmatrix}
                \frac{\partial y_1}{\partial\xv} \\
                \vdots \\
                \frac{\partial y_m}{\partial\xv} \\
            \end{pmatrix} &=
            \left(\frac{\partial\yv}{\partial x_1} \cdots \frac{\partial\yv}{\partial x_d}\right) =
            \begin{pmatrix}
                \frac{\partial y_1}{\partial x_1} & \cdots & \frac{\partial y_1}{\partial x_d} \\
                \vdots & \ddots & \vdots \\ 
                \frac{\partial y_m}{\partial x_1} & \cdots & \frac{\partial y_m}{\partial x_d}
            \end{pmatrix} = J_\yv \in \R^{m \times d}
        \end{align*}
\end{itemize}

\end{vbframe}

\begin{vbframe}{Scalar-by-vector}

Let $\xv \in \R^d$, $y,z : \R^d \to \R$ and $\Amat$ be a matrix.

\medskip

\begin{itemize}
    \item If $y$ is a \textbf{constant} function: $\frac{\partial y}{\partial\xv} = \mathbf{0}^T \in \R^{1\times d}$
    \item \textbf{Linearity}: $\frac{\partial (a \cdot y + z)}{\partial\xv} = a\frac{\partial y}{\partial\xv} + \frac{\partial z}{\partial\xv}$ \quad ($a$ constant)
    \item \textbf{Product} rule: $\frac{\partial (y \cdot z)}{\partial\xv} = y \frac{\partial z}{\partial\xv} + \frac{\partial y}{\partial \xv}z$
    \item \textbf{Chain} rule: $\frac{\partial g(y)}{\partial\xv} = \frac{\partial g(y)}{\partial y}\frac{\partial y}{\partial \xv}$ \quad ($g$ scalar-valued function)
    \item \textbf{Second} derivative: $\frac{\partial^2 y}{\partial\xv \partial\xv^T} = \nabla^2 y^T$ ($=\nabla^2 y$ if~$y\in\mathcal{C}^2$) (Hessian)
    \item $\frac{\partial(\xv^T\Amat\xv)}{\partial\xv} =  \xv^T(\Amat+\Amat^T)$
    \item $\frac{\partial(\yv^T \Amat \mathbf{z})}{\partial\xv} = \yv^T \Amat \frac{\partial\mathbf{z}}{\partial\xv} + \mathbf{z}^T \Amat^T \frac{\partial\yv}{\partial\xv}$ \quad ($\yv$, $\mathbf{z}$ vector-valued functions of~$\xv$)
\end{itemize}

\end{vbframe}

\begin{vbframe}{Vector-by-scalar}

Let $x \in \R$ and $\yv, \mathbf{z} : \R \to \R^m$.

\medskip

\begin{itemize}
    \item If $\mathbf{y}$ is a \textbf{constant} function: $\frac{\partial \mathbf{y}}{\partial x} = \mathbf{0} \in \R^{m}$
    \item \textbf{Linearity}: $\frac{\partial (a \cdot \yv + \mathbf{z})}{\partial x} = a \frac{\partial\yv}{\partial x} + \frac{\partial \mathbf{z}}{\partial x}$ \quad ($a$ constant)
    \item \textbf{Chain} rule: $\frac{\partial\mathbf{g}(\yv)}{\partial x} = \frac{\partial\mathbf{g}(\yv)}{\partial\yv}\frac{\partial \yv}{\partial x}$ \quad ($\mathbf{g}$ vector-valued function)
    \item $\frac{\partial(\Amat\yv)}{\partial x} = \Amat\frac{\partial\yv}{\partial x}$  \quad ($\mathbf{A}$ matrix)
\end{itemize}
\end{vbframe}

\begin{vbframe}{Vector-by-vector}

Let $\xv \in \R^d$ and $\yv, \mathbf{z} : \R^d \to \R^m$.

\medskip

\begin{itemize}
    \item If $\yv$ is a \textbf{constant} function: $\frac{\partial\yv}{\partial\xv} = \mathbf{0} \in \R^{m \times d}$
    \item $\frac{\partial \mathbf{x}}{\partial \mathbf{x}} = \mathbf{I} \in \R^{d \times d}$
    \item \textbf{Linearity}: $\frac{\partial (a \cdot \yv + \mathbf{z})}{\partial\xv} = a \frac{\partial\yv}{\partial\xv} + \frac{\partial\mathbf{z}}{\partial\xv}$ \quad ($a$ constant)
    \item \textbf{Chain} rule: $\frac{\partial\mathbf{g}(\yv)}{\partial\xv} = \frac{\partial\mathbf{g}(\yv)}{\partial\yv} \frac{\partial\yv}{\partial\xv}$ \quad ($\mathbf{g}$ vector-valued function)
    \item $\frac{\partial(\Amat\xv)}{\partial\xv} = \Amat$, $\frac{\partial(\xv^T\mathbf{B})}{\partial\xv} = \mathbf{B}^T$ \quad ($\Amat,\mathbf{B}$ matrices)
\end{itemize}

\end{vbframe}

\begin{vbframe}{Example}

Consider $f : \R^2 \to \R$ with
\begin{equation*}
    f(\xv) = \exp\left(-(\xv - \mathbf{c})^T \Amat (\xv - \mathbf{c})\right),
\end{equation*}

\vspace{-0.5\baselineskip}

where $\mathbf{c} = (1, 1)^T$ and $\Amat = \begin{pmatrix}1 & 1/2 \\ 1/2 & 1\end{pmatrix}$.

\medskip

Compute $\nabla f(\xv)$ at $\xv^\ast = \mathbf{0}$:

\medskip

\begin{enumerate}
    \item Write $f(\xv) = \exp(g(\mathbf{u}(\xv)))$ with $g(\mathbf{u}) = -\mathbf{u}^T\Amat\mathbf{u}$ and $\mathbf{u}(\xv) = \xv-\mathbf{c}$
    \item \textbf{Chain} rule: $\frac{\partial f(\xv)}{\partial\xv} = \exp(g(\mathbf{u}(\xv))) \frac{\partial g(\mathbf{u})}{\partial\mathbf{u}} \frac{\partial\mathbf{u}(\xv)}{\partial\xv}$
    \item $\mathbf{u}^\ast := \mathbf{u}(\xv^\ast) = (-1,-1)^T$, $g(\mathbf{u^\ast}) = -3$
    \item $\frac{\partial g(\mathbf{u})}{\partial\mathbf{u}} = -2\mathbf{u}^T\Amat$, $\frac{\partial g(\mathbf{u}^\ast)}{\partial\mathbf{u}} = (3, 3)$
    \item \textbf{Linearity}: $\frac{\partial\mathbf{u}(\xv)}{\partial\xv} = \frac{\partial(\xv - \mathbf{c})}{\partial\xv} = \frac{\partial\xv}{\partial\xv} - \frac{\partial\mathbf{c}}{\partial\xv} = \mathbf{I}_2$
    \item $\nabla f(\xv^\ast) = \frac{\partial f(\xv^\ast)}{\partial{\xv}}^T = (\exp(-3) \cdot (3, 3) \cdot \mathbf{I}_2)^T = \exp(-3) \begin{pmatrix}3 \\ 3\end{pmatrix}$
\end{enumerate}

\end{vbframe}

\endlecture

\end{document}