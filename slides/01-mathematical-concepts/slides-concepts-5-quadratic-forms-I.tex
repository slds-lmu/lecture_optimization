\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/quadratic_functions_2D_example_1_1.png}
\newcommand{\learninggoals}{
\item Definition of quadratic forms
\item Gradient, Hessian
\item Optima
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Mathematical Concepts: \\Quadratic forms I}
\lecture{Optimization in Machine Learning}
\sloppy

\begin{vbframe}{Univariate Quadratic Functions}

Consider a \textbf{quadratic function} $q: \R \to \R$

$$
q(x) = a \cdot x^2 + b \cdot x + c, \qquad a \ne 0.
$$

% We will set $b, c = 0$ to keep things simple. 

% Include a pic of a quadratic form 
\begin{figure}
    \includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/quadratic_functions_1D.png} \\
    \caption*{A quadratic function $q_1(x) = x^2$ (\textbf{left}) and $q_2(x) = - x^2$ (\textbf{right}).}
\end{figure}

\framebreak 

Basic properties: 

\begin{itemize}
    \item \textbf{Slope} of tangent at point $(x, q(x))$ is given by $q'(x) = 2 \cdot a \cdot x + b$
        \begin{figure}
            \includegraphics[height=0.2\textwidth, keepaspectratio]{figure_man/quadratic_functions_1D_derivative.png} \\
        \end{figure}
    \item \textbf{Curvature} of $q$ is given by $q''(x) = 2\cdot a$. 
        \begin{figure}
            \includegraphics[height=0.2\textwidth, keepaspectratio]{figure_man/quadratic_functions_1D_curvature.png} \\
            \caption*{\footnotesize $q_1 = x^2$ (orange), $q_2 = 2 x^2$ (green), $q_3 (x) = - x^2$ (blue), $q_4 = - 3 x^2$ (magenta)}
        \end{figure}

    \framebreak
    
    \item \textbf{Convexity/Concavity}:
        \begin{itemize}
            \setlength{\itemindent}{-0.5cm}
            \item $a > 0$: $q$~convex, bounded from below, unique global \textbf{minimum}
            \item $a < 0$: $q$~concave, bounded from above, unique global \textbf{maximum}
        \end{itemize}
    
    \item \textbf{Optimum}~$x^\ast$:
        \vspace{-0.5\baselineskip}
        \begin{equation*}
            q'(x^\ast) = 0 \quad\Leftrightarrow \quad 2ax^\ast + b = 0 \quad \Leftrightarrow \quad x^\ast = \frac{-b}{2a}  	
        \end{equation*}
\end{itemize}

\vspace{-0.5\baselineskip}

\begin{figure}
    \centering
    \includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/quadratic_functions_1D.png}
    \caption*{\textbf{Left:} $q_1(x) = x^2$ (convex).
        \textbf{Right:} $q_2(x) = - x^2$ (concave).}
\end{figure}

\end{vbframe}
  
\begin{vbframe}{Multivariate Quadratic Functions}

A quadratic function $q: \R^d \to \R$ has the following form: 

\begin{equation*}
    q(\xv) = \xv^T \Amat \xv + \bm{b}^T \xv + c
\end{equation*}

with $\bm{A} \in \R^{d \times d}$ full-rank matrix, $\bm{b} \in \R^d$, $c \in \R$. % To also keep things simple here, we will set $\bm{b}$ and $c$ to zero.

\vspace{0.5\baselineskip}

\begin{figure}
    \includegraphics[height=0.4\textwidth,width=0.4\textwidth]{figure_man/quadratic_functions_2D_example_1_1.png} ~~ \includegraphics[height=0.4\textwidth,width=0.4\textwidth]{figure_man/quadratic_functions_2D_example_1_2.png} \\
\end{figure}

\framebreak

W.l.o.g., assume $\Amat$ \textbf{symmetric}, i.e., $\Amat^T = \Amat$.

\vspace{0.5\baselineskip}

If~$\Amat$ not symmetric, there is always a symmetric matrix $\tilde \Amat$ s.t. 

\vspace*{-0.5\baselineskip}

\begin{equation*}
    q(\xv) = \xv^T \Amat \xv = \xv^T \tilde \Amat \xv = \tilde q(\xv).
\end{equation*}

\textbf{Justification}: We write

\begin{equation*}
    q(\xv) = \xv^T \Amat \xv = \frac{1}{2} \xv^T \underbrace{(\Amat + \Amat^T)}_{\tilde \Amat_1} \xv + \frac{1}{2} \xv^T \underbrace{(\Amat - \Amat^T)}_{\tilde \Amat_2} \xv
\end{equation*}

with $\tilde \Amat_1$ symmetric, $\tilde \Amat_2$ anti-symmetric (i.e., $\tilde \Amat_2^T = - \tilde \Amat_2$). Since $\xv^T \Amat^T \xv$ is a scalar, it is equal to its transpose: 

\vspace*{-1.25\baselineskip}

\begin{align*}
    \xv^T (\Amat - \Amat^T) \xv &=  \xv^T \Amat \xv - \xv^T \Amat^T \xv =  \xv^T \Amat \xv - \left(\xv^T \Amat^T \xv\right)^T \\
    &= \xv^T \Amat \xv - \xv^T \Amat \xv  = 0.
\end{align*}

Therefore, $q(\xv) = \tilde q(\xv)$ with $\tilde q(\xv) = \xv^T \tilde \Amat \xv$ with $\tilde \Amat = \tilde \Amat_1/2$. 

\end{vbframe}

\begin{vbframe}{Gradient and Hessian}
    
\begin{itemize}
    \item The \textbf{gradient} of~$q$ is
    
        \vspace{-0.5\baselineskip}
        
        \begin{equation*}
            \nabla q(\xv) = \left(\Amat^T + \Amat\right) \xv + \bm{b} = 2 \Amat \xv + \bm{b} \in \R^d
        \end{equation*}

        \vspace{-0.25\baselineskip}
    
        Derivative in direction $\bm{v}\in \R^d$ is (by chain rule)
    
        \begin{equation*}
            \frac{\text{d}q(\xv + h \cdot \bm{v})}{\text{d}h}~\bigg\rvert_{h = 0} = \nabla q(\xv + h \bm{v})^T \bm{v}~\bigg\rvert_{h = 0} = \nabla q(\xv)^T \bm{v}.
        \end{equation*}

    \vspace{0.5\baselineskip}

    \item The \textbf{Hessian} of~$q$ is
        
        \vspace{-0.5\baselineskip}
        
        \begin{equation*}
            \nabla^2 q(\xv) = \left(\bm{A}^T + \bm{A}\right) = 2 \Amat =: \mathbf{H} \in \R^{d \times d}
        \end{equation*}    
        
        Curvature in direction of $\bm{v}\in \R^d$ is (by chain rule)

        \vspace{-0.5\baselineskip}
        
        \begin{equation*}
            \frac{\text{d}^2 q(\xv + h \cdot \bm{v})}{\text{d} h^2}~\bigg\rvert_{h = 0}
            % = \frac{\text{d}\left[\nabla q(\xv + h \bm{v})^T\bm{v}\right]}{\text{d} h}~\bigg\rvert_{h = 0}
            = \bm{v}^T \nabla^2 q(\xv + h \bm{v}) \bm{v}~\bigg\rvert_{h = 0} = \bm{v}^T \mathbf{H} \bm{v}.
        \end{equation*}
\end{itemize}

\end{vbframe}


\begin{vbframe}{Optimum}

Since $\Amat$ has full rank, there exists a \textit{unique} stationary point~$\xv^\ast$ (minimum, maximum, or saddle point):

\vspace{-\baselineskip}

\begin{align*}
    \nabla q(\xv^\ast) &= 0 \\
    2 \Amat \xv^\ast + \bm{b} &= 0 \\
    \xv^\ast &= -\frac{1}{2} \Amat^{-1} \bm{b}.
\end{align*}  

\vspace{-0.5\baselineskip}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figure_man/minmaxsaddle.png}
    \caption*{
        \footnotesize
        \textbf{Left:} $\Amat$ positive definite.
        \textbf{Middle:} $\Amat$ negative definite.
        \textbf{Right:} $\Amat$ indefinite.}
\end{figure}

\end{vbframe}

\begin{vbframe}{Optima: Rank-deficient case}

\footnotesize

\textbf{Example:} Assume~$\Amat$ is \textbf{not} full rank but has a zero eigenvalue with eigenvector~$\bm{v}_0$.

\begin{itemize}
    \item Recall: $\bm{v}_0$ spans null space of~$\Amat$, i.e., $\Amat(\alpha\bm{v}_0) = 0$ for each~$\alpha\in\R$
    \item $\implies$ $\Amat(\xv + \alpha\bm{v}_0) = \Amat\xv$
    \item Since $\nabla q(\xv) = 2\Amat\xv + \bm{b}$:
        
        \vspace{-0.5\baselineskip}
        
        \begin{equation*}
            \nabla q(\xv+\alpha\bm{v}_0) = 2\Amat(\xv+\alpha\bm{v}_0)+\bm{b} = 2\Amat\xv+\bm{b} = \nabla q(\xv)
        \end{equation*}
    \item $\implies$ $q$ has infinitely many stationary points along line $\xv^\ast+\alpha\bm{v}_0$
    \item Since $\mathbf{H} = 2\Amat$, kind of stationary point not changing along~$\bm{v}_0$
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width=0.35\textwidth]{figure_man/convex-example.png}
\end{figure}

\end{vbframe}

\endlecture

\end{document}