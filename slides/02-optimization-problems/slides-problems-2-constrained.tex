\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Optimization Problems
  }{% Lecture title  
  Constrained problems
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/classes_optimization_problems.png
  }{
    \item Definition
    \item LP, QP, CP
    \item Ridge and Lasso
    \item Soft-margin SVM 
}


\begin{frame2}{Constrained Optimization Problem}
$$
\min_{\xv \in \mathcal{S}} \fx, \text{ with } f: \; \mathcal{S} \to \R.
$$
\begin{itemize}
\item \textbf{Constrained}, if domain $\mathcal{S}$ is restricted: $\mathcal{S} \textcolor{blue}{\subsetneq} \R^d.$
\item \textbf{Convex} if $f$ convex function and $\mathcal{S}$ convex set
\item Typically $\mathcal{S}$ is defined via functions called \textbf{constraints}
$$
\mathcal{S}:= \{\xv \in \R^d ~|~ g_i(\xv) \le 0, h_j(\xv) = 0 ~\forall~ i, j\}, \text{ where }
$$
\begin{itemize}
\item $g_i: \R^d \to \R, i = 1, ..., k$ are called inequality constraints,
\item $h_j: \R^d \to \R, j = 1, ..., l$ are called equality constraints.
\end{itemize}
\end{itemize}
\spacer
Equivalent formulation:
$$
\begin{aligned}
\min \quad & f(\mathbf{x})  \\
ext{such that} \quad & g_i(\mathbf{x}) \le 0 && \text{for } i=1,\ldots,k \\
& h_j(\mathbf{x}) = 0 && \text{for } j=1,\ldots,l.
\end{aligned}
$$
\end{frame2}


\begin{framei}{Linear program (LP)}
\item $f$ linear s.t. linear constraints. Standard form:
$$
\begin{aligned}
\min_{\xv \in \R^d} \quad & \bm{c}^\top \xv \\
ext{s.t. } \quad & \bm{A}\xv \ge \bm{b} \\
& \xv \ge 0
\end{aligned}
$$
for $\bm{c}\in \R^d, \bm{A} \in \R^{k \times d}$ and $\bm{b} \in \R^k$.
\splitV[0.5]{
\imageC[0.7]{figure_man/simplex_iter1.png}
}{
\imageC[0.7][https://de.wikipedia.org/wiki/Simplex-Verfahren]{figure_man/simplex.png}
}
\begin{center}\begin{footnotesize}
Visualization of constraints of 2D and 3D linear program.
\end{footnotesize}\end{center}
\end{framei}


\begin{framei}{Quadratic program (QP)}
\item $f$ quadratic form s.t. linear constraints. Standard form:
$$
\begin{aligned}
\min_{\xv \in \R^d} \quad & \frac{1}{2} \xv^\top \bm{A} \xv + \bm{b}^\top \xv + c \\
ext{s.t. } \quad & \bm{E}\xv \le \bm{f} \\
& \bm{G}\xv = \bm{h}
\end{aligned}
$$
$\bm{A} \in \R^{d \times d}, \bm{b} \in \R^d, c \in \R$, $\bm{E} \in \R^{k \times d}, \bm{f} \in \R^k$, $\bm{G} \in \R^{l \times d}$, $\bm{h} \in \R^l$.
\vfill
\imageC[0.5]{figure_man/quadratic-programm.jpeg}
\begin{footnotesize}
Visualization of quadratic objective (dashed) over linear constraints (grey). Source: Ma, Signal Processing Optimization Techniques, 2015. % I can't find this paper, so I didn't convert this to a link. https://scaron.info/blog/quadratic-programming-in-python.html seemingly uses the same figure
\end{footnotesize}
\end{framei}


\begin{framei}{Convex program (CP)}
\item $f$ convex, convex inequality constraints, linear equality constraints. Standard form:
$$
\begin{aligned}
\min_{\xv \in \R^d} \quad & \fx \\
ext{s.t. } \quad & g_i(\xv) \le 0, i = 1, ..., k \\
& \bm{A}\xv = \bm{b}
\end{aligned}
$$
for $\bm{A} \in \R^{l \times d}$ and $\bm{b} \in \R^l$.
\vfill
\imageC[0.8][https://de.mathworks.com/discovery/convex-optimization.html]{figure_man/cp_example.jpg}
\begin{center}\begin{footnotesize}
Convex program (left) vs. nonconvex program (right).
\end{footnotesize}\end{center}
\end{framei}


\begin{frame2}{Further types}
\splitVCC[0.58]{
\imageC[0.8]{figure_man/classes_optimization_problems.png}
}{
Quadratically constrained linear program (QCLP) and quadratically constrained quadratic program (QCQP).
}
\end{frame2}


\begin{frame2}{Example 1: Unit circle}
$$
\begin{aligned}
\min \quad & f(x_1, x_2) = x_1 + x_2 \\
ext{s.t. } \quad & h(x_1,x_2) = x_1^2 + x_2^2 - 1 = 0
\end{aligned}
$$
\vfill
\imageC[0.3]{figure_man/unit_circle.png}
$f, h$ smooth. Problem $\textbf{not convex}$ ($\mathcal{S}$ is not a convex set).
\spacer
\begin{footnotesize}
\textbf{Note: } If the constraint is replaced by $g(x_1, x_2) = x_1^2 + x_2^2 - 1 \le 0$, the problem is a convex program, even a quadratically constrained linear program (QCLP).
\end{footnotesize}
\end{frame2}


\begin{frame2}{Example 2: Maximum likelihood}
\textbf{Experiment}: Draw $m$ balls from a bag with balls of $k$ different colors.
Color $j$ has a probability of $p_j$ of being drawn.
\spacer
The probability to realize the outcome $\xv = (x_1, ..., x_k)$, $x_j$ being the number of balls drawn in color $j$, is:
$$
f(\xv, m, \bm{p}) = \begin{cases} \frac{m!}{x_1! \cdots x_k!} \cdot p_1^{x_1} \cdots p_k^{x_k} & \text{ if } \sum_{i = 1}^k x_i = m \\ 0 & \text{ otherwise}\end{cases}
$$
The parameters $p_j$ are subject to the following constraints:
$$
\begin{aligned}
0 \le p_j \le 1 && \text{ for all } i \\
\sum_{j = 1}^m p_j = 1. &&
\end{aligned}
$$
\end{frame2}


\begin{frame2}{Example 2: Maximum likelihood}
For a fixed $m$ and a sample $\D = \left(\xi[1], ..., \xi[n]\right)$, where $\sum_{j = 1}^k \xi[i]_j = m$ for all $i = 1, ..., n$, the negative log-likelihood is:
$$
\begin{aligned}
- \ell(\bm{p}) &= - \log \left(\prod_{i = 1}^n  \frac{m!}{\xi_1! \cdots \xi_k!} \cdot p_1^{\xi_1} \cdots p_k^{\xi_k}    \right) \\
&= \sumin \left[- \log(m!) + \sum_{j = 1}^k \log(\xi_j!) - \sum_{j = 1}^k \xi_j \log(p_j)\right] \\
&\propto - \sumin \sum_{j = 1}^k \xi_j \log(p_j)
\end{aligned}
$$
$f, g, h$ are smooth.\\\textbf{Convex program}: convex$^{(*)}$ objective + box/linear constraints).
\vfill
\begin{footnotesize}
${(*)}$: $\log$ is concave, $- \log $ is convex, and the sum of convex functions is convex.
\end{footnotesize}
\end{frame2}


% \begin{vbframe}{Example 1.1: Maximum Likelihood Estimation: Poisson Distribution}
% $\D = \left(x^{(1)}, ..., x^{(n)}\right)$ is sampled i.i.d. from density $f(x ~|~ \thetav)$. We want to find $\lambda$ which makes the observed data most likely.
% \begin{center}
% 	\includegraphics[width=0.4\textwidth, height=0.4\textwidth]{figure_man/ml_poisson_example_1.pdf} \\
% 	\begin{footnotesize}
% 		Example: Histogram of a sample drawn from a Poisson distribution $f(k ~|~ \lambda) := \P(x = k) = \frac{\lambda^k \cdot \exp(-\lambda)}{k!}$. 
% 	\end{footnotesize}
% \end{center}
% \framebreak 
%  We operationalize this as \textbf{maximizing} the log-likelihood function (or equivalently: minimizing the negative log-likelihood) with respect to $\lambda$:
% \begin{footnotesize}
% \begin{eqnarray*}
% 	\hat \lambda  &=& \text{arg min}_\lambda ~ - \ell(\lambda, \mathcal{D}) =\text{arg min}_\lambda  - \log \mathcal{L}(\lambda, \mathcal{D}) = \text{arg min}_\lambda - \log \prod_{i = 1}^n  f\left(\xi ~|~ \lambda\right) \\ &=& \text{arg min}_\lambda - \sumin f\left(x^{(i)} ~|~ \lambda\right) = \text{arg min}_\lambda \sumin \frac{- \lambda^{\xi} \cdot \exp(- \lambda)}{\xi!} 
% \end{eqnarray*}
% \end{footnotesize}
% \begin{center}
% 	\includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/ml_poisson_example_2.pdf} \\
% 	\begin{footnotesize}
% 		Example: The log-likelihood of a Poisson distribution for data example above. The objective function is univariate and differentiable, and the domain is \textcolor{red}{unconstrained}.
% 	\end{footnotesize}
% \end{center}
% \end{vbframe}
% \begin{vbframe}{Example 2: Maximum Likelihood Estimation}
% \textbf{Example}: Maximum Likelihood Estimation
% \lz
% For data $\left(\xv^{(1)}, ..., \xv^{(n)}\right)$, we want to find the maximum likelihood estimate
% $$
% \max_\theta L(\theta) = \prod_{i = 1}^n f(^{(i)}, \theta)
% $$
% In some cases, $\theta$ can only take \textbf{certain values}. 
% \lz 
% \begin{itemize}
% \item If $f$ is a Poisson distribution, we require the rate $\lambda$ to be non-negative, i.e. $\lambda \ge 0$
%   \item If $f$ is a multinomial distribution
% \begin{footnotesize}
%   $$
%   f(x_1, ..., x_p; n; \theta_1, ..., \theta_p) = \begin{cases} \binom{n!}{x_1! \cdot x_2! ... x_p!} \theta_1^{x_1} \cdot ... \cdot \theta_p^{x_p} & \text{if } x_1 + ... + x_p = n \\ 0 & \text{else}
%   \end{cases}
%   $$
% \end{footnotesize}
% \end{itemize}
% \end{vbframe}


\begin{frame2}{Example 3: Ridge regression}
Ridge regression can be formulated as regularized ERM:
$$
\thetah_{\text{Ridge}} = \argmin_{\thetav} \left\{ \sumin \left(\yi - \thetav^\top\xv\right)^2 + \lambda ||\thetav||_2^2 \right\}
$$
Equivalently it can be written as constrained optimization problem:
\splitVCC[0.48]{
$$
\begin{aligned}
\min_{\thetav} \quad & \sum_{i = 1}^n \left(\thetav^\top \xi - \yi\right)^2 \\
	ext{s.t. } \quad & \|\thetav\|_2 \le t
\end{aligned}
$$
}{
\imageC[0.45]{figure_man/ridge.png}
}
\spacer
$f, g$ smooth. \textbf{Convex program} (convex objective, quadratic constraint).
\end{frame2}


\begin{frame2}{Example 4: LASSO Regression}
Lasso regression can be formulated as regularized ERM:
$$
\thetah_{\text{Lasso}} =  \argmin_{\thetav} \left\{ \sumin \left(\yi - \thetav^\top\xv\right)^2 + \lambda ||\thetav||_1 \right\}
$$
Equivalently it can be written as constrained optimization problem:
\splitVCC[0.48]{
$$
\begin{aligned}
\min_{\thetav} \quad & \sum_{i = 1}^n \left(\thetav^\top \xi - \yi\right)^2 \\
\text{s.t. } \quad & \|\thetav\|_1 \le t
\end{aligned}
$$
}{
\imageC[0.45]{figure_man/lasso.png}
}
\spacer
$f$ smooth, $g$ \textbf{not smooth}. Still \textbf{convex program}.
\end{frame2}


\begin{frame2}{Example 5: Support Vector Machines}
The SVM problem can be formulated in $3$ equivalent ways: two primal, and one dual one (we will see later what "dual" means).
\begin{footnotesize}
Here, we only discuss the nature of the optimization problems. A more thorough statistical derivation of SVMs is given in ``Supervised learning''.\\
\end{footnotesize}
\spacer
\textbf{Formulation 1 (primal): } ERM with Hinge loss
$$
\sumin \max\left(1 - \yi f^{(i)}, 0\right) + \lambda \|\thetav\|_2^2, \quad f^{(i)} := \thetav^\top \xi
$$
\splitV{
\image[1]{figure_man/hinge.pdf}
}{
\begin{footnotesize}
Unconstrained, convex problem with non-smooth objective
\end{footnotesize}
}
\end{frame2}


\begin{frame2}{Example 5: Support Vector Machines}
\textbf{Formulation 2 (primal): } Geometric formulation
\begin{itemizeL}
\item Find decision boundary which separates classes with \textbf{maximum} safety distance
\item Distance to points closest to decision boundary (\enquote{safety margin $\gamma$}) should be \textbf{maximized}
\end{itemizeL}
\vfill
\imageC[0.5]{figure_man/svm-example.pdf}
\end{frame2}


\begin{frame2}{Example 5: Support Vector Machines}
\textbf{Formulation 2 (primal): } Geometric formulation
$$
\begin{aligned}
\min\limits_{\thetav, \theta_0} \quad & \frac{1}{2} \|\thetav\|^2 \\
\text{s.t.} \quad & \yi  \left( \scp{\thetav}{\xi} + \theta_0 \right) \ge 1 \quad \forall\, i \in \nset
\end{aligned}
$$
\spacer
\splitVCC[0.58]{
\imageC[0.6]{figure_man/svm_geometry.png}
}{
\begin{footnotesize}
Maximize safety margin $\gamma$. No point is allowed to violate safety margin constraint.
\end{footnotesize}
}
\end{frame2}


\begin{frame2}{Example 5: Support Vector Machines}
\textbf{Formulation 2 (primal): } Geometric formulation (soft constraints)
$$
\begin{aligned}
\min\limits_{\thetav, \theta_0, \zeta^{(i)}} \quad & \frac{1}{2} \|\thetav\|^2 + \textcolor{violet}{C   \sum_{i=1}^n \zeta^{(i)}} \\
\text{s.t.} \quad & \yi  \left( \scp{\thetav}{\xi} + \theta_0 \right) \ge 1 \textcolor{violet}{- \zeta^{(i)}} \quad \forall\, i \in \nset, \\
\text{and} \quad & \textcolor{violet}{\zeta^{(i)} \ge 0 \quad \forall\, i \in \nset.}
\end{aligned}
$$
\splitVCC[0.58]{
\imageC[0.6]{figure_man/boundary_with_violations.png}
}{
\begin{footnotesize}
Maximize safety margin $\gamma$. Margin \textcolor{violet}{violations are allowed, but are minimized}.
\end{footnotesize}
}
\spacer
The problem is a \textbf{QP}: Quadratic objective with linear constraints.
\end{frame2}


\begin{frame2}[footnotesize]{Example 5: Support Vector Machines}
\textbf{Formulation 3 (dual): } Dualizing the primal formulation
$$
\begin{aligned}
\max\limits_{\bm{\alpha} \in \R^n} \quad & \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_j\yi y^{(j)} \scp{\xi}{\xv^{(j)}} \\
\text{s.t. } \quad & 0 \le \alpha_i \le C \quad \forall\, i \in \nset, \quad \sum_{i=1}^n \alpha_i \yi = 0
\end{aligned}
$$
\spacer
\textit{Matrix notation:}
$$
\begin{aligned}
\max\limits_{\bm{\alpha} \in \R^n} \quad & \bm{\alpha}^\top \bm{1} - \frac{1}{2} \bm{\alpha}^\top \text{diag}(\bm{y}) \Xmat^\top \Xmat \text{diag}(\bm{y})\bm{\alpha} \\
\text{s.t. } \quad & 0 \le \alpha_i \le C \quad \forall\, i \in \nset, \quad \bm{\alpha}^\top \bm{y} = 0
\end{aligned}
$$
\textit{Kernelization:}
Replace dot product between $\xv$'s with $\bm{K}_{ij} = k(\xi, \xi[j])$, where $k(\cdot,\cdot)$ is a positive definite kernel function ($\Rightarrow$ $\bm{K}$ positive semi-definite).
\spacer
$$
\begin{aligned}
\max\limits_{\bm{\alpha} \in \R^n} \quad & \bm{\alpha}^\top \bm{1} - \frac{1}{2} \bm{\alpha}^\top \text{diag}(\bm{y}) \bm{K} \text{diag}(\bm{y})\bm{\alpha} \\
\text{s.t. } \quad & 0 \le \alpha_i \le C \quad \forall\, i \in \nset, \quad \bm{\alpha}^\top \bm{y} = 0
\end{aligned}
$$
\spacer
This is QP with a single affine equality constraint and $n$ box constraints.
\end{frame2}

% \begin{frame}{Example 5: Support Vector Machines}
% When applying the kernel trick to the dual (soft-margin) SVM problem by replacing $\scp{\xi}{\xv^{(j)}}$ by kernels $k(\xi, \xv^{(j)})$, we get the non-linear SVM:
% \begin{eqnarray*}
% 	& \max\limits_{\alpha \in \R^n} & \one^\top \alpha - \frac{1}{2} \alpha^\top \diag(\yv) \bm{K} \diag(\yv) \alpha \\
% 	& \text{s.t.} & \alpha^\top \yv = 0, \\
% 	& \quad & 0 \leq \alpha \leq C, 
% \end{eqnarray*}
% where $K_{ij} = k(\xi, \xv^{(j)})$. 
% \pause
% This is still a constrained convex quadratic problem, because $\bm{K} \in \R^{n \times n}$ is positive semi-definite. 
% \end{frame}

\endlecture
\end{document}
