\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\slvec}{\left(\zeta^{(1)}, \zeta^{(n)}\right)} % slack variable vector
\newcommand{\sli}[1][i]{\zeta^{(#1)}} % i-th slack variable
\newcommand{\scptxi}{\scp{\thetab}{\xi}} % scalar prodct of theta and xi
\newcommand{\alphav}{\bm{\alpha}} % vector alpha (bold) (basis fun coefficients)

\newcommand{\titlefigure}{figure_man/classes_optimization_problems.png}
\newcommand{\learninggoals}{
\item Definition
\item LP, QP, CP
\item Ridge and Lasso
\item Soft-margin SVM 
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Optimization Problems: \\Constrained problems}
\lecture{\inserttitle}
\sloppy

\begin{vbframe}{Constrained Optimization Problem }

$$
\min_{\xv \in \mathcal{S}} \fx, \text{ with } f: \; \mathcal{S} \to \R.
$$

\begin{itemize}
	\item \textbf{Constrained}, if domain $\mathcal{S}$ is restricted: $\mathcal{S} \textcolor{blue}{\subsetneq} \R^d.$

\item \textbf{Convex} if $f$ convex function and $\mathcal{S}$ convex set

	\item Typically $\mathcal{S}$ is defined via functions called \textbf{constraints}
	$$
		\mathcal{S}:= \{\xv \in \R^d ~|~ g_i(\xv) \le 0, h_j(\xv) = 0 ~\forall~ i, j\}, \text{ where }
	$$ 

\begin{itemize}
\item $g_i: \R^d \to \R, i = 1, ..., k$ are called inequality constraints,
\item $h_j: \R^d \to \R, j = 1, ..., l$ are called equality constraints.
\end{itemize}

\end{itemize}

\lz 

Equivalent formulation: 
\begin{eqnarray*}
\min && f(\mathbf{x})  \\
\text{such that} && g_i(\mathbf{x}) \le 0 \qquad \text{for } i=1,\ldots,k  \\
 && h_j(\mathbf{x}) = 0 \qquad \text{for } j=1,\ldots,l. 
\end{eqnarray*}

\end{vbframe}


\begin{vbframe}{Linear program (LP)}

\vspace*{-0.3cm}

\begin{itemize}
	\item $f$ linear s.t. linear constraints. Standard form: 
	\vspace*{-0.2cm}
	\begin{eqnarray*}
	\min_{\xv \in \R^d} && \bm{c}^\top\xv\\
	\text{s.t. } && \bm{A}\xv \ge \bm{b} \\
	&& \xv \ge 0
	\end{eqnarray*}
	for $\bm{c}\in \R^d, \bm{A} \in \R^{k \times d}$ and $\bm{b} \in \R^k$. 

	\begin{figure}
		\includegraphics[width=0.25\textwidth]{figure_man/simplex_iter1.png} ~~~ \includegraphics[width=0.25\textwidth]{figure_man/simplex.png} \\
		\begin{footnotesize}
		Visualization of constraints of 2D and 3D linear program (Source right figure: Wikipedia). 
		\end{footnotesize}
	\end{figure}
\end{itemize}
\end{vbframe}


\begin{vbframe}{Quadratic program (QP)}

\begin{itemize}
	\item $f$ quadratic form s.t. linear constraints. Standard form: 
	\vspace*{-0.3cm}
	\begin{eqnarray*}
	\min_{\xv \in \R^d} && \frac{1}{2} \xv^\top \bm{A} \xv + \bm{b}^\top \xv 
	+ c \\
	\text{s.t. } && \bm{E}\xv \le \bm{f} \\
	 && \bm{G}\xv = \bm{h} \\
	\end{eqnarray*}
	$\bm{A} \in \R^{d \times d}, \bm{b} \in \R^d, c \in \R$, $\bm{E} \in \R^{k \times d}, \bm{f} \in \R^k$, $\bm{G} \in \R^{l \times d}$, $\bm{f} \in \R^l$. 
\end{itemize}

%\vspace*{-0.8cm}
	\begin{figure}
		\includegraphics[width=0.3\textwidth]{figure_man/quadratic-programm.jpeg} \\
		\begin{footnotesize}
		Visualization of quadratic objective (dashed) over linear constraints (grey). Source: Ma, Signal Processing Optimization Techniques, 2015. 
		\end{footnotesize}
	\end{figure}

% \begin{footnotesize}
% $\leq$ and $\geq$ to be understood component-wise. 

% \end{footnotesize}

\end{vbframe}

\begin{vbframe}{Convex program (CP)}

\begin{itemize}
	\item $f$ convex, convex inequality constraints, linear equality constraints. Standard form: 
	\vspace*{-0.2cm}
	\begin{eqnarray*}
	\min_{\xv \in \R^d} && \fx\\
	\text{s.t. } && g_i(\xv) \le 0, i = 1, ..., k \\
	&& \bm{A}\xv = \bm{b}
	\end{eqnarray*}
	for $\bm{A} \in \R^{l \times d}$ and $\bm{b} \in \R^l$. 
\end{itemize}

\begin{figure}
	\includegraphics[width=0.8\textwidth]{figure_man/cp_example.jpg} \\
	\begin{footnotesize}
	Convex program (left) vs. nonconvex program (right). Source: Mathworks.
	\end{footnotesize}
\end{figure}
\vspace*{-0.6cm}


% \begin{itemize}
% \item \textbf{Box-constrained}:

% $$
% \mathcal{S} = \{\xv \in \R^d: x_i^l \leq x_i \leq x_i^u \text{ for all } i\}
% $$
% \item
% \textbf{Linear or non-linear constraints}
% \begin{eqnarray*}
% \mathcal{S} = \{\xv \in \R^d: g(\xv) \le 0;  h(\xv) = 0\}
% \end{eqnarray*}
% We call $g(\xv)\le 0$ an inequality constraint, and $h(\xv) = 0 $ an equality constraint. 

% \end{itemize}
\end{vbframe}


\begin{vbframe}{Further types}

\begin{columns}[T] % align columns
	\begin{column}{.58\textwidth}
		\begin{center}
			\includegraphics[width=0.8\textwidth]{figure_man/classes_optimization_problems.png} 
		\end{center}
	\end{column}
	\begin{column}{.38\textwidth} \vspace*{1.5cm}
		Quadratically constrained linear program (QCLP) and quadratically constrained quadratic program (QCQP). 
	\end{column}
\end{columns}



\end{vbframe}

\begin{vbframe}{Example 1: Unit circle}

\vspace*{-0.5cm} 

\begin{eqnarray*}
  \min && f(x_1, x_2) = x_1 + x_2 \\
  \text{s.t. } && h(x_1,x_2) = x_1^2 + x_2^2 - 1 = 0
\end{eqnarray*}

\begin{center}
  \includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/unit_circle.png} \\
\end{center}

$f, h$ smooth. Problem $\textbf{not convex}$ ($\mathcal{S}$ is not a convex set). 

\lz 

\begin{footnotesize}
\textbf{Note: } If the constraint is replaced by $g(x_1, x_2) = x_1^2 + x_2^2 - 1 \le 0$, the problem is a convex program, even a quadratically constrained linear program (QCLP). 
\end{footnotesize}

\end{vbframe}

\begin{vbframe}{Example 2: Maximum likelihood}

\textbf{Experiment}: Draw $m$ balls from a bag with balls of $k$ different colors.
Color $j$ has a probability of $p_j$ of being drawn. 

\lz 

The probability to realize the outcome $\xv = (x_1, ..., x_k)$, $x_j$ being the number of balls drawn in color $j$, is: 

$$
	f(\xv, m, \bm{p}) = \begin{cases} \frac{m!}{x_1! \cdots x_k!} \cdot p_1^{x_1} \cdots p_k^{x_k} & \text{ if } \sum_{i = 1}^k x_i = m \\ 0 & \text{ otherwise}\end{cases}
$$
The parameters $p_j$ are subject to the following constraints: 

\begin{eqnarray*}
	0 \le p_j \le 1 && \text{ for all } i \\
	 \sum_{j = 1}^m p_j = 1. &&
\end{eqnarray*}

\framebreak 

For a fixed $m$ and a sample $\D = \left(\xi[1], ..., \xi[n]\right)$, where $\sum_{j = 1}^k \xi[i]_j = m$ for all $i = 1, ..., n$, the negative log-likelihood is: 


\begin{eqnarray*}
	- \ell(\bm{p}) &=& - \log \left(\prod_{i = 1}^n  \frac{m!}{\xi_1! \cdots \xi_k!} \cdot p_1^{\xi_1} \cdots p_k^{\xi_k}    \right) \\
	&=& \sumin \left[- \log(m!) + \sum_{j = 1}^k \log(\xi_j!) - \sum_{j = 1}^k \xi_j \log(p_j)\right] \\
	&\propto& - \sumin \sum_{j = 1}^k \xi_j \log(p_j) 
\end{eqnarray*}

$f, g, h$ are smooth.\\\textbf{Convex program}: convex$^{(*)}$ objective + box/linear constraints). 

\vfill
\begin{footnotesize}
${(*)}$: $\log$ is concave, $- \log $ is convex, and the sum of convex functions is convex. 
\end{footnotesize}

\end{vbframe}




% \begin{vbframe}{Example 1.1: Maximum Likelihood Estimation: Poisson Distribution}
	
% $\D = \left(x^{(1)}, ..., x^{(n)}\right)$ is sampled i.i.d. from density $f(x ~|~ \thetab)$. We want to find $\lambda$ which makes the observed data most likely.

% \begin{center}
% 	\includegraphics[width=0.4\textwidth, height=0.4\textwidth]{figure_man/ml_poisson_example_1.pdf} \\
% 	\begin{footnotesize}
% 		Example: Histogram of a sample drawn from a Poisson distribution $f(k ~|~ \lambda) := \P(x = k) = \frac{\lambda^k \cdot \exp(-\lambda)}{k!}$. 
% 	\end{footnotesize}
% \end{center}

% \framebreak 

%  We operationalize this as \textbf{maximizing} the log-likelihood function (or equivalently: minimizing the negative log-likelihood) with respect to $\lambda$:

% \begin{footnotesize}
% \begin{eqnarray*}
% 	\hat \lambda  &=& \text{arg min}_\lambda ~ - \ell(\lambda, \mathcal{D}) =\text{arg min}_\lambda  - \log \mathcal{L}(\lambda, \mathcal{D}) = \text{arg min}_\lambda - \log \prod_{i = 1}^n  f\left(\xi ~|~ \lambda\right) \\ &=& \text{arg min}_\lambda - \sumin f\left(x^{(i)} ~|~ \lambda\right) = \text{arg min}_\lambda \sumin \frac{- \lambda^{\xi} \cdot \exp(- \lambda)}{\xi!} 
% \end{eqnarray*}
% \end{footnotesize}

% \begin{center}
% 	\includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/ml_poisson_example_2.pdf} \\
% 	\begin{footnotesize}
% 		Example: The log-likelihood of a Poisson distribution for data example above. The objective function is univariate and differentiable, and the domain is \textcolor{red}{unconstrained}.
% 	\end{footnotesize}
% \end{center}


% \end{vbframe}

% \begin{vbframe}{Example 2: Maximum Likelihood Estimation}

% \textbf{Example}: Maximum Likelihood Estimation

% \lz

% For data $\left(\xv^{(1)}, ..., \xv^{(n)}\right)$, we want to find the maximum likelihood estimate

% $$
% \max_\theta L(\theta) = \prod_{i = 1}^n f(^{(i)}, \theta)
% $$

% In some cases, $\theta$ can only take \textbf{certain values}. 

% \lz 

% \begin{itemize}
% \item If $f$ is a Poisson distribution, we require the rate $\lambda$ to be non-negative, i.e. $\lambda \ge 0$

%   \item If $f$ is a multinomial distribution

% \begin{footnotesize}
%   $$
%   f(x_1, ..., x_p; n; \theta_1, ..., \theta_p) = \begin{cases} \binom{n!}{x_1! \cdot x_2! ... x_p!} \theta_1^{x_1} \cdot ... \cdot \theta_p^{x_p} & \text{if } x_1 + ... + x_p = n \\ 0 & \text{else}
%   \end{cases}
%   $$
% \end{footnotesize}


% \end{itemize}

% \end{vbframe}

\begin{vbframe}{Example 3: Ridge regression}
		
Ridge regression can be formulated as regularized ERM: 

\begin{eqnarray*}  
	\thetah_{\text{Ridge}} &=& \argmin_{\thetab} \left\{ \sumin \left(\yi - \thetab^\top\xv\right)^2 + \lambda ||\thetab||_2^2 \right\}
\end{eqnarray*}

Equivalently it can be written as constrained optimization problem: 

\vspace*{-0.2cm}
\begin{columns}[T] % align columns
	\begin{column}{.48\textwidth}
		\begin{eqnarray*}
			\min_{\thetab} && \sum_{i = 1}^n \left(\thetab^\top \xi - \yi\right)^2 \\
			\text{s.t. } && \|\thetab\|_2 \le t
		\end{eqnarray*}
		
		%We are looking for the best SSE with $\thetab$ that lies within a $t$-ball around $0$. 
	\end{column}

	\begin{column}{.48\textwidth}
		\begin{center}
			\includegraphics[width=0.45\textwidth, keepaspectratio]{figure_man/ridge.png} 
		\end{center}
	\end{column}
\end{columns}

\vspace*{0.2cm}

$f, g$ smooth. \textbf{Convex program} (convex objective, quadratic constraint). 

\end{vbframe}
	
\begin{vbframe}{Example 4: LASSO Regression}
	
Lasso regression can be formulated as regularized ERM: 

\begin{eqnarray*}  
		\thetah_{\text{Lasso}} &=&  \argmin_{\thetab} \left\{ \sumin \left(\yi - \thetab^\top\xv\right)^2 + \lambda ||\thetab||_1 \right\} 
\end{eqnarray*}

Equivalently it can be written as constrained optimization problem: 

	\vspace*{-0.2cm}
	\begin{columns}[T] % align columns
		\begin{column}{.48\textwidth}
			\begin{eqnarray*}
				\min_{\thetab} && \sum_{i = 1}^n \left(\thetab^\top \xi - \yi\right)^2 \\
				\text{s.t. } && \|\thetab\|_1 \le t
			\end{eqnarray*}
		\end{column}

		\begin{column}{.48\textwidth}
			\begin{center}
				\includegraphics[width=0.45\textwidth, keepaspectratio]{figure_man/lasso.png} 
			\end{center}
		\end{column}
	\end{columns}	

\vspace*{0.2cm}

$f$ smooth, $g$ \textbf{not smooth}. Still \textbf{convex program}. 

\end{vbframe}

\begin{vbframe}{Example 5: Support Vector Machines} 

The SVM problem can be formulated in $3$ equivalent ways: two primal, and one dual one (we will see later what "dual" means). 

\begin{footnotesize}
	Here, we only discuss the nature of the optimization problems. A more thorough statistical derivation of SVMs is given in ``Supervised learning''. 
\end{footnotesize}

\vspace*{0.3cm}

\textbf{Formulation 1 (primal): } ERM with Hinge loss

$$
	\sumin \max\left(1 - \yi f^{(i)}, 0\right) + \lambda \|\thetab\|_2^2, \quad f^{(i)} := \thetab^\top \xi
$$

\vspace*{-0.3cm}

\begin{columns}[T] % align columns
	\begin{column}{.58\textwidth}
		\begin{figure}
			\begin{center}
				\includegraphics[width=0.8\textwidth]{figure_man/hinge.pdf}
			\end{center}
			% See hinge_vs_l2.R
		\end{figure}
	\end{column}
	\begin{column}{.38\textwidth} \vspace*{1.5cm}
		\begin{footnotesize}
		Unconstrained, convex problem with non-smooth objective
		\end{footnotesize}
	\end{column}
\end{columns}

\framebreak 

\textbf{Formulation 2 (primal): } Geometric formulation

\begin{itemize}
	\item Find decision boundary which separates classes with \textbf{maximum} safety distance
	\item Distance to points closest to decision boundary (\enquote{safety margin $\gamma$}) should be \textbf{maximized}
\end{itemize}

\begin{figure}
	\begin{center}
		\includegraphics[width=0.4\textwidth]{figure_man/svm-example.pdf}
	\end{center}
\end{figure}

\end{vbframe}

\begin{frame}{Example 5: Support Vector Machines} 

	\only<1>{

	\textbf{Formulation 2 (primal): } Geometric formulation

	\vspace*{-0.5cm}

	\begin{eqnarray*}
		& \min\limits_{\thetab, \thetab_0} & \frac{1}{2} \|\thetab\|^2 \\ % + C   \sum_{i=1}^n \sli \\
		& \text{s.t.} & \,\, \yi  \left( \scp{\thetab}{\xi} + \theta_0 \right) \geq 1 \quad \forall\, i \in \nset
		% & \text{and} & \,\, \sli \geq 0 \quad \forall\, i \in \nset.\\
	\end{eqnarray*}

	\begin{columns}[T] % align columns
		\begin{column}{.58\textwidth}
			\begin{figure}
				\begin{center}
					\includegraphics[width=0.6\textwidth]{figure_man/svm_geometry.png}
				\end{center}
				% Taken from here https://github.com/slds-lmu/lecture_i2ml/tree/master/figure
			\end{figure}
		\end{column}
		\begin{column}{.38\textwidth} \vspace*{1.5cm}
			\begin{footnotesize}
			Maximize safety margin $\gamma$. No point is allowed to violate safety margin constraint. 
			\end{footnotesize}
		\end{column}
	\end{columns}

	}

	\only<2>{

	\textbf{Formulation 2 (primal): } Geometric formulation (soft constraints)

	\vspace*{-0.5cm}

	\begin{eqnarray*}
		& \min\limits_{\thetab, \thetab_0,\sli} & \frac{1}{2} \|\thetab\|^2 + \textcolor{violet}{C   \sum_{i=1}^n \sli} \\
		& \text{s.t.} & \,\, \yi  \left( \scp{\thetab}{\xi} + \theta_0 \right) \geq 1 \textcolor{violet}{- \sli} \quad \forall\, i \in \nset,\\
		& \text{and} & \,\, \textcolor{violet}{\sli \geq 0 \quad \forall\, i \in \nset.}\\
	\end{eqnarray*}

\vspace*{-1cm}

	\begin{columns}[T] % align columns
		\begin{column}{.58\textwidth}
			\begin{figure}
				\begin{center}
					\includegraphics[width=0.6\textwidth]{figure_man/boundary_with_violations.png}
				\end{center}
				% Taken from here https://github.com/slds-lmu/lecture_i2ml/tree/master/figure
			\end{figure}
		\end{column}
		\begin{column}{.38\textwidth} \vspace*{1.5cm}
			\begin{footnotesize}
			Maximize safety margin $\gamma$. Margin \textcolor{violet}{violations are allowed, but are minimized}. 
			\end{footnotesize}
		\end{column}
	\end{columns}
	}

	The problem is a \textbf{QP}: Quadratic objective with linear constraints.  


\end{frame}	


%	\begin{frame}{Example 6: Support Vector Machines} 
%		
%		Mathematically, the support vector machine problem corresponds to the following optimization problem: 
%		
%		\begin{eqnarray*}
%			& \max\limits_{\thetab, \theta_0} & \gamma \\
%			& \text{s.t.} & \,\, d_f\left(\xi, \yi\right) \ge \gamma \quad \text{for all } i \in \{1, ..., n\}
%		\end{eqnarray*}
%		\pause
%		This is a convex quadratic program based on geometric intuition, but hard to optimize.
%		
%		\begin{center}
%			\includegraphics[width=2.5cm]{figure_man/svm_example.pdf} \\
%		\end{center}
%	\end{frame}	


\begin{frame}{Example 5: Support Vector Machines}

\begin{footnotesize}
\textbf{Formulation 3 (dual): } Dualizing the primal formulation 
\begin{eqnarray*}
	& \max\limits_{\alphav \in \R^n} & \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_j\yi y^{(j)} \scp{\xi}{\xv^{(j)}} \\
	& \text{s.t. } & 0 \le \alpha_i \le C \quad \forall\, i \in \nset, \quad \sum_{i=1}^n \alpha_i \yi = 0
\end{eqnarray*}
\end{footnotesize}


\vspace*{-0.5cm}

\begin{footnotesize}
\textit{Matrix notation:}
\begin{eqnarray*}
	& \max\limits_{\alphav \in \R^n} & \bm{\alpha}^\top \bm{1} - \frac{1}{2} \bm{\alpha}^\top ~ \text{diag}(\bm{y}) ~ \Xmat^\top \Xmat ~ \text{diag}(\bm{y})~\bm{\alpha} \\
	& \text{s.t. } & 0 \le \alpha_i \le C \quad \forall\, i \in \nset, \quad \bm{\alpha}^\top \bm{y} = 0
\end{eqnarray*}

\textit{Kernelization:}
Replace dot product between $\xv$'s with $\bm{K}_{ij} = k(\xi, \xi[j])$, where $k(\cdot,\cdot)$ is a positive definite kernel function ($\Rightarrow$ $\bm{K}$ positive semi-definite).
\end{footnotesize}

\begin{footnotesize}
\begin{eqnarray*}
	& \max\limits_{\alphav \in \R^n} & \bm{\alpha}^\top \bm{1} - \frac{1}{2} \bm{\alpha} ~ \text{diag}(\bm{y}) ~ \bm{K} ~ \text{diag}(\bm{y})~\bm{\alpha} \\
	& \text{s.t. } & 0 \le \alpha_i \le C \quad \forall\, i \in \nset, \quad \bm{\alpha}^\top \bm{y} = 0
\end{eqnarray*}

\vspace*{0.2cm}

This is QP with a single affine equality constraint and $n$ box constraints. 
\end{footnotesize}

\end{frame}

% \begin{frame}{Example 5: Support Vector Machines}
% When applying the kernel trick to the dual (soft-margin) SVM problem by replacing $\scp{\xi}{\xv^{(j)}}$ by kernels $k(\xi, \xv^{(j)})$, we get the non-linear SVM:
% \begin{eqnarray*}
% 	& \max\limits_{\alpha \in \R^n} & \one^\top \alpha - \frac{1}{2} \alpha^\top \diag(\yv) \bm{K} \diag(\yv) \alpha \\
% 	& \text{s.t.} & \alpha^\top \yv = 0, \\
% 	& \quad & 0 \leq \alpha \leq C, 
% \end{eqnarray*}
% where $K_{ij} = k(\xi, \xv^{(j)})$. 
% \pause
% This is still a constrained convex quadratic problem, because $\bm{K} \in \R^{n \times n}$ is positive semi-definite. 
% \end{frame}


\endlecture
\end{document}
