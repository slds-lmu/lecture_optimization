\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/cmaes/cmaes_generations.png}
\newcommand{\learninggoals}{
\item CMA-ES strategy
\item Estimation of distribution
\item Step size control
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{CMA-ES Algorithm}
\lecture{Optimization in Machine Learning}
\sloppy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \section{Covariance Matrix Adaptation Evolution Strategy (CMA-ES)}

%\begin{vbframe}{CMA-ES as part of many EDAs}
%\textbf{Estimation of Distribution Algorithms} (EDAs) widely used class of algorithms designed to solve optimization problems of the form
%
%\vspace{-10pt}
%\begin{eqnarray*}
%\xv^* = \argmax_{\xv \in \mathcal{S}} f(\xv), \quad \text{where } f:\mathcal{S}\rightarrow \R.
%\end{eqnarray*}
%
%
%Instead of solving above objective directly, EDAs solve related objective:
%\begin{eqnarray*}
%\thetab^\ast = \argmax_{\thetab} \E_{p(\xv|\thetab)} f(\xv),
%\end{eqnarray*}
%\vspace{-10pt}
%
%where $p(\xv|\thetab)$ is a probability density over $\mathcal{S}$, parameterized by $p$ parameters $\thetab \in \R^p$.
%
%\lz
%
%Reason for later formulation: convenience of derivative-free optimization, enabling of rigorous analysis and leveraging of a probabilistic formulation to incorporate auxiliary information.
%\end{vbframe}
%% \framebreak
\begin{frame}{Estimation of Distribution Algorithm}

\begin{minipage}{0.62\textwidth}
\begin{itemize}
    % \item General algorithmic template
    \item Instead of population, maintain distribution to sample offspring from
\end{itemize}

\vspace{\baselineskip}

\begin{enumerate}
    \item Draw $\lambda$ offsprings $\xv^{(i)}$ from $p(\cdot|\thetab^{[t]})$
    \item Evaluate fitness $f(\xv^{(i)})$ 
    %\item Reduce to $\mu$ best offspring
    %where $W(\cdot)$ gives weights for each $\xv^{(i)}$, typically $0$ or $1$ (order-preserving fitness transformation)
    \item Update $\thetab^{[t+1]}$ with $\mu$ best offsprings
\end{enumerate}

%This core algorithm is often modified in a variety of ways to improve performance via \textbf{Covariance Matrix Adaptation (CMA-ES)}.
\end{minipage}\hfill
\begin{minipage}{0.35\textwidth}\raggedleft
\begin{figure}
  \includegraphics[width=1\textwidth, height=0.9\textheight]{figure_man/cmaes/cmaes_eda.png}
\end{figure}
\end{minipage}

\end{frame}


% \begin{vbframe}{CMA-ES}

% Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is

% \begin{itemize}
% \item A state-of-the-art tool in evolutionary computation 
% \item Stochastic/randomized black box optimization algorithm
% \item For usage in continuous domain
% \item For non-linear, non-convex optimization problems
% \item Particularly effective in \enquote{hard}/ill-conditioned settings
% \end{itemize}
% \vspace{0.3cm}
% Detailed information on CMA-ES can be found in

% \begin{enumerate}
% \item Nikolaus Hansen. The CMA Evolution Strategy. 2016
% \item A. Auger, N. Hansen: Tutorial CMA-ES: Evolution Strategies and Covariance Matrix Adaptation. 2012.
% \end{enumerate}

%\end{vbframe}


\begin{vbframe}{Covariance Matrix Adaptation}

Sample distribution is multivariate Gaussian
$$
    \xv^{[t+1](i)} \sim \mathbf{m}^{[t]} + \sigma^{[t]} \normal (\bm{0}, \mathbf{C}^{[t]}) \quad \text{for } i = 1, \dots, \lambda
$$
\vspace{-20pt}
\begin{itemize}
\item $\xv^{[t+1](i)} \in \R^d$ $i$-th offspring; $\lambda \geq 2$ number of offspring
%\item $\normal(\bm{0}, \mathbf{C}^{[t]})$ is multivariate normal distribution with zero mean, covariance matrix $\mathbf{C}^{[t]}$. \textit{Note}: $\mathbf{m}^{[t]} + \sigma^{[t]} \normal(\bm{0}, \mathbf{C}^{[t]}) \sim \normal(\mathbf{m}^{[t]}, (\sigma^{[t]})^2 \mathbf{C}^{[t]})$.
\item $\mathbf{m}^{[t]} \in \R^d$ mean value and $\mathbf{C}^{[t]} \in \R^{d \times d}$ covariance matrix
\item $\sigma^{[t]} \in \R_{+}$ \enquote{overall} standard deviation/step size
% Up to the scalar factor $\sigma^{(g)^2}$, $\mathbf{C}^{(g)}$ is the covariance matrix of the search distribution.
\end{itemize}

\begin{figure}
  \includegraphics[width=0.5\textwidth]{figure_man/cmaes/cmaes_generations.png}
\end{figure}


\textbf{Question:} How to adapt $\mathbf{m}^{[t+1]}$, $\mathbf{C}^{[t+1]}$, $\sigma^{[t+1]}$ for next generation $t+1$?

\end{vbframe}


% \begin{vbframe}{Recall: Evolution Strategies (ES)}

% New search points are sampled normally distributed as perturbations of $\mathbf{m}$:
% \begin{eqnarray*}
% \xv_k \sim \mathbf{m} + \sigma \normal_k (\bm{0}, \mathbf{C}) \quad \text{for } i = 1, \dots, \lambda
% \end{eqnarray*}

% where $\xv_k$, $\mathbf{m} \in \R^{n}$, $\sigma \in \R_{+}$, $\mathbf{C} \in \R^{n \times n}$

% \begin{itemize}
% \item Mean vector $\mathbf{m} \in \R^d$ represents the favorite solution
% \item Step-size $\sigma \in \R_{+}$ controls the step length
% \item Covariance matrix $\mathbf{C} \in \R^{n \times n}$ determines the shape of the distribution ellipsoid.
% \end{itemize}

% Remaining question: How to update $\mathbf{m}$, $\mathbf{C}$ and $\sigma$?
% \end{vbframe}



% \begin{vbframe}{CMA-ES: Basic Method}
% \begin{enumerate}
% \item \textbf{Sample maximum entropy} distribution
% \item[] $x_i = m + \sigma \normal_i(\bm{0}, \mathbf{C})$ multivariate normal distribution
% \item \textbf{Ranking} solutions according to their fitness
% \item[] Invariance to order-preserving transformations
% \item \textbf{Update mean and covariance matrix} by natural gradient ascend, improving the \enquote{expected fitness} and the likelihood for good steps
% \item[] PCA $\rightarrow$ variable metric, new problem representation, invariant under changes of the coordinate system
% \item \textbf{Update step-size} based on non-local information
% \item[] Exploit correlations in the history of steps.
% \end{enumerate}
% \end{vbframe}



\begin{vbframe}{CMA-ES: Basic Method - Iteration 1}
% \begin{eqnarray*}
% \mathbf{m} \leftarrow \mathbf{m} + \sigma \yv_w, \quad \yv_w = \sum_{i=1}^{\mu} w_i \yv_{i:\lambda}, \quad \yv_i\sim \normal_i(\bm{0}, \mathbf{C})
% \end{eqnarray*}

\begin{enumerate}
    \addtocounter{enumi}{-1}
    \item Initialize $\mathbf{m}^{[0]},\sigma^{[0]}$ problem-dependent and $\mathbf{C}^{[0]}=\mathbf{I}_{d}$
        \scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_1.png}}

    \framebreak

    \item \textbf{Sample} $\lambda$ offsprings from distribution
    $$\xv^{[1](i)} = \mathbf{m}^{[0]} + \sigma^{[0]} \normal(\bm{0}, \mathbf{C}^{[0]})$$
        \scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_2.png}}

    \item \textbf{Selection and recombination} of $\mu<\lambda$ best-performing offspring using fixed weights $w_1\geq\ldots\geq w_{\mu}>0,\sum_{i=1}^{\mu} w_i = 1$. %solutions according to their fitness (\textit{Selection} of $\mu$ best)
    
    $\xv_{i:\lambda}$ is $i$-th ranked solution, ranked by $f(\xv_{i:\lambda})$.

        \scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_21.png}}
        \vspace{-0.5cm}\\
        Calculation of auxiliary variables ($\mu=3$ points) $\yv_w^{[1]} := \sum_{i=1}^{\mu} w_i (\xv_{i:\lambda}^{[1]}-\mathbf{m}^{[0]})/\sigma^{[0]} := \sum_{i=1}^{\mu} w_i \yv_{i:\lambda}^{[1]}$%, using $\mu = 3$ points. %(high fitness $\rightarrow$ high weights)

%Movement to new population mean $\mathbf{m}^{[1]}$ (disregarding $\sigma$) of the $\mu = 3$ selected points (high fitness $\rightarrow$ high weights, $\yv_w := \sum_{i=1}^{\mu} w_i \xv_{i:\lambda}$).

    \item \textbf{Update mean}
        \scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_3.png}}
        Movement towards the new distribution with mean
        
        $\mathbf{m}^{[1]} = \mathbf{m}^{[0]} + \sigma^{[0]} \yv_{w}^{[1]}$.

    \item \textbf{Update covariance matrix} %(\textit{Recombination}),
    
        Roughly: elongate density ellipsoid in direction of successful steps.
        
        $\mathbf{C}^{[1]}$ reproduces successful points with higher probability than $\mathbf{C}^{[0]}$. %Improving \enquote{expected fitness} and likelihood for successful steps.
        
        \scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_4.png}}
        \vspace{-0.5cm}

        \begin{small}
            Update $\mathbf{C}^{[0]}$ using sum of outer products and parameter $c_{\mu}$: %(simplified):
            $\mathbf{C}^{[1]} = (1-c_{\mu}) \mathbf{C}^{[0]} + c_{\mu} \sum_{i=1}^{\mu} w_i \yv_{i:\lambda}^{[1]}(\yv_{i:\lambda}^{[1]})^{\top}$ (rank-$\mu$ update).
            %\yv_w^{[1]} (\yv_w^{[1]})^\top$ (Rank 1 update).
        \end{small}

    %New Distribution $\normal^{[1]}\sim (\mathbf{m}^{[1]}, \mathbf{C}^{[1]})$ (disregarding $\sigma$) \\
    %of generation $t=1$, $\lambda = 6$.
\end{enumerate}
\end{vbframe}

\begin{vbframe}{CMA-ES: Basic Method - Iteration 2}
\begin{enumerate}
    \item \textbf{Sample} from distribution for new generation
        \scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_5.png}}

    \framebreak
    
    \item \textbf{Selection and recombination} of $\mu<\lambda$ best-performing offspring
\item \textbf{Update mean}
    \scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_6.png}}

    \item \textbf{Update covariance matrix} %(\textit{Recombination})
        \scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_7.png}}

    \item \textbf{Update step-size} exploiting correlation in history of steps.
    
        steps point in similar direction $\implies$ increase step-size
        
        steps cancel out $\implies$ decrease step-size
        
        \scalebox{0.95}{\includegraphics{figure_man/cmaes/cmaes_update_8.png}}

\end{enumerate}
\end{vbframe}

%\begin{vbframe}{Updating \MakeLowercase{$\mathbf{m}$}: The $(\mu/\mu_W, \lambda)$-ES}
%
%$(\mu/\mu_W, \lambda)$-ES marks the \textbf{E}volution \textbf{S}trategy with $\bm{\mu}$ parents, \textbf{W}eighted recombination of all $\bm{\mu}$ parents and $\bm{\lambda}$ offspring.
%
%\lz
%
%Let $\xv_{i:\lambda}$ be the $i$-th ranked solution point, the new mean vector is
%% such that $f(\xv_{1:\lambda}) \leq \dots \leq f(\xv_{\lambda:\lambda})$.
%
%\vspace{-10pt}
%\begin{eqnarray*}
%\mathbf{m}^{[t+1]} = \mathbf{m}^{[t]} + \sigma^{[t]} \underbrace{\sum_{i=1}^{\mu} w_i \xv_{i:\lambda}^{[t]}}_{=: \yv_w^{[t]}}
%\end{eqnarray*}
%
%where $w_1 \geq \dots \geq w_{\mu} > 0$, $\sum_{i=1}^{\mu} w_i = 1$ and $\frac{1}{\sum_{i=1}^{\mu} w_i^2} =: \mu_w \approx \frac{\lambda}{4}$.
%
%\lz
%
%The best $\mu$ points are selected from the new solutions (non-elitistic) and weighted intermediate recombination is applied.
%
%\lz
%
%If $w_{i=1:\mu} = 1/\mu$ then $\yv_w$ is equal to the mean of the $\mu$ best points.
%\end{vbframe}


%\begin{vbframe}{Updating \MakeLowercase{$\mathbf{m}$}: The $(\mu/\mu_W, \lambda)$-ES}
%
%Remarks on weights $w_i$
%
%\begin{itemize}
%\item $w_{i=1:\mu} \in \R_{>0}$ are positive weight coefficients for recombination
%\item Typically chosen as weighted average of $\mu$ selected points $w_{i=1:\mu} = 1/\mu$
%\item Assigning different weights $w_i$ should be interpreted as a selection mechanism
%\item Approaches exist, which give the remaining $\lambda-\mu$ points negative weights, such that $\lambda$ weights in total are used (e.g active covariance matrix adaptation)
%\item Weights depend only on the ranking, not on the function values directly $\rightarrow$ renders the algorithm invariant under order-preserving transformation of the objective function
%\end{itemize}
%\end{vbframe}

\begin{vbframe}{Updating $\mathbf{C}$: Full Update}

Full CMA update of $\mathbf{C}$ combines rank-$\mu$ update with a rank-$1$ update using exponentially smoothed evolution path $\mathbf{p}_c \in \mathbb{R}^{d}$ of successive steps and learning rate $c_1$:
$$\mathbf{p}_{c}^{[0]}=\bm{0}, \quad \mathbf{p}_{c}^{[t+1]} = (1-c_1)\mathbf{p}_{c}^{[t]} + \sqrt{\frac{c_1(2-c_1)}{\sum_{i=1}^{\mu}w_i^2}}\yv_w$$
Final update of $\mathbf{C}$ is
$$\mathbf{C}^{[t+1]}=(1-c_1-c_{\mu}{\scriptstyle\sum_j} w_j)\mathbf{C}^{[t]}+c_1 \underbrace{\mathbf{p}_{c}^{[t+1]}(\mathbf{p}_{c}^{[t+1]})^{\top}}_{\text{rank-$1$}}+c_{\mu}\underbrace{\sum_{i=1}^{\mu}w_i \yv_{i:\lambda}^{[t+1]}(\yv_{i:\lambda}^{[t+1]})^{\top}}_{\text{rank-$\mu$}}$$
\vspace{-0.4cm}
\begin{itemize}
    \item Correlation between generations used in rank-$1$ update
    \item Information from entire population is used in rank-$\mu$ update
\end{itemize}

\end{vbframe}

%\begin{vbframe}{Updating $C$: CMA - Rank-One Update}
%Initialize $\mathbf{m} \in \R^d$ and $\mathbf{C} = \bm{\I}$, set $\sigma = 1$, learning rate $c_{cov} \approx 2/d^2$. While not terminate

%\begin{align*}
%\xv^{(i)} &= \mathbf{m} + \sigma \normal_i(\bm{0}, \mathbf{C}) \\
%\mathbf{m} &\leftarrow \mathbf{m} + \sigma \yv_w, \quad \text{where } \yv_w = \sum_{i=1}^\mu \bm{w}_i(\xv_{i:\lambda}-\mathbf{m})/\sigma \\
%\mathbf{C} &\leftarrow (1-c_{cov}) \mathbf{C} + c_{cov}\mu_w \underbrace{\yv_w\yv_w^\top}_{\text{rank-one}}, \quad \text{where } \mu_w = \frac{1}{\sum_{i=1}^\mu w_i^2}
%\end{align*}

%The rank-one update was developed in several domains independently, conducting a \textbf{principle component analysis} (PCA) of steps $\yv_w$ sequentially in time and space.

% \lz

% \textit{In principle}: the adaptation increases the likelihood of successful steps $\yv_w$ to appear again.

% \textit{Different viewpoint}: the adaptation follows a natural gradient approximation of the expected fitness.

% \framebreak

% \begin{eqnarray*}
% \mathbf{C} &\leftarrow (1-c_{cov}) \mathbf{C} + c_{cov}\mu_w \yv_w\yv_w^\top
% \end{eqnarray*}

% \begin{itemize}
% \item Conducting a \textbf{principle component analysis} (PCA) of steps $\yv_w$ sequentially in time and space
% \item Approximation of the \textbf{inverse Hessian} on quadratic functions
% \item Learning of a new \textbf{rotated problem representation}
% \item[] Components only independent in the new representation
% \item Learning of all \textbf{pairwise dependencies} between variables
% \item[] Dependencies reflected by off-diagonal entries in the covariance matrix
% \item Learning of a \textbf{new} (Mahalanobis) \textbf{metric}
% \item For $\mu = 1$: Conducting a \textbf{natural gradient ascent} on the normal distribution $\normal$ (independent of the given coordinate system).
% \end{itemize}

%\end{vbframe}

%\begin{vbframe}{Updating $C$: CMA - Cumulation}
%\enquote{Cumulation} as a widely used technique and known under various names (\textit{exponential smoothing} in forecasting and time series, exponentially weighted \textit{moving average}, \textit{iterate averaging} in stochastic approximation, etc.).
%
%\lz
%
%Using cumulation / an evolution path for the rank-one update of the covariance matrix reduces the number of function evaluations to adapt to a straight ridge from about $\order(d^2)$ to $\order(d)$.
%
%\lz
%
%For the evolution/search path taken over a number of generation steps an exponentially weighted sum of steps $\yv_w$ is used:
%
%\begin{eqnarray*}
%\mathbf{p}_c \propto \sum_{t = 0}^T \underbrace{(1-c_{\mathbf{C}})^{T-i}}_{\substack{\text{exponentially} \\ \text{fading weights}}} \yv_w^{[t]}
%\end{eqnarray*}
%
%\framebreak
%
%Cumulation as \textit{recursive construction of the evolution path}:
%
%\begin{eqnarray*}
%\mathbf{p}_c^{[t+1]} = \underbrace{(1- c_{\mathbf{C}})}_{\text{decay factor}} \mathbf{p}_c^{[t]} + \underbrace{\sqrt{1-(1-c_{\mathbf{C}})^2}}_{\text{normalization factor}} \sqrt{\mu_w} \underbrace{\yv_w^{[t]}}_{input},
%\end{eqnarray*}
%
%where $\yv_{w}^{[t]} = \frac{\mathbf{m}^{[t+1]} - \mathbf{m}^{[t]}}{\sigma^{[t]}}$ and $\mu_w = \frac{1}{\sum w_i^2}$, $c_{C} << 1$. %FIXME: \ll
%History information is accumulated in the evolution path.
%
%\begin{figure}
%  \includegraphics[width=0.8\textwidth, height=0.4\textheight]{figure_man/cmaes/cmaes_cumulation_1.png}
%\end{figure}
%
%\framebreak
%
%
%$\yv_w \yv_w^\top$ was used for updating $\mathbf{C}$ and because $\yv_w \yv_w^\top = -\yv_w (-\yv_w)^\top$ the sign of $\yv_w$ is lost.
%
%\lz
%
%The \textbf{sign information} (signifying correlation between steps) is (re-)introduced by using the \textit{evolution path}.
%
%\begin{align*}
%\mathbf{p}_c^{[t+1]} &= \underbrace{(1- c_{\mathbf{C}})}_{\text{decay factor}} \mathbf{p}_{\mathbf{C}}^{[t]} + \underbrace{\sqrt{1-(1-c_{\mathbf{C}})^2}}_{\text{normalization factor}} \sqrt{\mu_w} \yv_w^{[t]} \\
%\mathbf{C}^{[t+1]} &= (1- c_{cov}) \mathbf{C}^{[t]} + c_{cov} \underbrace{\mathbf{p}_c^{[t+1]} (\mathbf{p}_c^{[t+1]})^\top}_{\text{rank-one}},
%\end{align*}
%
%where $\mu_w = \frac{1}{\sum w_i^2}, c_{cov} << c_{\mathbf{C}} << 1$, such that $1/{c_{\mathbf{C}}}$ is the \enquote{backward time horizon}. %FIXME: \ll
%
%\framebreak
%
%\begin{align*}
%\textcolor{green}{\mathbf{p}_c^{[t+1]}} &= \underbrace{(1- \textcolor{blue}{c_{\mathbf{C}}})}_{\text{decay factor}} \textcolor{green}{\mathbf{p}_{\mathbf{C}}^{[t]}} + \underbrace{\sqrt{1-(1-\textcolor{blue}{c_{\mathbf{C}}})^2}}_{\text{normalization factor}} \sqrt{\mu_w} \yv_w^{[t]} \\
%\textcolor{green}{\mathbf{C}^{[t+1]}} &= (1- \textcolor{blue}{c_{cov}})\textcolor{green}{\mathbf{C}^{[t]}} + \textcolor{blue}{c_{cov}} \underbrace{\textcolor{green}{\mathbf{p}_c^{[t+1]} (\mathbf{p}_c^{[t+1]}})^\top}_{\text{rank-one}},
%\end{align*}
%\vspace{-10pt}
%
%where $\mu_w = \frac{1}{\sum \textcolor{blue}{w_i}^2}, \textcolor{blue}{c_{cov}} << \textcolor{blue}{c_{\mathbf{C}}} << 1$, such that $1/{c_{\mathbf{C}}}$ is the \enquote{backward time horizon}. %FIXME: \ll
%
%\begin{figure}
%  \includegraphics[width=0.8\textwidth, height=0.37\textheight]{figure_man/cmaes/cmaes_cumulation_2.png}
%\end{figure}
%
%\end{vbframe}

%
%\begin{vbframe}{Updating $C$: CMA - Rank-$\mu$ Update}
%In case of \textit{large population sizes $\lambda$} the \textbf{rank-$\mu$ update} extends the update rule using $\mu > 1$ vectors to update$\mathbf{C}$ at each generation step.
%
%\lz
%
%The weighted empirical covariance matrix computes a weighted mean of the outer products of the best $\mu$ steps and has rank $\min(\mu, d)$ with probability 1.
%
%\vspace{-10pt}
%
%\begin{eqnarray*}
%\mathbf{C}_\mu^{[t+1]} = \sum_{i=1}^\mu \bm{w}_i \xv_{i:\lambda}^{[t+1]} (\xv_{i:\lambda}^{[t+1]})^\top
%\end{eqnarray*}
%
%The rank-$\mu$-update then reads
%\begin{eqnarray*}
%\mathbf{C}^{[t+1]} = (1-c_{cov}) \mathbf{C}^{[t]} + c_{cov} \mathbf{C}_{\mu}^{[t+1]},
%\end{eqnarray*}
%
%where $c_{cov} \approx \mu_w/d^2$ and $c_{cov} \leq 1$.
%
%\framebreak
%
%\begin{figure}
%  \includegraphics[width=1\textwidth, height=0.3\textheight]{figure_man/cmaes/cmaes_rankmu.png}
%\end{figure}
%
%\begin{enumerate}
%  \item Sampling $\lambda = 150$ solutions, where $\mathbf{C}^{[t]} = \bm{\I}$ and $\sigma^{[t]} = 1$.
%  \item[] $\xv^{[t+1](i)} = \mathbf{m}^{[t]} + \sigma^{[t]} \normal(\bm{0}, \mathbf{C}^{[t]})$
%  \item Calculation $\mathbf{C}$, where $\mu = 50$, $w_1 = \dots = w_{\mu} = 1/\mu$ and $c_{cov}=1$
%  \item[] $\mathbf{C}_{\mu}^{[t+1]} = 1/\mu \sum \xv_{1:\lambda}^{[t]} (\xv_{1:\lambda}^{[t]})^\top$ and $\mathbf{C}^{[t+1]} = (1-1) \times \mathbf{C}^{[t]} + 1 \times \mathbf{C}_{\mu}^{[t+1]}$
%  \item New distribution
%  \item[] $\mathbf{m}^{[t+1]} = \mathbf{m}^{[t]} + 1/\mu \sum \xv_{1:\lambda}^{[t]}$.
%\end{enumerate}
%
%\end{vbframe}
%
%
%\begin{vbframe}{Updating $C$: Rank-One and Rank-$\mu$ Update}
%\textbf{Rank-one update}
%
%\begin{itemize}
%\item Uses the evolution path
%\item Can reduce the number of \textit{function evaluations} to adapt to straight ridges from about $\order(d^2)$ to $\order(d)$.
%\end{itemize}
%
%\textbf{Rank-$\mu$ update}
%
%\begin{itemize}
%\item Increases the learning rate in large populations and therefore the primary mechanism for large populations (thumb rule: $\lambda \geq 3d + 10$)
%\item Can reduce the number of \textit{generations} from about $\order(d^2)$ to $\order(d)^{(12)}$, given $\mu_w\propto \lambda \propto d$.
%\end{itemize}
%
%
%\textbf{Hybrid Update}: rank-one and rank-$\mu$ update can be combined.
%\end{vbframe}



\begin{vbframe}{Updating $\sigma$: Methods Step-Size Control}
\begin{itemize}
\setlength\itemsep{1.0em}
\item \textbf{$1/5$-th success rule}: increases the step-size if more than 20 \% of the new solutions are successful, decrease otherwise
\item \textbf{$\sigma$-self-adaptation}: mutation is applied to the step-size and the better - according to the objective function value - is selected
\item \textbf{Path length control via cumulative step-size adaptation (CSA)}\\ Intuition:
\begin{itemize}
    \item Short cumulative step-size $\triangleq$ steps cancel $\to$ decrease $\sigma^{[t+1]}$ 
    \item Long cumulative step-size $\triangleq$ corr. steps $\to$ increase $\sigma^{[t+1]}$ 
\end{itemize}
\vspace{0.3cm}
\begin{center}
\includegraphics[width=0.7\textwidth]{figure_man/cmaes/cumulative-step-size.png}
\end{center}

%\item Alternative step-size adaptation mechanism: two-point step-size adaptation, median success rule, population success rule.
\end{itemize}
\end{vbframe}

%\begin{vbframe}{Updating $\sigma$: Path Length Control (CSA)}
%Measure the length of the evolution path with informal steps:
%\begin{itemize}
%\item perpendicular under random selection (in expectation)
%\item perpendicular in the desired solution (to be most effective)
%\end{itemize}
%
%\begin{figure}
%  \includegraphics[width=1\textwidth, height=0.33\textheight]{figure_man/cmaes/cmaes_path.png}
%\end{figure}
%
%Pathway of mean vector $\mathbf{m}$ in the generation sequence of above pictures
%\begin{enumerate}
%\item decreases $\sigma$ as single steps cancel each other off
%\item ideal case as single steps are uncorrelated
%\item increases $\sigma$ as single steps point in same direction
%\end{enumerate}
%
%\framebreak
%
%Initialize $\mathbf{m} \in \R^d$, $\sigma \in \R_+$, evolution path $p_\sigma = \bm{0}$.
%
%Set $c_\sigma \approx 4/d$, $d_\sigma \approx 1$.
%
%\begin{align*}
%\mathbf{m}^{[t+1]} &= \mathbf{m}^{[t]} + \sigma^{[t]} \yv_w^{[t]}\\
%\mathbf{p}_\sigma^{[t+1]} &= (1- c_\sigma) \mathbf{p}_\sigma^{[t]} + \underbrace{\sqrt{1-(1-c_\sigma)^2}}_{\text{accounts for } 1-c_\sigma} \underbrace{\sqrt{\mu_w}}_{\text{account for} w_i} \yv_w^{[t]} \\
%\sigma^{[t+1]} &= \sigma^{[t]} \times \underbrace{\exp\biggl( \frac{c_\sigma}{d_\sigma}\Bigl(\frac{||\mathbf{p}_\sigma^{[t+1]}||}{\E||\normal(\bm{0}, \bm{\I})||} - 1 \Bigl)\biggl)}_{>1 \Longleftrightarrow ||\mathbf{p}_\sigma|| \text{ is greater than its expectation}}
%\end{align*}
%
%\framebreak



% \begin{vbframe}{Updating $\sigma$: Path Length Control (CSA)}
% \begin{figure}
%   \includegraphics[width=1\textwidth, height=0.7\textheight]{figure_man/cmaes/cmaes_step-size.png}
% \end{figure}

% CSA effective and robust for $\lambda \leq n$.

% \end{vbframe}


% \begin{frame}{Updating $C$: CMA - Rank-One Update}
% \begin{eqnarray*}
% \mathbf{m} \leftarrow \mathbf{m} + \sigma \yv_w, \quad \yv_w = \sum_{i=1}^{\mu} w_i \yv_{i:\lambda}, \quad \yv_i\sim \normal_i(\bm{0}, \mathbf{C})
% \end{eqnarray*}

% \begin{figure}
% \begin{overprint}
% \centering
% \only<1>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_1.png}}

% Initial distribution with $\mathbf{C} = 1$.}
% \only<2>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_2.png}}

% Initial distribution with $\mathbf{C} = 1$.}
% \only<3>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_3.png}}

% Movement to the population mean $m$ (disregarding $\sigma$) with $\yv_w$.}
% \only<4>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_4.png}}

% Blue circle as a mixture of $\mathbf{C}$ and step $\yv_w$: $\mathbf{C} \leftarrow 0.8\times \mathbf{C} + 0.2 \times \yv_w \yv^\top$.}
% \only<5>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_5.png}}

% Movement towards the new distribution (disregarding $\sigma$).}
% \only<6>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_6.png}}

% New Distribution (disregarding $\sigma$).}
% \only<7>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_7.png}}

% Movement to the population mean $\mathbf{m}$.}
% \only<8>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_8.png}}

% Green circle as a mixture of $\mathbf{C}$ and step $\yv_w$: $\mathbf{C} \leftarrow 0.8\times \mathbf{C} + 0.2 \times \yv_w \yv^\top$.}
% \only<9>{\scalebox{0.6}{\includegraphics[width=1.5\textwidth, height=0.75\textheight]{figure_man/cmaes_rankone_9.png}}

% Movement towards the new distribution (disregarding $\sigma$).}
% \end{overprint}
% \end{figure}
% \end{frame}


\endlecture
\end{document}

