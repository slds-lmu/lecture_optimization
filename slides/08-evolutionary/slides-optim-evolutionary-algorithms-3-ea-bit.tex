\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math.tex}
\input{../../latex-math/basic-ml.tex}
\input{../../latex-math/optim-basics.tex}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
Evolutionary Algorithms
}{
GA / Bit Strings
}{
figure_man/var-selection2.png
}{
\item Recombination
\item Mutation 
\item Simple examples
}


\begin{framei}{Binary encoding}
\item In theory: Each problem can be encoded binary
\item In practice: Binary not always best representation (e.g., if values are numeric, trees or programs)
\spacer
\item We typically encode problems with \textbf{binary decision variables} in binary representation.
\spacer
\item[] \textbf{Examples:}
\item Scheduling problems
\item Integer / binary linear programming
\item Feature selection 
\item ... 
\end{framei}


\begin{framev}[fs=small]{Recombination for bit strings}
Two individuals $\xv,\tilde{\xv} \in \{0, 1\}^d$ encoded as bit strings can be recombined as follows:
\begin{itemize}
\item \textbf{1-point crossover:}
Select crossover $k \in \{1, ..., d - 1\}$ randomly.
Take first $k$ bits from parent 1 and last $d-k$ bits from parent 2.
\begin{center}
\begin{tabular}{c @{\hspace{2\tabcolsep}} *{6}{c}}
\textcolor{red}{1} & \textcolor{blue}{1}  & & \textcolor{red}{1}  \\
\textcolor{red}{0} & \textcolor{blue}{0}  & &  \textcolor{red}{0}  \\ \cmidrule{1-4}
\textcolor{red}{0} & \textcolor{blue}{1}  &$\Rightarrow$ & \textcolor{blue}{1}  \\
\textcolor{red}{1} & \textcolor{blue}{1}  & &   \textcolor{blue}{1}  \\
\textcolor{red}{1} & \textcolor{blue}{0}  & &   \textcolor{blue}{0}
\end{tabular}
\end{center}
\item \textbf{Uniform crossover:}
Select bit $j$ with probability $p$ from parent 1 and $1-p$ from parent 2.
\begin{center}
\begin{tabular}{c @{\hspace{2\tabcolsep}} *{6}{c}}
\textcolor{red}{1} & \textcolor{blue}{0}  & & \textcolor{red}{1}  \\
\textcolor{red}{0} & \textcolor{blue}{0}  & &  \textcolor{blue}{0}  \\
\textcolor{red}{0} & \textcolor{blue}{1}  &$\Rightarrow$ & \textcolor{blue}{1}  \\
\textcolor{red}{0} & \textcolor{blue}{1}  & &   \textcolor{blue}{1}  \\
\textcolor{red}{1} & \textcolor{blue}{0}  & &   \textcolor{red}{1}
\end{tabular}
\end{center}
\end{itemize}
\end{framev}


\begin{framev}{Mutation for bit strings}
Offspring $\xv \in \{0, 1\}^d$ encoded as a bit string can be mutated as follows:
\spacer
\begin{itemize}
\item \textbf{Bitflip:} Each bit $j$ is flipped with probability $p \in (0,1)$.
\end{itemize}
\begin{center}
\begin{tabular}{c @{\hspace{2\tabcolsep}} *{5}{c}}
\\[1ex]
1  &               & \textcolor{red}{0}  \\
0  &               & 0  \\
0  & $\Rightarrow$ & 0  \\
0  &               & \textcolor{red}{1}  \\
1  &               & 1
\end{tabular}
\end{center}
\end{framev}


\begin{framei}{Example 1: One-Max Example}
\item $\xv \in \{0, 1\}^d, d = 15$ bit vector representation.
\item \textbf{Goal:} Find the vector with the maximum number of 1's. 
\item Fitness: $f(\xv) = \sum_{i = 1}^d x_i$
\item $\mu = 15$, $\lambda = 5$, $(\mu + \lambda)$-strategy, bitflip mutation, no recombination
\vfill
\imageC[0.65]{figure_man/one_max_example.pdf}
{\footnotesize \textbf{Green:} Representation of best individual per iteration. Right scale shows fitness.}
\end{framei}


\begin{framev}[fs=small]{Example 2: Feature selection}
We consider the following toy setting:
\begin{itemize}
\item Generate design matrix $\Xmat \in \R^{n \times p}$ by drawing $n = 1000$ samples of $p = 50$ independent normally distributed features with $\mu_j = 0$ and $\sigma_j^2 > 0$ varying between 1 and 5 for $j = 1, \dots, p$.
\item Linear regression problem with dependent variable $\yv$:
$$
\yv = \Xmat \thetav + \eps
$$
with $\eps \sim \mathcal N(0, 1)$.
\spacer
Parameter $\thetav$:
\begin{align*}
\theta_0 &= - 1.2 \\
\theta_j &= \begin{cases}
1 & \text{for $j \in {1, 7, 13, 19, 25, 31, 37, 43}$} \\
0 & \text{otherwise}
\end{cases}
\end{align*}
$\Rightarrow$ Only 8 out of 50 equally influential features
\end{itemize}
% Data generation and EA run: rsrc/var-selection.R
\end{framev}


\begin{framei}{Example 2: Feature selection}
\item \textbf{Aim:} Find influential features
\item \textbf{Encoding:} $\zv \in \{0, 1\}^p$, $z_j = 1$ means $\theta_j$ included in model
\item \textbf{Fitness function} $f(\zv)$: BIC of the model belonging to $\zv$
\item \textbf{Mutation:} Bit flip with $p = 0.3$
\item \textbf{Recombination:} Uniform crossover with $p=0.5$
\item \textbf{Survival selection:} $(\mu + \lambda)$ strategy with $\mu = 100$ and $\lambda =50$
\vfill
\imageC[1]{figure_man/example3.png}
\end{framei}


\begin{framev}{Example 2: Feature selection}
\imageC[1]{figure_man/var-selection1.png}
\end{framev}


\begin{framev}{Example 2: Feature selection}
\imageC[1]{figure_man/var-selection2.png}
\end{framev}

\endlecture
\end{document}
