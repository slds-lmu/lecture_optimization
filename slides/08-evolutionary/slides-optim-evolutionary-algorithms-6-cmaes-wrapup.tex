\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/cmaes/cmaes_muds_1.png}
\newcommand{\learninggoals}{
\item Advantages \& Limitations
\item IPOP-CMA-ES
\item Benchmark
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{CMA-ES Wrap Up}
\lecture{Optimization in Machine Learning}
\sloppy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{vbframe}{CMA-ES: Wrap Up}

\begin{algorithm}[H]
  \begin{footnotesize}
  \begin{center}
  \caption{CMA-ES}
    \begin{algorithmic}[1]
    \State Input: $m \in \R_n$, $\sigma \in \R_+$, $\lambda$ (problem dependent)
    \State Initialize: $\bm{C} = \bm{\I}$, $\bm{p}_{\bm{c}} = \bm{0}$, $\bm{p}_{\sigma} = \bm{0}$
    \State Set: $c_{\bm{C}} \approx 4/d$, $c_{\sigma} \approx 4/d$, $c_1 \approx 2/d^2$, $c_{\mu} \approx \mu_w/d^2$, $c_1+c_{\mu} \leq 1$, $d_{\sigma} \approx 1+\sqrt{\mu_w/d}$ and $w_{i=1,\dots, \lambda}$ such that $\mu_w = \frac{1}{\sum_{i=1}^{\mu} w_i^2} \approx 0.3 \lambda$
    \While{not terminate}
      \State $\xv^{(k)} = \bm{m} + \sigma \normal_i(\bm{0}, \bm{C}) \quad \text{for } i=1,\dots, \lambda$ \quad \textit{Sampling}
      \State $\bm{m} \leftarrow m + \sigma \bm{y}_w, \quad \text{where } \bm{y}_w = \sum_{i=1}^{\mu} w_i \xv_{i:\lambda}$ \quad \textit{Update mean}
      \State $\bm{p}_{\bm{C}} \leftarrow (1-c_{\bm{C}})\bm{p}_{\bm{C}} + \one_{\{||\bm{p}_{\sigma}|| < 1.5\sqrt{n}\}}\sqrt{1-(1-c_{\bm{C}})^2} \sqrt{\mu_w} \bm{y}_w$ \quad \textit{Cumulation of $\bm{C}$}
      \State $\bm{p}_{\sigma} \leftarrow (1-c_{\bm{C}})\bm{p}_{\bm{C}} + \sqrt{1-(1-c_{\sigma})^2} \sqrt{\mu_w} \bm{C}^{-\frac{1}/{2}} \bm{y}_w$ \quad \textit{Cumulation of $\sigma$}
      \State $\bm{C} \leftarrow (1-c_1-c_{\mu}) \bm{C} + c_1 \bm{p}_{\bm{C}} \bm{p}_{\bm{C}}^\top + c_{\mu} \sum_{i=1}^{\mu} w_i \xv_{i:\lambda} \xv_{i:\lambda}^\top$ \quad \textit{Update $\bm{C}$}
      \State $\sigma \leftarrow \sigma \times \exp\biggl( \frac{c_\sigma}{d_\sigma}\Bigl(\frac{||\bm{p}_\sigma||}{\E||\normal(\bm{0}, \bm{\I})||} - 1 \Bigl)\biggl)$ \quad \textit{Update $\sigma$}
    \EndWhile
    \end{algorithmic}
    \end{center}
    \end{footnotesize}
\end{algorithm}

% \framebreak
% {\tiny
% \begin{align*}
% \text{\textbf{Sampling}} \quad & \xv_i^{(g+1)} \sim \bm{m}^{(g)} + \sigma \normal(\bm{0}, \bm{C}^{(g)} \quad \text{for } i = 1,\dots, \lambda\\
% \text{\textbf{Evaluation}} \quad & \text{Calculate fitness of all } \lambda \text{ individuals and sort them}\\
% \text{\textbf{Selection}} \quad & \bm{m}^{(g+1)} = \sum_{i =1}^{\mu} w_i\xv_{i:\lambda}^{(g+1)}, \\
% & \text{where } \sum_{i=1}^{\mu} w_i = 1, \quad w_1 \geq w_2 \geq \dots \geq w_{\mu} > 0\\[2em]
% \text{\textbf{Recombination,Adaptation}}  \quad & \bm{C}^{(g+1)} = (1-c_{cov})\bm{C}^{(g)} + \frac{c_{cov}}{\mu_{cov}} \overbrace{\bm{p}_c^{(g+1)}{\bm{p}_c^{(g+1)}}^\top}^{\text{rank-one update}} \\
% & + c_{cov} \Bigl( 1 - \frac{1}{\mu_{cov}}\Bigl) \times \underbrace{\sum_{i = 1}^{\mu} w_i\bm{y}_{i:\lambda}^{(g+1)} \bigl(w_i\bm{y}_{i:\lambda}^{(g+1)}\bigl)^\top}_{\text{rank-$mu$ update}}, \\
% & \sigma^{(g+1)} = \sigma^{(g)} \exp\biggl( \frac{c_\sigma}{d_\sigma}\Bigl(\frac{||\bm{p}_\sigma^{(g+1)}||}{\E||\normal(\bm{0}, \bm{\I})||} - 1 \Bigl)\biggl)
% \end{align*}
% }%

\end{vbframe}


\begin{vbframe}{CMA-ES: Wrap Up - Default Values} % Parameter
\scriptsize{
Related to selection and recombination:

\begin{itemize}
\item $\bm{\lambda}$: offspring number, population size \textcolor{blue}{$\quad 4 + \lfloor 3 \ln d\rfloor$}
\item $\bm{\mu}$: parent number, solutions involved in mean update \textcolor{blue}{$\quad \lfloor \lambda / 2\rfloor$}
\item $\bm{w_i}$: recombination weights (preliminary convex shape) \textcolor{blue}{$\quad \ln \frac{\lambda + 1}{2} - \ln i, \text{ for } i = 1, \dots, \lambda$}
\end{itemize}

Related to $\bm{C}$-update:

\begin{itemize}
\item $\bm{1-c_{\bm{C}}}$: decay rate for evolution path, cumulation factor \textcolor{blue}{$\quad 1- \frac{4+\mu_{eff}/d}{d+4+2\mu_{eff}/d}$}
\item $\bm{c_1}$: learning rate for rank-one update of $\bm{C}$ \textcolor{blue}{$\quad \frac{2}{(d+1.3)^2+\mu_{eff}}$}
\item $\bm{c_{\mu}}$: learning rate for rank-$\mu$ update of $\bm{C}$ \textcolor{blue}{$\quad \min\Bigl(1-c_1, 2\cdot \frac{\mu_{eff}-2+1/\mu_{eff}}{(d+2)^2+\mu_{eff}}\Bigl)$}
\end{itemize}

Related to $\sigma$-update:

\begin{itemize}
\item $\bm{1-c_{\sigma}}$: decay rate for evolution path \textcolor{blue}{$\quad 1- \frac{\mu_{eff}+2}{d+\mu_{eff}+5}$}
\item $\bm{d_{\sigma}}$: damping for $\sigma$-change \textcolor{blue}{$\quad 1+2 \max\Bigl(0, \sqrt{\frac{\mu_{eff}-1}{d+1}}-1\Bigl) + c_\sigma$}
\end{itemize}

with $\mu_{eff}=\bigl(\frac{||\bm{w}||_1}{||\bm{w}||_2}\bigl)=\frac{(\sum_{i=1}^\mu |w_i|)^2}{\sum_{i=1}^\mu w_i^2} = \frac{1}{\sum_{i=1}^\mu w_i^2}$ and \textcolor{blue}{typical default parameter values}.

$\mu_{eff}$ introduced for a general framework such that even negative weights can be taken into account (for the remaining $\lambda-\mu$ points).

% The default values depend only on the dimension. In the first place they do not depend on the objective function.
}
\end{vbframe}


\begin{frame}{CMA-ES: Wrap Up}
\begin{figure}
\begin{overprint}
\centering
\only<1>{\includegraphics[width=0.95\textwidth, height=0.85\textheight]{figure_man/cmaes/cmaes_muds_1.png}}
\only<2>{\includegraphics[width=0.95\textwidth, height=0.85\textheight]{figure_man/cmaes/cmaes_muds_2.png}}
\only<3>{\includegraphics[width=0.95\textwidth, height=0.85\textheight]{figure_man/cmaes/cmaes_muds_3.png}}
\only<4>{\includegraphics[width=0.95\textwidth, height=0.85\textheight]{figure_man/cmaes/cmaes_muds_4.png}}
\end{overprint}
\end{figure}
\end{frame}


\begin{vbframe}{CMA-ES: Wrap Up - Advantages}
CMA-ES can outperform other strategies in following cases:

\begin{itemize}
\item Non-separable problems (parameters of the objective function are dependent)
\item Derivative of the objective function is not available
\item High dimensional problems (large $d$)
\item Very large search space
\end{itemize}
\end{vbframe}


\begin{vbframe}{CMA-ES: Wrap Up - Limitations}
CMA-ES can be outperformed by other strategies in following cases:

\begin{itemize}
\item Partly separable problems (i.e. optimization of $n$-dimensional objective function can be divided into a series of $d$ optimizations of every single parameter)
\item Derivative of the objective function is easily available $\rightarrow$ Gradient Descend / Ascend
\item Low dimensional problems (large $d$)
\item Problems that can be solved by using a relative small number of function evaluations (e.g. $< 10 d$ evaluations)
\end{itemize}
\end{vbframe}


\begin{vbframe}{CMA-ES: IPOP}
\footnotesize{
\begin{itemize}
\item Many special forms and extensions of the \enquote{basic} CMA-ES exist
\item CMA-ES efficiently minimizes unimodal objective functions and is in particular superior on ill-conditioned, non-separable problems
\item Default population size $\lambda_{default}$ has been tuned for unimodal
functions and however can get stuck in local optima on multi-modal functions, such that convergence to global optima is not guaranteed
\item It could be shown that increasing the population size improves the performance of the CMA-ES on multi-modal functions
\item \textbf{IPOP-CMA-ES} is a special form of restart-CMA-ES, where the \textit{population size is increased for each restart} (IPOP)
\item By increasing the population size the search characteristic becomes more global after each restart
\item For the restart strategy CMA-ES is stopped whenever one stopping criterion (not further discussed here) is met, and an independent restart is launched with the population size increased by a factor of 2 (values between 1.5 and 5 are reasonable).
\end{itemize}
}
\end{vbframe}


\begin{vbframe}{CMA-ES: Wrap Up - Benchmark EAs}
\footnotesize{
\textit{Example:} Black-box optimization of 25 benchmark functions under thoroughly defined experimental and recording conditions for the 2005 IEEE Congress on Evolutionary Computation: Session on Real-Parameter Optimization.

17 papers were submitted, 11 were accepted, thereunder hybrid methods.

\lz

\textit{Two of the Algorithms:}
\begin{itemize}
\item L-CMA-ES (Auger and Hansen. 2005a): A CMA evolution strategy with small population size and small initial step-size to emphasize on local search characteristics. Independent restarts are conducted until the target function value is reached or the maximum number of function evaluations is exceeded.
% Auger and Hansen. A Restart CMA Evolution Strategy With Increasing Population Size. 2005a
\item G-CMA-ES (Auger and Hansen. 2005b): A CMA evolution strategy restarted with increasing population size (IPOP). Independent restarts are conducted with increasing population size until the target function value is reached or the maximum number of function evaluations is exceeded. With the initial small population size the algorithm converges fast, with the succeeding larger population sizes the global search performance is emphasized in subsequent restarts.
% G-CMA-ES (Auger and Hansen. Performance Evaluation of an Advanced Local Search Evolutionary Algorithm. 2005b)
\end{itemize}
}

\framebreak

\begin{figure}
\includegraphics[width=1\textwidth, height=0.55\textheight]{figure_man/cmaes/cmaes_benchmark.png}
\end{figure}

\vspace{-15pt}
\scriptsize{
\begin{itemize}
\item Comparison of performance results from 11 algorithms for search space dimension 10 and 30 on different function subsets
\item Expected number of function evaluations (FEs) to reach the target function value is normalized by the value of the best algorithm on the respective function $\text{FEs}_{best}$
\item Calculation of the empirical cumulative distribution function of FEs / $\text{FEs}_{best}$ for each algorithm over different sets of functions in 10 and 30D
\item Small values for FEs / $\text{FEs}_{best}$ and therefore large values of the graphs are preferable.
\end{itemize}
}
\end{vbframe}

\endlecture
\end{document}

