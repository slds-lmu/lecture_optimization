\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
Nonlinear Programs
}{
Solvers
}{
figure_man/convex_programs.png
}{
\item Sequential quadratic programming
\item Penalty methods
\item Barrier / interior-point methods
\item Constrained optimization in R
}

\begin{framei}{Sequential quadratic programming}
\item For simplification, consider only equality constraints:
$$
\min f(\xv) \qquad \text{s.t. } \quad h(\xv) = 0
$$
\item[Idea:] Instead of $f$, optimize 2nd order Taylor approximation at point $\bm{\tilde x}$
$$
\tilde f(\xv) = f(\bm{\tilde x}) +  \nabla_{x} f(\bm{\tilde x})^T (\xv- \bm{\tilde x})+ \frac{1}{2} (\xv- \bm{\tilde x})^T \nabla^2_{xx} f(\bm{\tilde x}) (\xv- \bm{\tilde x})
$$
\item $h$ is also replaced by its linear approximation at $\bm{\tilde x}$
$$
\tilde h(\xv) = h(\bm{\tilde x}) + \nabla h(\bm{\tilde x})^T(\xv- \bm{\tilde x})
$$
\end{framei}


\begin{framei}{Sequential quadratic programming}
\item With $\bm{d} := (\xv- \bm{\tilde x})$ we formulate the \textbf{quadratic auxiliary problem}
\begin{align*}
\min_{\bm{d}} \quad & \tilde f(\bm{d}) := f(\bm{\tilde x}) + \bm{d}^T \nabla_{x} f(\bm{\tilde x}) + \frac{1}{2} \bm{d}^T \nabla^2_{xx} f(\bm{\tilde x}) \bm{d} \\
\text{s.t. } \quad & \tilde h(\bm{d}) :=  h(\bm{\tilde x}) + \nabla h(\bm{\tilde x})^T\bm{d} = 0
\end{align*}
\item Even if no optimality conditions can be formulated for the actual problem, KKT conditions apply at the optimum of this problem
\item If $\nabla^2_{xx} f(\xv)$ is positive semidefinite, it is a \textbf{convex optimization problem}
\end{framei}


\begin{framei}[fs=small]{Sequential quadratic programming}
\item Using the Lagrange function
$$
L(\bm{d}, \bm{\beta}) = \bm{d}^T \nabla_{x} f(\bm{\tilde x}) + \frac{1}{2} \bm{d}^T \nabla^2_{xx} f(\bm{\tilde x}) \bm{d} + \bm{\beta}^T (h(\bm{\tilde x}) + \nabla h(\bm{\tilde x})^T\bm{d})
$$
we formulate the KKT conditions
\begin{itemize}
\item $\nabla_{\bm{d}} L(\bm{d}, \bm{\beta}) = \nabla_{x} f(\bm{\tilde x}) + \nabla^2_{xx} f(\bm{\tilde x}) \bm{d} + \nabla h(\bm{\tilde x})^T\bm{\beta} = 0$
\item $h(\bm{\tilde x}) + \nabla h(\bm{\tilde x})^T\bm{d} = 0$
\end{itemize}
\item Or in matrix notation
$$
\begin{pmatrix} \nabla^2_{xx} f(\bm{\tilde x}) & \nabla h(\bm{\tilde x})^T \\ \nabla h(\bm{\tilde x}) & 0 \end{pmatrix} \begin{pmatrix} \bm{d} \\ \bm{\beta} \end{pmatrix} = - \begin{pmatrix} \nabla_{x} f(\bm{\tilde x}) \\ h(\bm{\tilde x}) \end{pmatrix}
$$
\item The \textbf{quadratic subproblem} can be traced back to solving a linear system
\end{framei}


\begin{frame}{SQP algorithm}
\begin{algorithm}[H]
\caption{SQP for problems with equality constraints}
\begin{algorithmic}[1]
\State Select a feasible starting point $\mathbf{x}^{(0)} \in \R^n$
\While {Stop criterion not fulfilled}
\State Solve quadratic subproblem by solving the equation
$$\begin{pmatrix} \nabla^2_{xx} L(\xv, \bm{\mu}) & \nabla h(\xv)^T \\ \nabla h(\xv) & 0 \end{pmatrix} \begin{pmatrix} \bm{d} \\ \bm{\beta} \end{pmatrix} = - \begin{pmatrix} \nabla_{x} L(\xv, \bm{\mu}) \\ h(\xv) \end{pmatrix}$$
\State Set $\xv^{(i + 1)} = \xv^{(i)} + \bm{d}$
\EndWhile
\end{algorithmic}
\end{algorithm}
\end{frame}


\begin{framei}{Penalty methods}
\item[Idea:] Replace constrained problem with a sequence of unconstrained problems using a \textbf{penalty function}
\item Instead of
$$
\min f(\xv) \qquad \text{s.t. } \quad h(\xv) = 0
$$
we look at the unconstrained problem
$$
\min_{\xv} p(\xv) = f(\xv) + \rho \frac{\|h(\xv)\|^2}{2}
$$
\item Under appropriate conditions, solutions for $\rho \to \infty$ converge to the solution of the original problem
\end{framei}


\begin{framei}{Barrier method}
\item[Idea:] Establish a ``barrier'' that penalizes if $\xv$ comes too close to the edge of the feasible set $\bm{S}$
\item For the problem
$$
\min f(\xv) \qquad \text{s.t. } \quad g(\xv) \le 0
$$
a common \textbf{barrier function} is
$$
B_\rho = f(\xv) - \rho \sum_{i = 1}^m \ln(-g_i(\xv))
$$
\item Penalty term becomes larger as $\xv$ approaches $0$, i.e., the limit of the feasible set
\item Under certain conditions, solutions of $\min B_\rho$ for $\rho\to 0$ converge to the optimum
\item Also called \textbf{interior-point method}
\end{framei}


\begin{framei}{Constrained optimization in R}
\item \texttt{optim(..., method = "L-BFGS-B")} uses quasi-Newton methods and can handle box constraints
\item \texttt{nlminb()} uses trust-region procedures and can also handle box constraints
\item \texttt{constrOptim()} can be used for linear inequality constraints and is based on interior-point methods
\item \texttt{nloptr} is an interface to NLopt, an open-source library for nonlinear optimization
\end{framei}

\endlecture
\end{document}
