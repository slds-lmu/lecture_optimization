\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/Weak_and_Strong_Duality.png}
\newcommand{\learninggoals}{
\item KKT conditions
\item Regularity conditions 
\item Examples 
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Nonlinear programs: Regularity conditions}
\lecture{\inserttitle}
\sloppy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{vbframe}{Stationary point of the Lagrangian}

When we introduced the Lagrangian $\mathcal{L}$ from a geometrical
perspective for the equality constraint problem, 
we realized that the geometrical conditions for the optimum
coincided with finding a stationary point of $\mathcal{L}$: 

$$
  \begin{pmatrix}
  \nabla_\xv \mathcal{L}(\xv^\ast, \beta) \\
  \nabla_\beta \mathcal{L}(\xv^\ast, \beta)
  \end{pmatrix} = \begin{pmatrix} \nabla f(\xv^\ast) + \beta \nabla h(\xv^\ast) \\
  h(\xv)
  \end{pmatrix} = \begin{pmatrix}
  0 \\ 0 \end{pmatrix}
$$

\lz

For this and the general Lagrangian, this leads to the following question.
\lz

\textbf{Question: } Is $\nabla L(\xv, \bm{\alpha}, \bm{\beta}) = 0$ a \textbf{necessary / sufficient condition for the optimum}?

\end{vbframe}

\begin{vbframe}{KKT Conditions}

In order to be able to formulate necessary and sufficient conditions for optimality, we need the \textbf{Karush-Kuhn-Tucker conditions} (KKT conditions).

\lz

A triple $(\xv, \bm{\alpha}, \bm{\beta})$ satisfies the KKT conditions if

  \begin{itemize}
    \item $\nabla_x L(\xv, \bm{\alpha}, \bm{\beta}) = 0$ (stationarity)
    \item $g_i(\xv) \le 0, h_j(\xv) = 0$ for all $i, j$ (primal feasibility)
    \item $\bm{\alpha} \ge 0$ (dual feasibility)
    \item $\alpha_i g_i(\xv) = 0$ for all $i$ (complementary slackness)
  \end{itemize}



  \framebreak


\textbf{Necessary condition for optimality}:

Let $\xv^*$ be a local minimum. If certain regularity conditions are fulfilled, there are $\bm{\alpha}^*, \bm{\beta}^*$ such that $(\xv^*, \bm{\alpha}^*, \bm{\beta}^*)$ fulfill the KKT conditions.

  \lz

  Under certain conditions, KKT conditions are also sufficient for optimality.

  \lz

  \textbf{Sufficient condition for optimality}:

  Given a \textbf{convex problem} ($f$ convex, $\mathcal{S}$ convex) and $(\xv^*, \bm{\alpha}^*, \bm{\beta}^*)$ satisfies the KKT conditions. Then $\xv^*$ is a global solution to the problem.


  % \begin{footenotesize}
  % \textbf{Beweisskizze:}
  %
  % \begin{eqnarrayÜ}
  % f(\xv^*) &\overset{\text{Dualität}}{=}& q(\bm{\alpha}, \bm{\beta}) &\overset{\text{Def.}}{=}& \min_{\xv} f(\xv) + \sum_{i=1}^k \alpha_i g_i(\xv) + \sum_{j=1}^l \beta_j h_j(\xv)
  % \\ &\le& f(\xv^*) + \sum_{i=1}^k \underbrace{\alpha_i g_i(\xv^*)}_{= 0 ~~ \forall~i} + \sum_{j=1}^l \beta_j \underbrace{h_j(\xv^*)}_{= 0 ~~ \forall~j} = f(\xv^*)
  % \end{eqnarray*}
  %
  % Daher muss für alle Ungleichheiten Gleichheit gelten. Wir sehen
  %
  % \begin{itemize}
  % \item Da $\bm
  % \end{itemize}


\end{vbframe}

\begin{vbframe}{Regularity conditions}

There are different regularity conditions (or constraint qualifications) that ensure that the KKT conditions apply (ACQ, LICQ, MFCQ, Slater condition, ...).

\lz

To be able to use the above results, at least one regularity condition must be examined to prove that the function behaves \enquote{regular}.

\lz

We do not go further into these regularity conditions here and refer to \url{https://docs.ufpr.br/~ademir.ribeiro/ensino/cm721/kkt.pdf}.

\end{vbframe}

\begin{vbframe}{Ridge regression}

The following two formulas are common for ridge regression:

\lz
\textbf{Formula 1:}
\begin{eqnarray}
  \min_{\bm{\theta}} && f_\lambda(\bm{\theta}) := \|\bm{y} - \mathbf{X}\bm{\theta}\|_2^2 + \lambda \|\bm{\theta}\|_2^2
\label{eq:form1}
\end{eqnarray}

\vspace*{-0.2cm}
\textbf{Formula 2:}
\begin{align}
\begin{split}
\min_{\bm{\theta}} & \quad \|\bm{y} - \mathbf{X}\bm{\theta}\|_2^2 \\
\text{s.t. } & \quad \|\bm{\theta}\|_2^2 - t \le 0
\end{split}
\label{eq:form2}
\end{align}



Why are these two formulas (for appropriate values $t, \lambda$) equivalent?

\framebreak

\textbf{Visualization:} see additional material

\lz

\begin{center}
\includegraphics[width = 0.8\textwidth]{figure_man/ridge_original.png} \\
Quadratic-Loss for the \texttt{cars} dataset without penalty.
\end{center}

\lz

\begin{center}
\includegraphics[width = 0.45\textwidth]{figure_man/ridge_formulation1.png} ~~ \includegraphics[width = 0.45\textwidth]{figure_man/ridge_formulation2.png} \\
Left: loss for Ridge regression with penalty term. Right: loss for ridge regression with corresponding constraint.
\end{center}


\framebreak

First, consider \eqref{eq:form1}. If $\bm{\theta}^*$ is our minimum, then the necessary condition applies.

$$
\nabla f_\lambda(\bm{\theta}^*) = - 2 \bm{y}^T\mathbf{X} + 2 (\bm{\theta}^*)^T \mathbf{X}^T\mathbf{X} + 2 \lambda (\bm{\theta}^*)^T = 0.
$$

We now show that we can find a $t$ so that $\bm{\theta}^*$ is also solution for \eqref{eq:form2}.

\lz

We calculate the Lagrange function of \eqref{eq:form2}

\vspace*{-0.5cm}
\begin{eqnarray*}
  L(\bm{\theta}, \alpha) &=& \|\bm{y} - \mathbf{X}\bm{\theta}\|_2^2 + \alpha (\|\bm{\theta}\|^2_2 - t).
\end{eqnarray*}

The first KKT condition (stationarity of the Lagrange function) is

$$
\nabla_\theta L(\bm{\theta}, \alpha)= - 2\bm{y}^T\mathbf{X} + 2 \bm{\theta}^T \mathbf{X}^T\mathbf{X} + 2 \alpha \bm{\theta}^T = 0.
$$

Since we know from \eqref{eq:form1} that $\nabla f_\lambda(\bm{\theta}^*) = 0$, this condition is fulfilled if we set $\bm{\theta} = \bm{\theta}^*$ and $\alpha = \lambda$.

\lz

However, complementary slackness must still apply for the KKT conditions.

$$
\alpha(\|\bm{\theta}\|^2_2 - t) = 0
$$

This is the case if we choose $t = \|\bm{\theta}^*\|^2$.

\lz

Vice versa it can be shown that a solution of \eqref{eq:form2} is a solution of \eqref{eq:form1} if we set $\lambda = \alpha$.



\framebreak


%
% Für die Optimallösung gilt also notwendigerweise $\|\bm{\theta}\|^2_2 = t$ und die erste KKT-Bedingung lautet dann $\nabla_\theta L(\bm{\theta}, \bm{\alpha}) = \nabla_\theta \frac{1}{2} \|\bm{y} - \xv\bm{\theta}\|^2 = \nabla_\theta f(x) = 0$.
%
% \framebreak
%
% Setzen wir in Problem 2 $\lambda = \alpha$ so erhalten wir
%
%




\end{vbframe}

\endlecture
\end{document}


