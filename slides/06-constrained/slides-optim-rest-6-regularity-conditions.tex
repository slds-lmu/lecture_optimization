\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/optim-basics}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
Nonlinear programs
}{
Regularity Conditions
}{
figure_man/Weak_and_Strong_Duality.png
}{
\item KKT conditions
\item Regularity conditions 
\item Examples 
}

\begin{framei}{Stationary point of the Lagrangian}
\item When we introduced the Lagrangian $\mathcal{L}$ from a geometrical perspective for the equality constraint problem, we realized that the geometrical conditions for the optimum coincided with finding a stationary point of $\mathcal{L}$: 
$$
\begin{pmatrix}
\nabla_{\xv} \mathcal{L}(\xvs, \beta) \\
\nabla_{\beta} \mathcal{L}(\xvs, \beta)
\end{pmatrix} = \begin{pmatrix} \nabla f(\xvs) + \beta \nabla h(\xvs) \\
h(\xv)
\end{pmatrix} = \begin{pmatrix}
0 \\ 0 \end{pmatrix}
$$
\item For the general Lagrangian, this leads to the following question:\\
\spacer
\item[] Is $\nabla L(\xv, \bm{\alpha}, \bm{\beta}) = 0$ a \textbf{necessary / sufficient condition for the optimum}?
\end{framei}


\begin{framei}[sep=L]{KKT Conditions}
\item To formulate necessary and sufficient conditions for optimality, we need the \textbf{Karush-Kuhn-Tucker conditions} (KKT conditions)
\item A triple $(\xv, \bm{\alpha}, \bm{\beta})$ satisfies the KKT conditions if
\begin{itemize}
\item $\nabla_x L(\xv, \bm{\alpha}, \bm{\beta}) = 0$ (stationarity)
\item $g_i(\xv) \le 0, h_j(\xv) = 0$ for all $i, j$ (primal feasibility)
\item $\bm{\alpha} \ge 0$ (dual feasibility)
\item $\alpha_i g_i(\xv) = 0$ for all $i$ (complementary slackness)
\end{itemize}
\end{framei}


\begin{framei}{KKT Conditions}
\item[\textbf{Necessary condition for optimality:}]
Let $\xv^*$ be a local minimum.\\
If certain regularity conditions are fulfilled, there are $\bm{\alpha}^*, \bm{\beta}^*$ such that $(\xv^*, \bm{\alpha}^*, \bm{\beta}^*)$ fulfill the KKT conditions
\vfill
\item Under certain conditions, KKT conditions are also sufficient for optimality
\vfill
\item[\textbf{Sufficient condition for optimality:}]
Given a \textbf{convex problem} ($f$ convex, $\mathcal{S}$ convex) and $(\xv^*, \bm{\alpha}^*, \bm{\beta}^*)$ satisfies the KKT conditions.\\
Then $\xv^*$ is a global solution to the problem
\end{framei}

\begin{framei}{Regularity conditions}
\item Different regularity conditions (or constraint qualifications) ensure that the KKT conditions apply (ACQ, LICQ, MFCQ, Slater condition, ...)
\item To use the above results, at least one regularity condition must be examined to prove that the function behaves ``regular''
\item We do not go further into these regularity conditions here
\end{framei}

\begin{framei}{Ridge regression}
\item The following two formulas are common for ridge regression:
\item[] \textbf{Formula 1:}
\begin{equation}
\min_{\thetav} \quad f_\lambda(\thetav) := \|\yv - \Xmat\thetav\|_2^2 + \lambda \|\thetav\|_2^2
\label{eq:form1}
\end{equation}
\item[] \textbf{Formula 2:}
\begin{equation}
\begin{aligned}
\min_{\thetav} & \quad \|\yv - \Xmat\thetav\|_2^2 \\
\text{s.t. } & \quad \|\thetav\|_2^2 - t \le 0
\end{aligned}
\label{eq:form2}
\end{equation}
\item Why are these two formulas (for appropriate values $t, \lambda$) equivalent?
\end{framei}


\begin{framei}[fs=small]{Ridge regression -- visualization}
\item \textbf{Visualization:} see additional material
\vfill
\imageC[0.50]{figure_man/ridge_original.png}
\item Quadratic loss for the \texttt{cars} dataset without penalty
\vfill
\splitVCC{
\imageC[0.9]{figure_man/ridge_formulation1.png}
}{
\imageC[0.9]{figure_man/ridge_formulation2.png}
}
\item Left: loss for ridge regression with penalty term\\
Right: loss for ridge regression with corresponding constraint
\end{framei}


\begin{framei}[fs=small]{Ridge regression -- equivalence}
\item Consider \eqref{eq:form1}. If $\thetav^*$ is our minimum, the necessary condition applies:
$$
\nabla f_\lambda(\thetav^*) = - 2 \yv^T\Xmat + 2 (\thetav^*)^T \Xmat^T\Xmat + 2 \lambda (\thetav^*)^T = 0
$$
\item We show that we can find a $t$ so that $\thetav^*$ is also solution for \eqref{eq:form2}
\item We calculate the Lagrange function of \eqref{eq:form2}:
$$
L(\thetav, \alpha) = \|\yv - \Xmat\thetav\|_2^2 + \alpha (\|\thetav\|^2_2 - t)
$$
\item The first KKT condition (stationarity) is:
$$
\nabla_\theta L(\thetav, \alpha)= - 2\yv^T\Xmat + 2 \thetav^T \Xmat^T\Xmat + 2 \alpha \thetav^T = 0
$$
\item Since $\nabla f_\lambda(\thetav^*) = 0$, this is fulfilled if we set $\thetav = \thetav^*$ and $\alpha = \lambda$
\end{framei}


\begin{framei}{Ridge regression -- equivalence}
\item However, complementary slackness must still apply for the KKT conditions:
$$
\alpha(\|\thetav\|^2_2 - t) = 0
$$
\item This is the case if we choose $t = \|\thetav^*\|^2$
\vfill
\item Vice versa it can be shown that a solution of \eqref{eq:form2} is a solution of \eqref{eq:form1} if we set $\lambda = \alpha$
\end{framei}

\endlecture
\end{document}
