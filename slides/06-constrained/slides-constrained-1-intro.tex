\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
Constrained Optimization
}{
Introduction
}{
figure_man/convex_programs.png
}{
\item Examples of constrained optimization in statistics and ML 
\item General definition
\item Hierarchy of convex constrained problems
}

\begin{framei}[fs=small]{Constrained Optimization in Statistics}
\item[Example:] Maximum Likelihood Estimation
\item For data $(\xv^{(1)}, \ldots, \xv^{(n)})$, we want to find the MLE
$$
\max_\theta L(\theta) = \prod_{i = 1}^n f(\xv^{(i)}, \theta)
$$
\item In some cases, $\theta$ can only take \textbf{certain values}
\vfill
\item If $f$ is a Poisson distribution, we require $\lambda \ge 0$
\item If $f$ is a multinomial distribution
$$
f(x_1, \ldots, x_p; n; \theta_1, \ldots, \theta_p) = \begin{cases} \frac{n!}{x_1! \cdots x_p!} \theta_1^{x_1} \cdots \theta_p^{x_p} & \text{if } x_1 + \ldots + x_p = n \\ 0 & \text{else}
\end{cases}
$$
\item The probabilities $\theta_i$ must lie between $0$ and $1$ and sum to $1$:
$$
0 \le \theta_i \le 1 \text{ for all } i \quad \text{and} \quad \theta_1 + \ldots + \theta_p = 1
$$
\end{framei}


\begin{framei}{Constrained Optimization in ML}
\item \textbf{Lasso regression}:
$$
\min_{\bm{\beta}\in \R^p} \frac{1}{n}\sum_{i = 1}^n\left(y^{(i)} - \bm{\beta}^\top\xv^{(i)}\right)^2 \quad \text{s.t. } \|\bm{\beta}\|_1 \le t
$$
\item \textbf{Ridge regression}:
$$
\min_{\bm{\beta}\in \R^p} \frac{1}{n}\sum_{i = 1}^n\left(y^{(i)} - \bm{\beta}^\top\xv^{(i)}\right)^2 \quad \text{s.t. } \|\bm{\beta}\|_2 \le t
$$
\vfill
\imageC{figure_man/lasso-ridge.png}
\end{framei}


\begin{framei}[fs=small]{Constrained Optimization in ML}
\item \textbf{Constrained Lasso regression}:
$$
\min_{\bm{\beta}\in \R^p} \frac{1}{n}\sum_{i = 1}^n\left(y^{(i)} - \bm{\beta}^\top\xv^{(i)}\right)^2 \quad \text{s.t. } \|\bm{\beta}\|_1 \le t, ~ \mathbf{C}\bm{\beta} \le \mathbf{d}, ~ \Amat\bm{\beta} = \mathbf{b}
$$
\item Matrices $\Amat\in \R^{l \times p}$ and $\mathbf{C} \in \R^{k \times p}$ have full row rank
\item This model includes many Lasso variants as special cases, e.g., the Generalized Lasso, (sparse) isotonic regression, log-contrast regression for compositional data, etc.
\furtherreading{GAINES2018}
\end{framei}


\begin{framei}[fs=small]{Constrained Optimization in ML}
\item Dual formulation of the SVM: convex QP with box constraints plus one linear constraint
$$
\max_{\boldsymbol{\alpha} \in \R^n} \sum_{i=1}^n \alpha_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_j\yi y^{(j)} \scp{\xi}{\xv^{(j)}}
$$
$$
\text{s.t. } 0 \le \alpha_i \le C, \quad \sum_{i=1}^n \alpha_i \yi = 0
$$
\end{framei}


\begin{framei}{Constrained Optimization}
\item[Definition] \textbf{Constrained Optimization problem}:
$$
\min f(\xv) \quad \text{s.t. } g_i(\xv) \le 0 ~ (i=1,\ldots,k), \quad h_j(\xv) = 0 ~ (j=1,\ldots,l)
$$
\item $g_i: \R^d \to \R$ are \textbf{inequality constraints}
\item $h_j: \R^d \to \R$ are \textbf{equality constraints}
\vfill
\item The \textbf{feasible set} is the set of inputs $\xv$ that fulfill the constraints:
$$
\mathcal{S} := \{\xv \in \R^d ~|~ g_i(\xv) \le 0, h_j(\xv) = 0 ~\forall~ i, j\}
$$
\end{framei}


\begin{framei}[fs=small]{Constrained Convex Optimization}
\item \textbf{Convex programs}: convex objective $f$, convex inequality constraints $g_i$, and affine equality constraints $h_j$ (i.e., $h_j(\xv) = \Amat_j^\top \xv - \mathbf{b}_j$)
\vfill
\imageC[0.4]{figure_man/convex_programs.png}
\vfill
\item \textbf{Linear program (LP)}: $f$ and all constraints $g_i, h_j$ are linear
\item \textbf{Quadratic program (QP)}: $f$ is a quadratic form, constraints are linear
$$
\fx = \frac{1}{2}\xv^\top \mathbf{Q} \xv + \mathbf{c}^\top \xv + d \quad \text{for } \mathbf{Q} \in \R^{d \times d}, \mathbf{c} \in \R^d, d \in \R
$$
\item Also: second-order cone programs (SOCP), semidefinite programs (SDP), cone programs (CP)
\end{framei}


\begin{framei}{Constrained Convex Optimization}
\item SOCPs play a pivotal role in statistics and engineering
\furtherreading{LOBO1998}
\item In ML, SDPs are at the heart of, e.g., learning kernels from data
\furtherreading{LANCKRIET2004}
\vfill
\item This categorization of convex optimization problem classes helps design specialized \emph{optimization methods} tailored to specific problem types
\item Keyword: disciplined convex programming
\furtherreading{GRANT2006}
\end{framei}

\endlecture
\end{document}
