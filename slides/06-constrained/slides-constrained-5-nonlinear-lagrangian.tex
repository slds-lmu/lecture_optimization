\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/optim-basics}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
Constrained Optimization
}{
Nonlinear programs and Lagrangian
}{
figure_man/Weak_and_Strong_Duality.png
}{
\item Lagrangian for general constrained optimization
\item Geometric intuition for Lagrangian duality
\item Properties and examples
}

\begin{framei}{Nonlinear Constrained Optimization}
\item Previous lecture: \textbf{Linear programs}
$$
\min_{\xv \in \R^d} f(\xv) := \mathbf{c}^T\xv \quad
\text{s.t. }  \Amat \xv \le \bv, \;
\mathbf{G} \xv = \mathbf{h} 
$$
\item Related to its (Lagrange) dual formulation by the \textit{Lagrangian}
$$
\mathcal{L}(\xv,\bm{\alpha},\bm{\beta}) = \mathbf{c}^T \xv + \bm{\alpha}^T (\Amat \xv - \bv) + \bm{\beta}^T (\mathbf{G} \xv - \mathbf{h})
$$
\item \textbf{Weak duality:} For $\bm{\alpha} \geq 0$ and $\bm{\beta}$:
$$
\fxvs \geq \min_{\xv \in \S} \mathcal{L}(\xv, \bm{\alpha}, \bm{\beta}) \geq \min_{\xv \in \R^d} \mathcal{L}(\xv, \bm{\alpha}, \bm{\beta}) =: g(\bm{\alpha}, \bm{\beta})
$$
\item \textbf{Recall:} Implicit domain constraint in \textit{Lagrange dual function} $g(\bm{\alpha}, \bm{\beta})$
\end{framei}


\begin{framei}{General Constrained Optimization}
\item General form of a constrained optimization problem
$$
\min_{\xv \in \R^d} f(\xv) \quad
\text{s.t. } g_i(\xv) \le 0, \; i=1,\ldots, k, \quad
h_j(\xv) = 0, \; j=1,\ldots, \ell
$$
\item Functions $f$, $g_i$, $h_j$ generally nonlinear
\item Transfer the Lagrangian from linear programs:
$$
\mathcal{L}(\xv, \bm{\alpha}, \bm{\beta}) := f(\xv) + \sum_{i=1}^k \alpha_i g_i(\xv) + \sum_{j=1}^\ell \beta_j h_j(\xv)
$$
\item Dual variables $\alpha_i \ge 0$ and $\beta_i$ are also called \textit{Lagrange multipliers}
\end{framei}


\begin{framei}{Constrained problems: the direct way}
\item Simple constrained problems can be solved in a direct way
\item[Example 1:]
$$
\min_{x\in \R} 2 - x^2 \quad \text{s.t. } x - 1 = 0
$$
\item \textbf{Solution:} Resolve the constraint by $x - 1 = 0 \Rightarrow x = 1$\\
and insert it into the objective: $\xs = 1, \; f(\xs) = 1$
\end{framei}


\begin{framei}{Constrained problems: the direct way}
\item[Example 2:]
$$
\min_{\xv \in \R^2} - 2 + x_1^2 + 2 x_2^2 \quad \text{s.t. } x_1^2 + x_2^2 - 1 = 0
$$
\item \textbf{Solution:} Resolve the constraint $x_1^2 = 1 - x_2^2$\\
and insert it into the objective
$$
f(x_1, x_2) = -2 + (1 - x_2^2) + 2 x_2^2 = -1 + x_2^2
$$
\item $\Rightarrow$ Minimum at $\xvs = (\pm 1, 0)^T$
\item However, direct way mostly not possible
\end{framei}


\begin{framei}{A classic example: milkmaid problem}
\item \textbf{Question 1:} Is there a general recipe for solving general constrained nonlinear optimization problems?
\item \textbf{Question 2:} Can we understand this recipe geometrically?
\item \textbf{Question 3:} How does this relate to the Lagrange function approach?
\vfill
\item For this purpose, we consider the classical ``milkmaid problem''
\item Assume a milkmaid is sent to the field to get the day's milk
\item The milkmaid wants to finish her job as quickly as possible
\item However, she has to clean her bucket first at the nearby river
\end{framei}


\begin{framei}{Milkmaid problem}
\item Where is the best point $P$ to clean her bucket?
\vfill
\imageC[0.6]{figure_man/milkmaid1.png}
\end{framei}


\begin{framei}{Milkmaid problem}
\item \textbf{Aim:} Find point $P$ at the river for minimum total distance $f(P)$
\item $f(P) := d(M, P) + d(P, C)$ ($d$ measures distance)
\item $h(P) = 0$ describes the river
\vfill
\imageC[0.4]{figure_man/milkmaid2.png}
\end{framei}


\begin{framei}{Milkmaid problem}
\item Corresponding optimization problem:
$$
\min_{x_1, x_2} f(x_1, x_2) \quad \text{s.t.} \; h(x_1, x_2) = 0
$$
\vfill
\imageC[0.4]{figure_man/milkmaid2.png}
\end{framei}


\begin{framei}{Milkmaid problem}
\item \textbf{Question:} How far can the milkmaid get for a fixed total distance $f(P)$?
\item \textbf{Assume:} We only care about $d(M,P)$
\vfill
\imageC[0.4]{figure_man/milkmaid3.png}
\item \textbf{Observe:} Radius of circle touching river first is the minimal distance
\end{framei}


\begin{framei}{Milkmaid problem}
\item For $f(P) = d(M,P) + d(P,C)$: Use an \textbf{ellipse}
\item \textbf{Def.:} Given two focal points $F_1$, $F_2$ and distance $2a$:
$$
E = \{ P \in \R^2 \;|\; d(F_1,P) + d(P,F_2) = 2a \}
$$
\vfill
\imageC[0.5]{figure_man/ellipse.png}
\end{framei}


\begin{framei}{Milkmaid problem}
\item Let $M$ and $C$ be focal points of a collection of ellipses
\item Find \textbf{smallest} ellipse touching the river $h(x_1, x_2)$
\item Define $P$ as the touching point
\vfill
\imageC[0.45]{figure_man/milkmaid4.png}
\item \textbf{Question:} How can we make sense of this mathematically?
\end{framei}


\begin{framei}{Milkmaid problem}
\item \textbf{Recall:} Gradient is normal (perpendicular) to contour lines
\item Since contour lines of $f$ and $h$ touch, gradients are parallel:
$$
\nabla f(P) = \beta \nabla h(P)
$$
\item Multiplier $\beta$ is called \textbf{Lagrange multiplier}
\vfill
\imageC[0.45]{figure_man/milkmaid5.png}
\end{framei}


\begin{framei}{Lagrange function}
\item \textbf{General:} Solve problem with single equality constraint by:
$$
\nabla f(\xv) = \beta \nabla h(\xv) \quad \text{and} \quad h(\xv) = 0
$$
\item \textbf{First line}: Parallel gradients $|$ \textbf{Second line:} Constraint
\item \textbf{Observe:} Above system is equivalent to
$$
\nabla \mathcal{L}(\xv, \beta) = \zero
$$
for \textbf{Lagrange function} (or \textbf{Lagrangian}) $\mathcal{L}(\xv, \beta) := f(\xv) + \beta h(\xv)$
\item Indeed:
$$
\mat{
\nabla_{\xv} \mathcal{L}(\xv, \beta) \\
\nabla_{\beta} \mathcal{L}(\xv, \beta)
}
= \mat{
\nabla f(\xv) + \beta \nabla h(\xv) \\
h(\xv)
}
$$
\end{framei}


\begin{framei}{Lagrange function}
\item Idea can be extended to \textbf{inequality} constraints $g(\xv) \leq 0$
\item There are two possible cases for a solution:
\begin{itemize}
\item Solution $\xv_b$ inside feasible set: constraint is inactive ($\alpha_b =0$)
\item Solution $\xv_a$ on boundary $g(\xv) = 0$: $\nabla f(\xv_a) = \alpha_a \nabla g(\xv_a)$ ($\alpha_a > 0$)
\end{itemize}
\vfill
\imageC[0.45]{figure_man/constraint_lagrange.png}
\end{framei}


\begin{framei}[fs=small]{Lagrange function and primal problem}
\item General constrained optimization problems:
$$
\min_{\xv} f(\xv) \quad
\text{s.t.} \; g_i(\xv) \leq 0, \; i = 1, \ldots ,k, \quad
h_j(\xv) = 0, \; j = 1, \ldots, \ell
$$
\item Extend Lagrangian ($\alpha_i\ge 0$, $\beta_i$ Lagrange multipliers):
$$
\mathcal{L}(\xv, \bm{\alpha}, \bm{\beta}) := f(\xv) + \sum_{i=1}^k \alpha_i g_i(\xv) + \sum_{j=1}^\ell \beta_j h_j(\xv)
$$
\item \textbf{Equivalent} primal problem:
$$
\min_{\xv} \max_{\bm{\alpha}\ge 0, \bm{\beta}}  \mathcal{L}(\xv, \bm{\alpha}, \bm{\beta})
$$
\item \textbf{Question:} Why?
\end{framei}


\begin{framei}{Lagrange function and primal problem}
\item For simplicity: Consider only single inequality constraint $g(\xv) \leq 0$
\vfill
\item If $\xv$ \textbf{breaks} inequality constraint ($g(\xv) > 0$):
$$
\max_{\alpha \ge 0} \mathcal{L}(\xv, \alpha) = \max_{\alpha \ge 0} f(\xv) +  \alpha g(\xv) = \infty
$$
\item If $\xv$ \textbf{satisfies} inequality constraint ($g(\xv) \le 0$):
$$
\max_{\alpha \ge 0} \mathcal{L}(\xv, \alpha) = \max_{\alpha \ge 0} f(\xv) +  \alpha g(\xv) = f(\xv)
$$
\vfill
\item Combining yields \textbf{original formulation}:
$$
\min_{\xv} \max_{\alpha \ge 0} \mathcal{L}(\xv, \alpha) =
\begin{cases}
\infty & \text{if } g(\xv) > 0 \\
\min_{\xv} f(\xv) & \text{if }  g(\xv) \le 0
\end{cases}
$$
\item Similar argument holds for equality constraints $h_j(\xv)$
\end{framei}


\begin{framei}[fs=small]{Example: Lagrange function for QPs}
\item We consider quadratic programming
$$
\min_{\xv} f(\xv) := \frac{1}{2} \xv^T \mathbf{Q} \xv \quad
\text{s.t.} \; h(\xv) := \mathbf{C} \xv - \mathbf{d} = \zero
$$
with $\mathbf{Q} \in \R^{d\times d}$ symmetric, $\mathbf{C} \in \R^{\ell \times d}$, and $\mathbf{d} \in \R^\ell$
\item Lagrange function: $\mathcal{L}(\xv, \bm{\beta}) = \frac{1}{2} \xv^T \mathbf{Q} \xv + \bm{\beta}^T (\mathbf{C} \xv - \mathbf{d})$
\item Solve
$$
\nabla \mathcal{L}(\xv, \bm{\beta}) =
\begin{pmatrix}
\partial \mathcal{L} / \partial \xv \\
\partial \mathcal{L} / \partial {\bm{\beta}}
\end{pmatrix} =
\begin{pmatrix}
\mathbf{Q} \xv + \mathbf{C}^T\bm{\beta} \\
\mathbf{C} \xv - \mathbf{d}
\end{pmatrix} = \zero
\quad \Leftrightarrow \quad
\begin{pmatrix}
\mathbf{Q} & \mathbf{C}^T \\
\mathbf{C} & \zero
\end{pmatrix}
\begin{pmatrix} \xv \\ \bm{\beta} \end{pmatrix} =
\begin{pmatrix} \zero \\ \mathbf{d} \end{pmatrix}
$$
\item \textbf{Observe:} Solve QP by solving a linear system
\end{framei}


\begin{framei}[fs=small]{Lagrange duality}
\item \textbf{Dual problem:}
$$
\max_{\bm{\alpha} \ge 0, \bm{\beta}} \min_{\xv}  \mathcal{L}(\xv, \bm{\alpha}, \bm{\beta})
$$
\item Define \textbf{Lagrange dual function} $g(\bm{\alpha}, \bm{\beta}) := \min_{\xv}  \mathcal{L}(\xv, \bm{\alpha}, \bm{\beta})$
\vfill
\item Important characteristics of the dual problem:
\item \textbf{Convexity} (pointwise minimum of \textit{affine} functions)
\begin{itemize}
\item Gives methods based on dual solutions
\item Might be computationally inefficient (expensive minimizations)
\end{itemize}
\item \textbf{Weak duality:} $\fxvs \geq g(\bm{\alpha}^*, \bm{\beta}^*)$
\item \textbf{Strong duality} if primal problem satisfies \textit{Slater's condition}$^{(1)}$:\\
$\fxvs = g(\bm{\alpha}^*, \bm{\beta}^*)$
\vfill
\item[$^{(1)}$] \textbf{Slater's condition}: Primal problem convex and ``strictly feasible'' ($\exists \xv \forall i: g_i(\xv) < 0$)
\end{framei}

\endlecture
\end{document}
