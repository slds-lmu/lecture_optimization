\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/optim-basics}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
First order methods
}{
GD on quadratic forms
}{
figure_man/gd.png
}{
\item Eigendecomposition of\\quadratic forms
\item GD steps in eigenspace
}

\begin{framei}{Quadratic forms \& GD}
\item We consider the quadratic function $q(\xv) = \xv^T \Amat \xv - \bv^T \xv$
\item We assume that the Hessian $\H = 2 \Amat$ has full rank
\item We assume $q$ is convex, so $\Amat$ is psd
\item The optimal solution is $\xvs = \frac{1}{2} \Amat^{-1} \bv$
\item With $\nabla q(\xv) = 2 \Amat \xv - \bv$, gradient descent iterates as
\vfill
$$
\xv^{[t+1]} = \xv^{[t]} - \alpha (2 \Amat \xv^{[t]} - \bv)
$$
\imageC[0.45][http://doi.org/10.23915/distill.00006]{figure_man/gd.png}
\end{framei}


\begin{framei}{Eigendecomposition of quadratic forms}
\item We want to work in the coordinate system induced by $q$
\item \textbf{Recall:} That system is given by the eigenvectors of $\H = 2\Amat$
\item Eigendecomposition: $\Amat = \V \bm{\Lambda} \V^T$
\item $\V$ contains eigenvectors $\vv_i$ and $\bm{\Lambda} = \diag(\lambda_1, \dots, \lambda_n)$ stores eigenvalues
\item Change of basis: $\wv^{[t]} = \V^T (\xv^{[t]} - \xvs)$
\vfill
\imageC[0.45]{figure_man/gd_eigenspace.png}
\end{framei}


\begin{framei}{GD steps in eigenspace}
\item With $\wv^{[t]} = \V^T (\xv^{[t]} - \xvs)$, a single GD step starts with
$$
\xv^{[t+1]} = \xv^{[t]} - \alpha (2 \Amat \xv^{[t]} - \bv)
$$
\item This becomes
$$
\wv^{[t+1]} = \wv^{[t]} - 2 \alpha \bm{\Lambda} \wv^{[t]}
$$
\end{framei}


\begin{framei}{GD steps in eigenspace: proof}
\item A single GD step satisfies $\xv^{[t+1]} = \xv^{[t]} - \alpha (2 \Amat \xv^{[t]} - \bv)$
\item Multiplying with $\V^T$ and subtracting $\xvs$ yields
\begin{align*}
\V^T (\xv^{[t+1]} - \xvs) &= \V^T (\xv^{[t]} - \xvs) - \alpha \V^T (2 \Amat \xv^{[t]} - \bv) \\
\wv^{[t+1]} &= \wv^{[t]} - \alpha \V^T (2 \Amat (\xv^{[t]} - \xvs) + \underbrace{2 \Amat \xvs - \bv}_{=0}) \\
&= \wv^{[t]} - 2 \alpha \bm{\Lambda} \V^T (\xv^{[t]} - \xvs) \\
&= \wv^{[t]} - 2 \alpha \bm{\Lambda} \wv^{[t]}
\end{align*}
\end{framei}


\begin{frame2}{GD steps in eigenspace}
Therefore, for each coordinate $i$:
\begin{align*}
w_i^{[t+1]} &= w_i^{[t]} - 2 \alpha \lambda_i w_i^{[t]} \\
&= (1 - 2 \alpha \lambda_i) w_i^{[t]} \\
&= (1 - 2 \alpha \lambda_i)^{t+1} w_i^{[0]}.
\end{align*}
\end{frame2}


\begin{framei}{GD error in original space}
\item Move back to the original space:
$$
\xv^{[t]} - \xvs = \V \wv^{[t]} = \sum_{i = 1}^{d} (1 - 2 \alpha \lambda_i)^t w_i^{[0]} \vv_i
$$
\item This gives the difference in $\xv$-space in closed form
\item The difference vector is written in the eigenbasis, initial error components $w_i^{[0]}$ decay with rate $1 - 2 \alpha \lambda_i$
\item Smaller absolute values yield faster decay; values close to $1$ slow convergence
\item Eigenvectors with larger eigenvalues typically converge fastest, especially early on
\end{framei}


\begin{framei}{GD regret}
\item We can also analyze the regret in $y$-space:
$$
q(\xv^{[t]}) - q(\xvs) = \frac{1}{2} \sum_{i = 1}^{d} (1 - 2 \alpha \lambda_i)^{2t} \lambda_i (w_i^{[0]})^2
$$
\item This is straightforward to derive; formulas only become slightly lengthy
\item Rates become more similar once the slow modes dominate
\vfill
\imageC[0.45]{figure_man/gd_conv.png}
\end{framei}


\begin{framei}{Optimal step size}
\item Convergence requires $|1 - 2 \alpha \lambda_i| < 1$ for all $i$
\item Therefore $0 < \alpha \lambda_i < 1$
\item The slowest component governs the rate:
$$
\max_{i = 1, \dots, n} |1 - 2 \alpha \lambda_i| = \max(|1 - 2 \alpha \lambda_1|, |1 - 2 \alpha \lambda_n|)
$$
\item The optimum occurs at $\alpha = \frac{1}{\lambda_1 + \lambda_n}$
\item The optimal rate equals $\frac{\lambda_n / \lambda_1 - 1}{\lambda_n / \lambda_1 + 1}$
\item Convergence is driven by the condition number $\kappa = \frac{\lambda_n}{\lambda_1}$
\item $\kappa = 1$ is ideal and yields convergence in one step
\end{framei}

\endlecture
\end{document}
