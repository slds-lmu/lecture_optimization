\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
First order methods
}{
GD on quadratic forms
}{
figure_man/gd.png
}{
\item Eigendecomposition of\\quadratic forms
\item GD steps in eigenspace
}

\begin{framei}{Quadratic forms \& GD}
\item We consider the quadratic function $q(\xv) = \xv^T \Amat \xv - \mathbf{b}^T \xv$
\item We assume that the Hessian $\mathbf{H} = 2 \Amat$ has full rank
\item We assume $q$ is convex, so $\Amat$ is psd
\item The optimal solution is $\xv^\ast = \frac{1}{2} \Amat^{-1} \mathbf{b}$
\item With $\nabla q(\xv) = 2 \Amat \xv - \mathbf{b}$, gradient descent iterates as
\vfill
$$
\xv^{[t+1]} = \xv^{[t]} - \alpha (2 \Amat \xv^{[t]} - \mathbf{b})
$$
\vfill
\imageC[0.5][http://doi.org/10.23915/distill.00006]{figure_man/gd.png} % SOURCE: Distill "Why Momentum Really Works" (2017)
\end{framei}


\begin{framei}{Eigendecomposition of quadratic forms}
\item We want to work in the coordinate system induced by $q$
\item \textbf{Recall:} That system is given by the eigenvectors of $\mathbf{H} = 2\Amat$
\item Eigendecomposition: $\Amat = \mathbf{V} \bm{\Lambda} \mathbf{V}^T$
\item $\mathbf{V}$ contains eigenvectors $\mathbf{v}_i$ and $\bm{\Lambda} = \text{diag}(\lambda_1, \dots, \lambda_n)$ stores eigenvalues
\item Change of basis: $\mathbf{w}^{[t]} = \mathbf{V}^T (\xv^{[t]} - \xv^\ast)$
\vfill
\imageC[0.45]{figure_man/gd_eigenspace.png}
\end{framei}


\begin{framei}{GD steps in eigenspace}
\item With $\mathbf{w}^{[t]} = \mathbf{V}^T (\xv^{[t]} - \xv^\ast)$, a single GD step starts with
$$
\xv^{[t+1]} = \xv^{[t]} - \alpha (2 \Amat \xv^{[t]} - \mathbf{b})
$$
\item This becomes
$$
\mathbf{w}^{[t+1]} = \mathbf{w}^{[t]} - 2 \alpha \bm{\Lambda} \mathbf{w}^{[t]}
$$
\end{framei}


\begin{framei}[fs=small]{GD steps in eigenspace: proof}
\item A single GD step satisfies $\xv^{[t+1]} = \xv^{[t]} - \alpha (2 \Amat \xv^{[t]} - \mathbf{b})$
\item Multiplying with $\mathbf{V}^T$ and subtracting $\xv^\ast$ yields
\begin{align*}
\mathbf{V}^T (\xv^{[t+1]} - \xv^\ast) &= \mathbf{V}^T (\xv^{[t]} - \xv^\ast) - \alpha \mathbf{V}^T (2 \Amat \xv^{[t]} - \mathbf{b}) \\
\mathbf{w}^{[t+1]} &= \mathbf{w}^{[t]} - \alpha \mathbf{V}^T (2 \Amat (\xv^{[t]} - \xv^\ast) + \underbrace{2 \Amat \xv^\ast - \mathbf{b}}_{=0}) \\
&= \mathbf{w}^{[t]} - 2 \alpha \bm{\Lambda} \mathbf{V}^T (\xv^{[t]} - \xv^\ast) \\
&= \mathbf{w}^{[t]} - 2 \alpha \bm{\Lambda} \mathbf{w}^{[t]}
\end{align*}
\end{framei}


\begin{frame2}{GD steps in eigenspace}
Therefore, for each coordinate $i$:
\begin{align*}
w_i^{[t+1]} &= w_i^{[t]} - 2 \alpha \lambda_i w_i^{[t]} \\
&= (1 - 2 \alpha \lambda_i) w_i^{[t]} \\
&= (1 - 2 \alpha \lambda_i)^{t+1} w_i^{[0]}.
\end{align*}
\end{frame2}


\begin{framei}{GD error in original space}
\item Move back to the original space:
$$
\xv^{[t]} - \xv^\ast = \mathbf{V} \mathbf{w}^{[t]} = \sum_{i = 1}^{d} (1 - 2 \alpha \lambda_i)^t w_i^{[0]} \mathbf{v}_i
$$
\item This gives the difference in $\xv$-space in closed form
\item The difference vector is written in the eigenbasis, initial error components $w_i^{[0]}$ decay with rate $1 - 2 \alpha \lambda_i$
\item Smaller absolute values yield faster decay; values close to $1$ slow convergence
\item Eigenvectors with larger eigenvalues typically converge fastest, especially early on
\end{framei}


\begin{framei}{GD regret}
\item We can also analyze the regret in $y$-space:
$$
q(\xv^{[t]}) - q(\xv^\ast) = \frac{1}{2} \sum_{i = 1}^{d} (1 - 2 \alpha \lambda_i)^{2t} \lambda_i (w_i^{[0]})^2
$$
\item This is straightforward to derive; formulas only become slightly lengthy
\item Rates become more similar once the slow modes dominate
\vfill
\imageC[0.45]{figure_man/gd_conv.png}
\end{framei}


\begin{framei}{Optimal step size}
\item Convergence requires $|1 - 2 \alpha \lambda_i| < 1$ for all $i$
\item Therefore $0 < \alpha \lambda_i < 1$
\item The slowest component governs the rate:
$$
\max_{i = 1, \dots, n} |1 - 2 \alpha \lambda_i| = \max(|1 - 2 \alpha \lambda_1|, |1 - 2 \alpha \lambda_n|)
$$
\item The optimum occurs at $\alpha = \frac{1}{\lambda_1 + \lambda_n}$
\item The optimal rate equals $\frac{\lambda_n / \lambda_1 - 1}{\lambda_n / \lambda_1 + 1}$
\item Convergence is driven by the condition number $\kappa = \frac{\lambda_n}{\lambda_1}$
\item $\kappa = 1$ is ideal and yields convergence in one step
\end{framei}

\endlecture
\end{document}
