\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/gd.png}
\newcommand{\learninggoals}{
\item Eigendecomposition of\\quadratic forms
\item GD steps in eigenspace
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{First order methods: GD on quadratic forms}
\lecture{\inserttitle}
\sloppy
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Quadratic forms \& GD}

\begin{itemize}
	\item We consider the quadratic function $q(\xv) = \xv^\top \Amat \xv - \mathbf{b}^\top \xv$. 
	\item We assume that Hessian $\mathbf{H} = 2 \Amat$ has full rank
	\item Optimal solution is $\xv^\ast = \frac{1}{2} \Amat^{-1} \mathbf{b}$ 
	\item As $\nabla q(\xv) = 2 \Amat \xv - \mathbf{b}$, iterations of gradient descent are
\end{itemize}

\vspace{-0.5\baselineskip}

\begin{equation*}
    \xv^{[t+1]} = \xv^{[t]} - \alpha (2 \Amat \xv^{[t]} - \mathbf{b})
\end{equation*}

\vspace{-\baselineskip}

\begin{figure}
	\includegraphics[height=0.3\textwidth, keepaspectratio]{figure_man/gd.png}
    \caption*{\centering \footnotesize
        The following slides follow the blog post "Why Momentum Really Works", Distill, 2017.
        \url{http://doi.org/10.23915/distill.00006}}
\end{figure}

\end{vbframe}

\begin{vbframe}{Eigendecomposition of quadratic forms}

\begin{itemize}
    \setlength{\itemsep}{0.5em}
    \item We want to work in the coordinate system given by $q$
    \item \textbf{Recall:} Coordinate system is given by the eigenvectors of $\mathbf{H} = 2\Amat$
    \item Eigendecomposition of $\Amat = \mathbf{V} \bm{\Lambda} \mathbf{V}^\top$
    \item $\mathbf{V}$ contains eigenvectors $\mathbf{v}_i$ and $\bm{\Lambda} = \text{diag}(\lambda_1, ..., \lambda_n)$ eigenvalues
    \item Change of basis: $\mathbf{w}^{[t]} = \mathbf{V}^\top (\xv^{[t]} - \xv^\ast)$
\end{itemize}

\begin{figure}
	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/gd_eigenspace.png} \\
\end{figure}

\end{vbframe}

\begin{vbframe}{GD steps in eigenspace}

With $\mathbf{w}^{[t]} = \mathbf{V}^\top (\xv^{[t]} - \xv^\ast)$, a single GD step

\begin{equation*}
    \xv^{[t + 1]} = \xv^{[t]} - \alpha (2 \Amat \xv^{[t]} - \mathbf{b})
\end{equation*}
becomes
\begin{equation*}
    \mathbf{w}^{[t+1]} = \mathbf{w}^{[t]} - 2 \alpha \bm{\Lambda} \mathbf{w}^{[t]}.
\end{equation*}

Therefore:

\vspace{-\baselineskip}

\begin{align*}
    w_i^{[t+1]} &= w_i^{[t]} - 2 \alpha \lambda_i w_i^{[t]} \\
    &= (1 - 2 \alpha \lambda_i) w_i^{[t]} \\
    &= \cdots \\
    &= (1 - 2 \alpha \lambda_i)^{t+1} w_i^{[0]}
\end{align*}

\framebreak

\textbf{Proof} (for $\mathbf{w}^{[t+1]} = \mathbf{w}^{[t]} - 2 \alpha \bm{\Lambda} \mathbf{w}^{[t]}$)\textbf{:}

\begin{itemize}
    \item A single GD step means
        \begin{equation*}
            \xv^{[t+1]} = \xv^{[t]} - \alpha (2 \Amat \xv^{[t]} - \mathbf{b})
        \end{equation*}
    \item Then:
        \begin{align*}
            \mathbf{V}^\top (\xv^{[t+1]} - \xv^\ast) &= \mathbf{V}^\top (\xv^{[t]} - \xv^\ast) - \alpha \mathbf{V}^\top (2 \Amat \xv^{[t]} - \mathbf{b}) \\
            \mathbf{w}^{[t+1]} &= \mathbf{w}^{[t]} - \alpha \mathbf{V}^\top (2 \Amat \xv^{[t]} - \mathbf{b}) \\
            \mathbf{w}^{[t+1]} &= \mathbf{w}^{[t]} - \alpha \mathbf{V}^\top (2 \Amat (\xv^{[t]} - \xv^\ast) + \underbrace{2 \Amat \xv^\ast - \mathbf{b}}_{=0}) \\
            &= \mathbf{w}^{[t]} - 2 \alpha \bm{\Lambda} \mathbf{V}^\top (\xv^{[t]} - \xv^\ast) \\
            &= \mathbf{w}^{[t]} - 2 \alpha \bm{\Lambda} \mathbf{w}^{[t]}
        \end{align*}
\end{itemize}

\end{vbframe}

\begin{vbframe}{GD error in original space}

\begin{itemize}
    \setlength{\itemsep}{1em}
    \item Move back to \textbf{original space}:
        \begin{equation*}
            \xv^{[t]} - \xv^\ast = \mathbf{V} \mathbf{w}^{[t]} = \sum_{i = 1}^{d} (1 - 2 \alpha \lambda_i)^t w_i^{[0]} \mathbf{v}_i 
        \end{equation*}
    \item \textbf{Intuition:} Initial error components $w^{[0]}_i$ (in the eigenbasis) decay with rate $1 - 2 \alpha \lambda_i$
    \item \textbf{Therefore:} For sufficiently small step sizes $\alpha$, error components along eigenvectors with large eigenvalues decay quickly
\end{itemize}

\framebreak

We now consider the contribution of each eigenvector to the total loss

\begin{equation*}
    q(\xv^{[t]}) - q(\xv^\ast) = \frac{1}{2} \sum_{i}^{d} (1 - 2 \alpha \lambda_i)^{2t} \lambda_i (w_i^{[0]})^2
\end{equation*}

\begin{figure}
	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/gd_conv.png} \\
\end{figure}

%\framebreak

%The considerations above already gave some guidance on how to choose the optimal step size $\alpha$. In order to converge, $| 1 - \alpha \lambda_i| $ must be strictly less than $1$. 

%\begin{itemize}
%	\item use eigenvectors of $A$
%	\item GD in closed form 
%	\item decomposing the error, plot with convergence
%	\item choosing step size, show dependence on condition number 
%	\item maybe with example of polynomial regression
%\end{itemize}
\end{vbframe}


\endlecture
\end{document}

