\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/optim-basics}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
First order methods
}{
Comparison of first order methods
}{
figure_man/linesearch.png
}{
\item Gradient Descent
\item Stochastic Gradient Descent
\item Momentum
\item Step size decay
}

\begin{framei}[fs=small]{Comparison of first order methods}
\item Comparison of (S)GD, (S)GD + momentum, and (S)GD + momentum + step size control on simulated data
\item We do not use analytical solution on purpose although one exists:
\item Linear regression (squared loss) simulation $\yv = \Xmat\thetav^{\ast} + \epsv$ with $n=500$ samples and $p=11$ features, where $\thetav^{\ast}=(-5,-4,\ldots0,\ldots,4,5)^{\top}$, $\epsv \sim \normal(\zero, \id)$, and $\Xmat \sim \normal(\zero, \Sigma)$ for $\Sigma=\id$ (i.i.d. features) or $\Sigma_{i,j}=0.99^{|i-j|}$ (corr. features)
\item Indep. features result in a condition number of $\approx 2.9$, whereas the corr. feature set-up produces a (moderately) bad condition number of $\approx 600$
\item We set the momentum parameter to $0.8$ and the decay step size using schedule $\alpha^{[t]}=\alpha^{[0]} \cdot \texttt{decay}^{t/t_{\text{max}}}$ for $\texttt{decay}=0.1$
\item For GD and SGD we use different step sizes to show that benefit of momentum/decay depends heavily on step size
\item ERM has unique global minimizer given by $\thetavh = (\Xmat^{\top}\Xmat)^{-1}\Xmat^{\top} \yv$
\item We also track the optimization error $\Vert \thetav - \thetavh \Vert_2$
\end{framei}


\begin{framei}[fs=small]{Lin-Reg (GD + med step size, default case)}
\item GD with medium $\alpha=2\cdot10^{-3}$ and indep. features:
\imageC[0.7]{figure_man/simu_linmod/GD_reg_med_lr_iters.pdf}
\imageC[0.75]{figure_man/simu_linmod/GD_reg_coef_med.pdf}
\item Irreducible error due to additive noise is $\sigma=1$. Dotted lines indicate global minimizers
\item All variants converge to global min. \textbf{Momentum} accelerates optimization and \textbf{decay} slows down optimization under that step size
\end{framei}


\begin{framei}[fs=footnotesize]{Lin-Reg (GD + corr. features)}
\item GD with medium $\alpha=2\cdot10^{-3}$ and bad conditioning (corr. features):
\imageC[0.7]{figure_man/simu_linmod/GD_reg_med_lr_corr_iters.pdf}
\imageC[0.75]{figure_man/simu_linmod/GD_reg_coef_med_corr.pdf}
\item Irreducible error due to additive noise is $\sigma=1$. Dotted lines indicate global minimizers
\item Moderately bad \textbf{conditioning slows down optim} severely! Only \textbf{momentum} w/o decay comes close to global min
\item \textbf{Momentum} causes ``overshooting'' of some coefs. Corr. features cause corr. global minimizers
\end{framei}


\begin{framei}[fs=small]{Lin-Reg (GD + small step size)}
\item GD with (too small) $\alpha=3\cdot10^{-4}$ and indep. features:
\imageC[0.7]{figure_man/simu_linmod/GD_reg_small_lr_iters.pdf}
\imageC[0.75]{figure_man/simu_linmod/GD_reg_coef_small.pdf}
\item Irreducible error due to additive noise is $\sigma=1$. Dotted lines indicate global minimizers
\item Only two \textbf{momentum} variants come close to global min in $t_{\text{max}}=10000$
\item \textbf{Decay} worsens performance as $\alpha$ was already too low
\end{framei}


\begin{framei}[fs=footnotesize]{Lin-Reg (GD + large step size)}
\item GD with large $\alpha=1.5$ and indep. features:
\imageC[0.7]{figure_man/simu_linmod/GD_reg_large_lr_iters.pdf}
\imageC[0.75]{figure_man/simu_linmod/GD_reg_coef_large.pdf}
\item Irreducible error due to additive noise is $\sigma=1$. Dotted lines indicate global minimizers
\item Super fast convergence in $<20$ steps
\item \textbf{Decay} here accelerates optim while \textbf{momentum} becomes slow and unstable
\item Coefficients oscillate at beginning which is reduced by decay
\end{framei}


\begin{framei}[fs=small]{Lin-Reg (SGD + med step size)}
\item SGD with medium $\alpha=2\cdot10^{-3}$ and indep. features:
\imageC[0.7]{figure_man/simu_linmod/SGD_reg_med_lr_iters.pdf}
\imageC[0.75]{figure_man/simu_linmod/SGD_reg_coef_med.pdf}
\item \textbf{Momentum} accelerates optim initially but is eventually outperformed by other variants
\item \textbf{Momentum+decay} is both fast initially and has small final error
\item \textbf{Decay} performs best overall but is slowest initially
\end{framei}


\begin{framei}[fs=small]{Lin-Reg (SGD + large step size)}
\item SGD with large $\alpha=1 \cdot 10^{-2}$ and indep. features:
\imageC[0.7]{figure_man/simu_linmod/SGD_reg_large_lr_iters.pdf}
\imageC[0.75]{figure_man/simu_linmod/SGD_reg_coef_large.pdf}
\item Irreducible error due to additive noise is $\sigma=1$
\item High variance in SGD dynamics
\item \textbf{Momentum} becomes unstable while \textbf{decay} (necessary to eliminate noise) performs best and is fastest
\end{framei}


\begin{framei}[fs=footnotesize]{Digression: solving OLS with QR Decomp.}
\item Solving linear least squares via (S)GD is rarely done in practice
\item But inversion of $\Xmat^{\top}\Xmat$ is numerically unstable and to be avoided
\item A standard numerical approach applies \textbf{QR Decomposition}:
\begin{itemize}
\item Factorize \( \Xmat \in \mathbb{R}^{n \times p} \) as \( \Xmat = \bm{Q}\bm{R} \), where \( \bm{Q} \in \mathbb{R}^{n \times p} \) (thin form) so that $\bm{Q}^{\top}\bm{Q}=\id$ and \( \bm{R} \in \mathbb{R}^{p \times p} \) is upper triangular
\item Purpose: replace solving \( \thetavh = (\Xmat^{\top} \Xmat)^{-1} \Xmat^{\top} \yv \) with a more numerically stable method by avoiding direct inversion
\item The QR decomposition can be computed using Gram-Schmidt orthogonalization or Householder transformations
\end{itemize}
\item \textbf{Steps}:
\begin{itemize}
\item Decompose \( \Xmat \) into \( \bm{Q} \) and \( \bm{R} \)
\item Compute \( \bm{Q}^{\top} \yv \)
\item Solve triangular system \( \bm{R} \thetavh = \bm{Q}^{\top} \yv \) via \textbf{back substitution}
\vfill
\item Why this system? Remember normal equation for least squares problem is $\Xmat^{\top}\Xmat \thetavh = \Xmat^{\top}\yv$. Now replace $\Xmat=\bm{Q} \bm{R}$:
$$
\Xmat^{\top}\Xmat \thetavh = \Xmat^{\top}\yv \iff \bm{R}^{\top}(\bm{Q}^{\top} \bm{Q}) \bm{R}^{\top} \thetavh = \bm{R}^{\top} \bm{Q}^{\top} \yv
$$
$$
\bm{R}^{\top}\bm{R} \thetavh = \bm{R}^{\top} \bm{Q}^{\top} \yv \iff \bm{R} \thetavh = \bm{Q}^{\top} \yv
$$
\end{itemize}
\end{framei}


\begin{framei}[fs=small]{Digression: solving OLS with QR Decomp.}
\item $\bm{R} \thetavh = \bm{Q}^{\top} \yv$ is a triangular system easily solvable by back substitution:
$$
\bm{R} = \begin{bmatrix} 
r_{11} & r_{12} & \dots & r_{1p} \\ 
0 & r_{22} & \dots & r_{2p} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
0 & 0 & \dots & r_{pp} 
\end{bmatrix}, \quad 
\thetavh = \begin{bmatrix} 
\thetah_1 \\ 
\thetah_2 \\ 
\vdots \\ 
\thetah_p 
\end{bmatrix}, \quad \bm{Q}^{\top} \yv = \begin{bmatrix} 
b_1 \\ 
b_2 \\ 
\vdots \\ 
b_p 
\end{bmatrix}
$$
\vfill
\item \textbf{Steps for Back Substitution}:
\begin{itemize}
\item Start with the last equation (1 unknown):
$
\thetah_p = \frac{b_p}{r_{pp}}
$
\item Move upwards to the \( (p-1) \)-th equation:
$
\thetah_{p-1} = \frac{b_{p-1} - r_{p-1,p} \thetah_p}{r_{p-1,p-1}}
$
\item Continue this process up to the first row:
$
\thetah_i = \frac{b_i - \sum_{j=i+1}^p r_{ij} \thetah_j}{r_{ii}} \quad \text{for each } i = p-1, \dots, 1
$
\vfill
\item Back substitution leverages triangular structure of $\bm{R}$, moving upward from the last row and inserting already known values of $\thetavh$ as we go
\item[] Back substitution leverages triangular structure of $\bm{R}$, moving upward from the last row and inserting already known values of $\thetavh$ as we go
\end{itemize}
\end{framei}


\begin{framei}{(S)GD for logistic regression}
\item[] Comparison of (S)GD, (S)GD + momentum, and (S)GD + momentum + step size control on simulated data:
\vfill
\item Logistic regression (log loss) with same simulation setup as for linear regression
\item To simulate response, we set $y^{(i)} \sim \mathcal{B}(\pi^{(i)}), \pi^{(i)} = \frac{1}{1+e^{-(\xv^{(i)})^{\top}\thetav^{\ast}}}$
\item We set the momentum parameter to $0.8$ and $\texttt{decay}=0.1$
\item We again use different step sizes to illustrate benefit of momentum and decay depends on step size
\item ERM has unique global minimizer but no closed-form solution. We can approximate $\thetavh$ using the \texttt{glm} solution (second-order optim)
\item We also track the optimization error $\Vert \thetav - \thetavh \Vert_2$
\end{framei}


\begin{framei}[fs=small]{Log-Reg (GD + med step size)}
\item GD with medium $\alpha=0.25$ and indep. features:
\imageC[0.7]{figure_man/simu_linmod/GD_log_med_lr_iters.pdf}
\imageC[0.75]{figure_man/simu_linmod/GD_log_coef_med.pdf}
\item Dotted lines indicate global minimizers
\item All variants converge to global min
\item \textbf{Momentum} accelerates and \textbf{decay} slows down optimization. \textbf{GD+mom} performs best
\end{framei}


\begin{framei}[fs=footnotesize]{Log-Reg (GD + corr. features)}
\item GD with medium $\alpha=0.25$ and bad conditioning (corr. features):
\imageC[0.7]{figure_man/simu_linmod/GD_log_med_lr_corr_iters.pdf}
\imageC[0.75]{figure_man/simu_linmod/GD_log_coef_med_corr.pdf}
\item Dotted lines indicate global minimizers
\item Moderately bad conditioning slows down optim slightly and affects ERM solutions
\item Only \textbf{momentum w/o decay} converges exactly to global min in $t_{\text{max}}$
\item Momentum causes "\textbf{overshooting}" of some coefs
\end{framei}


\begin{framei}[fs=small]{Log-Reg (SGD + med step size)}
\item SGD with medium $\alpha=0.03$ and indep. features:
\imageC[0.7]{figure_man/simu_linmod/SGD_log_med_lr_iters.pdf}
\imageC[0.75]{figure_man/simu_linmod/SGD_log_coef_med.pdf}
\item Under SGD dynamics become more noisy
\item \textbf{Momentum} accelerates while \textbf{decay} slows down optimization
\item Only the coefs of \textbf{momentum w/o decay} come close to global minimizers
\end{framei}


\begin{framei}[fs=footnotesize]{Log-Reg (SGD + large step size)}
\item SGD with large $\alpha=0.3$ and indep. features:
\imageC[0.7]{figure_man/simu_linmod/SGD_log_large_lr_iters.pdf}
\imageC[0.75]{figure_man/simu_linmod/SGD_log_coef_large.pdf}
\item \textbf{High variance} in optim dynamics
\item Plain SGD and \textbf{Momentum} become unstable and oscillate at suboptimal loss values/diverge while \textbf{decay} (necessary to eliminate oscillations) performs best and is fastest
\item \textbf{Momentum+decay} initially diverges but recovers once step size reduces (would converge with many more steps)
\end{framei}


\begin{framei}{Classification on MNIST (SGD)}
\item For a more realistic application, we compare optimizers on training an NN on subset of MNIST image classification task
\item DNN has two hidden layers with $128$ and $64$ ReLU-activated units and Kaiming normal init. The loss is cross-entropy
\item Instead of SGD we use mini-batch SGD with $100$ images per batch
\item For the step size schedule, we use cosine decay $\alpha^{[t]} = \alpha^{[0]} \left[(1-r_{\text{min}}) \cdot \frac{1}{2}\left(1 + \cos\left(\pi \cdot \frac{t}{t_{\text{max}}}\right)\right) + r_{\text{min}}\right]$ with final step size fraction $r_{\text{min}}=0.01$
\item In case of momentum we set the parameter to $0.8$
\item Regular initial step size is $0.01$ and to $0.1$ for large step size
\end{framei}


\begin{framei}[fs=footnotesize]{Classification on MNIST (SGD)}
\item Mini-batch SGD with $\alpha \in \{0.01, 0.1\}$ for 100 epochs (5000 iterations):
\imageC[0.8]{figure_man/simu_mnist/SGD_compar.pdf}
\item[] \textbf{Observations} (\textcolor{green}{green}/\textcolor{cyan}{cyan} + \textcolor{blue}{blue}/\textcolor{magenta}{purple} train losses overlap but not val. acc.):
\item \textbf{Momentum} drastically speeds up optimization in all settings
\item \textbf{Plain SGD}: step size decay slows optimization slightly
\item SGD, SGD+cos and SGD+mom achieve \textbf{same} val. acc. $\Rightarrow$ no generalization benefit for medium $\alpha$
\item \textbf{Cosine decay+momentum} improves generalization for medium $\alpha$ slightly
\item \textbf{Large step size} with momentum and decay performs best
\end{framei}


\begin{framei}[fs=small]{Classification on MNIST (GD vs. SGD)}
\item Why is it not a good idea to use GD in most DL applications? SGD is much faster
\item Compare runtime of mini-batch SGD (batch size=$100$) with GD (constant $\alpha=0.01$ without momentum for $t_{\text{max}}=5000$ iterations):
\imageC[0.9]{figure_man/simu_mnist/SGD_GD_compar.pdf}
\item[] \textbf{Observations}:
\item Mini-batch SGD is over an order of magnitude faster
\item SGD generalizes better than GD despite being much faster
\end{framei}

\endlecture
\end{document}
