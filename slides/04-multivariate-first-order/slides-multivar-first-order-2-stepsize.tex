\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
First order methods
}{
Step size and optimality
}{
figure_man/big_small_stepsize_cropped.png
}{
\item Impact of step size
\item Fixed vs. adaptive step size
\item Exact line search
\item Armijo rule \& Backtracking
\item Bracketing \& Pinpointing
}

\begin{framei}{Controlling step size: Fixed \& adaptive}
\item Iteration $t$: Choose not only descent direction $\mathbf{d}^{[t]}$, but also step size $\alpha^{[t]}$
\item First approach: \textbf{Fixed} step size $\alpha^{[t]} = \alpha > 0$
\begin{itemizeS}[footnotesize]
\item If $\alpha$ too small, procedure may converge very slowly (left)
\item If $\alpha$ too large, procedure may not converge $\rightarrow$ \enquote{jumps} around optimum (middle)
\end{itemizeS}
\item \textbf{Adaptive} step size $\alpha^{[t]}$ can provide better convergence (right)
\splitVThree
{\imageC{figure_man/stepsize_small.png}}
{\imageC{figure_man/stepsize_large.png}}{\imageC{figure_man/stepsize_adaptive.png}}
\begin{center}\begin{footnotesize}
Steps of line searches for $f(\bm{x}) = 10 x_1^2 + x_2^2/2$
\end{footnotesize}\end{center} 		
\end{framei}
	
\begin{framei}{Step size control: Diminishing step size}
\item How can we adaptively control step size?
\item A natural way of selecting $\alpha^{[t]}$ is to decrease its value over time
\item \textbf{Example:} GD on
$$f(x) = \begin{cases}
\frac{1}{2} x^2 & \text{if $|x| \le \delta$}, \\
\delta \cdot (|x|- 1 / 2 \cdot \delta) & \text{otherwise}.\end{cases}$$
\imageC{figure_man/fixed_vs_adaptive.pdf}
\begin{center}\begin{footnotesize}
GD with small constant (\textbf{red}), large constant (\textbf{green}), and diminishing (\textbf{blue}) step size
\end{footnotesize}\end{center}
\end{framei}	
	
\begin{framei}{Step size control: Exact Line Search}
\item Use \textbf{optimal} step size in each iteration:
$$ \alpha^{[t]} = \argmin_{\alpha \in \R_{\ge 0}} g(\alpha) = \argmin_{\alpha \in \R_{\ge 0}} f(\bm{x}^{[t]} + \alpha \mathbf{d}^{[t]})$$
\splitVCC[0.6]
{\item Need to solve a \textbf{univariate} optimization problem in each iteration
$\Rightarrow$ univariate optimization methods
\item \textbf{Problem:} Expensive, \textcolor{red}{prone to poorly conditioned problems}
\item \textbf{But:} No need for \textit{optimal} step size. Only need a step size that is ``good enough''.
\item \textbf{Reason:} Effort may not pay off, but in some cases slows down performance.}
{\imageC{figure_man/line_search_rosenbrock.png}
\imageC{figure_man/line_search_rosenbrock_alpha.png}}
\end{framei}
	
\begin{framei}{Armijo rule}
\item[] 
\imageC[0.5]{figure_man/armijo.pdf}
\onslide<1>{
\item\textbf{Inexact line search:} Minimize objective ``sufficiently'' without computing optimal step size exactly
\item Common condition to guarantee ``sufficient'' decrease: \textbf{Armijo rule}}
\onslide<2>{
\vspace{-2cm}
\item Fix $\gamma_1 \in (0, 1)$. $\alpha$ satisfies \textbf{Armijo rule} in $\xv$ for descent direction $\mathbf{d}$ if
$$f(\xv + \alpha \mathbf{d}) \le \fx + \gamma_1 \alpha \nabla \fx^\top \mathbf{d}.$$
\item\textbf{Note:} $\nabla \fx^\top \mathbf{d} < 0$ ($\mathbf{d}$ \textit{descent} dir.) $\implies f(\xv + \alpha \mathbf{d}) < \fx$.}
\onslide<3>{
\vspace*{-2cm}
\item\textbf{Feasibility:} For descent direction $\mathbf{d}$ and $\gamma_1 \in (0, 1)$, there exists $\alpha>0$ fulfilling Armijo rule.In many cases, Armijo rule guarantees local convergence of GD and is therefore frequently used.}
\end{framei}
	
\begin{framei}[fs=small]{Backtracking line search}
\item Procedure to meet the Armijo rule: \textbf{Backtracking} line search
\item \textbf{Idea:} Decrease $\alpha$ until Armijo rule is met
\begin{algorithm}[H]
\caption{Backtracking line search}
\begin{algorithmic}[1]
\State Choose initial step size $\alpha = \alpha_{\text{init}}$, $0 < \gamma_1 < 1$ and $0 < \tau < 1$
\While{$f(\bm{x} + \alpha \mathbf{d}) > f(\bm{x}) + \gamma_1 \alpha \nabla f(\bm{x})^\top \mathbf{d}$}
\State Decrease $\alpha$: $\alpha \leftarrow \tau \cdot \alpha$
\EndWhile
\end{algorithmic}
\end{algorithm}
\imageC[0.7]{figure_man/backtracking-example.png}
\begin{center}\footnotesize(Source: Martins and Ning. \textit{Engineering Design Optimization}, 2021.)\end{center}
\end{framei}
	
\begin{framei}{Wolfe conditions}
\item Backtracking is simple and shows good performance in practice
\item \textbf{But:} Two undesirable scenarios
\begin{enumerate}
\item Initial step size $\alpha_{\text{init}}$ is too large $\Rightarrow$ need multiple evaluations of $f$
\item Step size is too small with highly negative slopes
\end{enumerate}
\item\textbf{Solution} for small step sizes:
\begin{itemizeM}
\item Fix $\gamma_2$ with $0 < \gamma_1 < \gamma_2 < 1$.
\item $\alpha$ satisfies \textbf{sufficient curvature condition} in $\xv$ for $\mathbf{d}$ if
$$| \nabla f(\xv + \alpha \mathbf{d})^\top \mathbf{d} | \leq \gamma_2 | \nabla f(\xv)^\top \mathbf{d} |.$$
\end{itemizeM}
\begin{framed}
Armijo rule + sufficient curvature condition = \textbf{Wolfe conditions}
\end{framed}
\end{framei}

\begin{framei}{Wolfe conditions}
\item\textbf{Algorithm} for finding a Wolfe point (point satisfying Wolfe conditions):
\begin{enumerate}
\item \textbf{Bracketing:} Find interval containing Wolfe point
\item \textbf{Pinpointing:} Find Wolfe point in interval from bracketing
\end{enumerate}
\splitVTT
{\imageR[0.8]{figure_man/wolfe_bracketing.png}}
{\imageL[0.5]{figure_man/wolfe_pinpointing.png}}
\begin{center}{\footnotesize \textbf{Left:} Bracketing.\textbf{Right:} Pinpointing. \\
(Source: Martins and Ning. \textit{EDO}, 2021.)}
\end{center}
\end{framei}

\begin{framei}{Bracketing \& Pinpointing}
\item\textbf{Example:}
\begin{itemizeM}
\item Large initial step size results in quick bracketing but multiple pinpointing steps (\textbf{left}).
\item Small initial step size results in multiple bracketing steps but quick pinpointing (\textbf{right}).
\end{itemizeM}
\vfill
\imageC[0.7]{figure_man/bracketing-pinpointing-example.png}
\centering\footnotesize Source: Martins and Ning. \textit{EDO}, 2021.
\end{framei}
   
\endlecture
\end{document}

