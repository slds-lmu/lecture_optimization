\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
First order methods
}{
Adam and friends
}{
figure_man/sattle_point2-50.png
}{
\item Adaptive step sizes
\item AdaGrad 
\item RMSProp
\item Adam
}

\begin{framei}{Adaptive step sizes}
\item Step size is probably the most important control parameter
\item Has strong influence on performance
\item Natural to use different step size for each input individually and automatically adapt them
\vfill
\imageC[0.75]{figure_man/momentum/sgd_without_momentum.png}
\end{framei}


\begin{framei}[fs=footnotesize]{AdaGrad}
\item AdaGrad adapts step sizes by
scaling them inversely proportional to square root of the sum of the past squared derivatives
\begin{itemize}
\item Inputs with large derivatives get smaller step sizes
\item Inputs with small derivatives get larger step sizes
\end{itemize}
\item Accumulation of squared gradients can result in premature small step sizes (Goodfellow et al., 2016)
\begin{algorithm}[H]
\footnotesize
\caption{AdaGrad}
\begin{algorithmic}[1]
\State \textbf{require} Global step size $\alpha$ \strut
\State \textbf{require} Initial parameter $\thetav$ \strut
\State \textbf{require} Small constant $\beta$, perhaps $10^{-7}$, for numerical stability \strut
\State \textbf{Initialize} gradient accumulation variable $\mathbf{r} = \mathbf{0} $
\While{stopping criterion not met}
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample minibatch of $m$ examples from the training set $\{\tilde{\xv}^{(1)},\dots,\tilde{\xv}^{(m)}\}$ \strut}
\State Compute gradient estimate: $\hat{\mathbf{g}} \leftarrow \frac{1}{m} \nabla_{\thetav} \sum_{i} \Lxym$
\State Accumulate squared gradient $\mathbf{r} \leftarrow \mathbf{r} + \hat{\mathbf{g}} \odot  \hat{\mathbf{g}}$ where $\odot$: element-wise product %(Hadamard)
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Compute update: $\nabla \thetav = - \frac{\alpha}{\beta + \sqrt\mathbf{r}} \odot \hat{\mathbf{g}}$ \strut}
\State Apply update: $\thetav \leftarrow \thetav + \nabla\thetav$
\EndWhile
\end{algorithmic}
\end{algorithm}
\end{framei}


\begin{framei}[fs=footnotesize]{RMSProp}
\item Modification of AdaGrad
\item Resolves AdaGrad's radically diminishing step sizes
\item Gradient accumulation is replaced by exponentially weighted moving average
\item Theoretically, leads to performance gains in non-convex scenarios
\item Empirically, RMSProp is a very effective optimization algorithm.
Particularly, it is employed routinely by DL practitioners
\vspace{-0.5em}\begin{algorithm}[H]
\footnotesize
\caption{RMSProp}
\begin{algorithmic}[1]
\State \textbf{require} Global step size $\alpha$ and decay rate $\rho \in [0, 1)$ \strut
\State \textbf{require} Initial parameter $\mathbf{\thetav}$ \strut
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{\textbf{require} Small constant $\beta$, perhaps $10^{-6}$, for numerical stability \strut}
\State Initialize gradient accumulation variable $\mathbf{r} = \mathbf{0} $
\While{stopping criterion not met}
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample minibatch of $m$ examples from the training set $\{\tilde{\xv}^{(1)},\dots,\tilde{\xv}^{(m)}\}$ \strut}
\State Compute gradient estimate: $\hat{\mathbf{g}} \leftarrow \frac{1}{m} \nabla_\theta \sum_{i} \Lxym$
\State Accumulate squared gradient $\mathbf{r} \leftarrow \rho \mathbf{r} + (1 - \rho) \hat{\mathbf{g}} \odot  \hat{\mathbf{g}}$
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Compute update: $\nabla\mathbf{\thetav} = - \frac{\alpha}{\beta + \sqrt\mathbf{r}} \odot \hat{\mathbf{g}}$ \strut}
\State Apply update: $\mathbf{\thetav} \leftarrow \mathbf{\thetav} + \nabla\mathbf{\thetav}$
\EndWhile
\end{algorithmic}
\end{algorithm}
\end{framei}


\begin{framei}{Adam}
\item Adaptive Moment Estimation also has adaptive step sizes
\item Uses the 1st and 2nd moments of gradients
\begin{itemize}
\item Keeps an exponentially decaying average of past gradients (1st moment)
\item Like RMSProp, stores an exp-decaying avg of past squared gradients (2nd moment)
\item Can be seen as combo of RMSProp + momentum
\end{itemize}
\end{framei}


\begin{framev}{Adam}
\begin{algorithm}[H]
\scriptsize
\caption{Adam}
\begin{algorithmic}[1]
\State \textbf{require} Global step size $\alpha$ (suggested default: 0.001) \strut
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{\textbf{require} Exponential decay rates for moment estimates, $\rho_1$ and $\rho_2$ in $[0,1)$ (suggested defaults: 0.9 and 0.999 respectively)} \strut
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{\textbf{require} Small constant $\beta$ (suggested default $10^{-8}$) \strut}
\State \textbf{require} Initial parameters $\thetav$ 
\State Initialize time step $t = 0$
\State Initialize 1st and 2nd moment variables $\mathbf{s}^{[0]} = 0, \mathbf{r}^{[0]} = 0$
\While{stopping criterion not met}
\State $t \leftarrow t + 1$
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Sample a minibatch of $m$ examples from the training set $\{\tilde{x}^{(1)},\dots,\tilde{x}^{(m)}\}$ \strut}
\State Compute gradient estimate: $\hat{\mathbf{g}}^{[t]} \leftarrow \frac{1}{m} \nabla_{\thetav} \sum_{i} \Lxym$
\State Update biased first moment estimate: $\mathbf{s}^{[t]} \leftarrow \rho_1 \mathbf{s}^{[t-1]}  + (1 - \rho_1) \hat{\mathbf{g}}^{[t]}$
\State Update biased second moment estimate: $\mathbf{r}^{[t]} \leftarrow \rho_2 \mathbf{r}^{[t-1]}  + (1 - \rho_2) \hat{\mathbf{g}}^{[t]} \odot \hat{\mathbf{g}}^{[t]}$
\State Correct bias in first moment: $\hat{\mathbf{s}} \leftarrow \frac{\mathbf{s}^{[t]} }{1-\rho_1^t}$
\State Correct bias in second moment: $\hat{\mathbf{r}} \leftarrow \frac{\mathbf{r}^{[t]} }{1-\rho_2^t}$
\State \parbox[t]{\dimexpr\linewidth-\algorithmicindent}{Compute update: $\nabla\thetav = - \alpha \frac{\hat{\mathbf{s}}}{\sqrt{\hat{\mathbf{r}}} + \beta}$ \strut}
\State Apply update: $\thetav \leftarrow \thetav + \nabla\thetav$
\EndWhile
\end{algorithmic}
\end{algorithm}
\end{framev}


\begin{framei}[fs=small]{Adam: Bias in moment estimates}
\item Initializes moment variables $\mathbf{s}$ and $\mathbf{r}$ with zero $\Rightarrow$ Bias towards zero
$$
\E[\mathbf{s}^{[t]}] \neq \E [\hat{\mathbf{g}}^{[t]}] \quad \text{and} \quad \E[\mathbf{r}^{[t]}] \neq \E [\hat{\mathbf{g}}^{[t]} \odot \hat{\mathbf{g}}^{[t]}]
$$
($\E$ calculated over minibatches)
\item Indeed: Unrolling $\mathbf{s}^{[t]}$ yields
\begin{footnotesize}
\begin{gather*}
\mathbf{s}^{[0]} = 0 \\
\mathbf{s}^{[1]} = \rho_1\mathbf{s}^{[0]} + (1 - \rho_1) \hat{\mathbf{g}}^{[1]} = (1 - \rho_1) \hat{\mathbf{g}}^{[1]} \\
\mathbf{s}^{[2]} = \rho_1\mathbf{s}^{[1]} + (1 - \rho_1) \hat{\mathbf{g}}^{[2]} = \rho_1 (1 - \rho_1) \hat{\mathbf{g}}^{[1]} + (1 - \rho_1) \hat{\mathbf{g}}^{[2]} \\
\mathbf{s}^{[3]} = \rho_1\mathbf{s}^{[2]} + (1 - \rho_1) \hat{\mathbf{g}}^{[3]} = \rho_1^2 (1 - \rho_1) \hat{\mathbf{g}}^{[1]} + \rho_1 (1 - \rho_1) \hat{\mathbf{g}}^{[2]} + (1 - \rho_1) \hat{\mathbf{g}}^{[3]}
\end{gather*}
\end{footnotesize}
\item Therefore: $\mathbf{s}^{[t]}  = (1 - \rho_1) \sum_{i=1}^t \rho_1^{t - i} \hat{\mathbf{g}}^{[i]}$
\item \textbf{Note:} Contributions of past $\hat{\mathbf{g}}^{[i]}$ decreases rapidly
\end{framei}


\begin{framei}[fs=small]{Adam: Bias correction}
\item We continue with
\begin{footnotesize}
\begin{align*}
\E [\mathbf{s}^{[t]}] &= \E [ (1 - \rho_1) \sum_{i=1}^t \rho_1^{t - i} \hat{\mathbf{g}}^{[i]}] \\
&= \E [\hat{\mathbf{g}}^{[t]}] (1 - \rho_1) \sum_{i=1}^t \rho_1^{t - i} + \zeta\\
&= \E [\hat{\mathbf{g}}^{[t]}] (1 - \rho_1^{t}) + \zeta,
\end{align*}
\end{footnotesize}
where we approximated $\hat{\mathbf{g}}^{[i]}$ by $\hat{\mathbf{g}}^{[t]}$.
The resulting error is put in $\zeta$ and be kept small due to the exponential weights of past gradients
\item Therefore: $\mathbf{s}^{[t]}$ is a biased estimator of $\hat{\mathbf{g}}^{[t]}$
\item But bias vanishes for $t \to \infty$ ($\rho_1^t \rightarrow 0$)
\item Ignoring $\zeta$, we correct for the bias by $\hat{\mathbf{s}}^{[t]} = \frac{\mathbf{s}^{[t]}}{(1 - \rho_1^{t})}$
\item Analogously: $\hat{\mathbf{r}}^{[t]} = \frac{\mathbf{r}^{[t]}}{(1 - \rho_2^{t})}$
\end{framei}


\begin{framev}{Comparison of optimizers: Animation}
\vfill
\splitV[0.49]{
\imageC[1][https://giphy.com/embed/SJVFO3IcVC0M0]{figure_man/sattle_point2-50.png}
}{
\imageC[1][https://giphy.com/embed/SJVFO3IcVC0M0]{figure_man/sattle_point2-100.png}
}
\vfill
\textbf{Comparison} of SGD optimizers near saddle point \\
\textbf{Left:} After start.
\textbf{Right:} Later \\
All methods accelerate compared to vanilla SGD \\
Best is RMSProp, then AdaGrad (Adam is missing here)
\vfill
{\scriptsize Credits: Dettmers (2015) and Radford}
\end{framev}


\begin{framev}{Comparison on quadratic form}
\image{figure_man/momentum/comparison_adam.png}
SGD vs SGD with Momentum vs Adam on a quadratic form
\end{framev}

\endlecture
\end{document}
