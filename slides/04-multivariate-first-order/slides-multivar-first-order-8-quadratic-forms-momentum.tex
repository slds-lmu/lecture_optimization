\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
First order methods
}{
Momentum on quadratic forms
}{
figure_man/momentum_conv_osc.png
}{
\item Momentum update in Eigenspace
\item Effect of $\varphi$
}

\begin{frame2}{Momentum update}
$$
\boldsymbol{\nu}^{[ t+1 ] } = \varphi \boldsymbol{\nu}^{[ t ] } + \alpha \nabla f(\xv^{[ t ] })
$$
$$
\xv^{[ t+1 ] } = \xv^{[ t ] } - \boldsymbol{\nu}^{[ t+1 ] }
$$
which simplifies to
$$
\boldsymbol{\nu}^{[ t+1 ] } = \varphi \boldsymbol{\nu}^{[ t ] } + \alpha (\Amat \xv^{[ t ] } - b)
$$
$$
\xv^{[ t+1 ] } = \xv^{[ t ] } - \boldsymbol{\nu}^{[ t+1 ] }
$$
for the quadratic form.
\end{frame2}


\begin{frame2}[fs=footnotesize]{Dynamics of momentum}
Change basis as before with $\boldsymbol{w}^{[ t ] } = \bm{V}^\top (\xv^{[ t ] } - \xv^\ast)$ and $\boldsymbol{u}^{[ t ] } = \bm{V} \boldsymbol{\nu}^{[ t ] }$, again each component acts independently, but $w_i^{[ t ] }$ and $u_i^{[ t ] }$ are coupled:
$$
u_i^{[ t+1 ] } = \varphi u_i^{[ t ] } + \alpha \lambda_i w_i^{[ t ] }
$$
$$
w_i^{[ t+1 ] } = w_i^{[ t ] } - u_i^{[ t+1 ] }
$$
We rewrite this:
$$
\begin{pmatrix}
1 & 0 \\
1 & 1
\end{pmatrix}
\begin{pmatrix}
u_i^{[ t+1 ] } \\
w_i^{[ t+1 ] }
\end{pmatrix} =
\begin{pmatrix}
\varphi & \alpha \lambda_i \\
0 & 1
\end{pmatrix}
\begin{pmatrix}
u_i^{[ t ] } \\
w_i^{[ t ] }
\end{pmatrix}
$$
inverting the matrix on the LHS, and unravel the recursion:
$$
\begin{pmatrix}
1 & 0 \\
1 & 1
\end{pmatrix}^{-1} =
\begin{pmatrix}
1 & 0 \\
-1 & 1
\end{pmatrix}
\quad ; \quad
\begin{pmatrix}
u_i^{[ t+1 ] } \\
w_i^{[ t+1 ] }
\end{pmatrix} =
\begin{pmatrix}
\varphi & \alpha \lambda_i \\
-\varphi & 1 - \alpha \lambda_i
\end{pmatrix}
\begin{pmatrix}
u_i^{[ t ] } \\
w_i^{[ t ] }
\end{pmatrix} = R^{ t+1 }
\begin{pmatrix}
u_i^{0} \\
w_i^{0}
\end{pmatrix}
$$
\end{frame2}


\begin{frame2}{Matrix power representation}
Taking a $2 \times 2$ matrix to the $t^{th}$ power can be expressed via its eigenvalues, $\sigma_1$ and $\sigma_2$, where $R_j = \frac{R - \sigma_j I}{\sigma_1 - \sigma_2}$:
$$
R^t = \begin{cases}
\sigma_1^t R_1 - \sigma_2^t R_2, & \text{if} \, \sigma_1 \neq \sigma_2 \\
\sigma_1^t (t R/\sigma_1 - (t-1) I), & \text{if} \, \sigma_1 = \sigma_2
\end{cases}
$$
\begin{itemize}
\item Careful, R is not symmetric, so the EVs can be complex
\item In contrast to GD, where we got one geometric series, we have two coupled series with real or complex values
\end{itemize}
\end{frame2}


\begin{frame2}{Eigenvalues of recursion matrix}
The eigenvalues of an arbitrary $2 \times 2$ matrix are:
$$
A = \begin{pmatrix} a & b \\ c & d \end{pmatrix}\quad \text{and}\quad
\lambda_{1,2} = \frac{\trace(A) \pm \sqrt{ \trace(A)^2 - 4 \det(A)}}{2}
$$
For us this is:
$$
\sigma_{1,2} = \frac{1 + \phi - \alpha \lambda_i \pm \sqrt{ (1 + \phi - \alpha \lambda_i)^2 - 4 \phi }}{2}
$$
\begin{itemize}
\item We need both $|\sigma_1|, |\sigma_2| \leq 1$ for convergence
\item For the complex case this reduces to $2 \sqrt{\phi}$, which is surprisingly independent of $\alpha$ and $\lambda_i$
\item For the real case we cannot simplify
\end{itemize}
\end{frame2}


\begin{frame2}[fs=footnotesize]{Momentum convergence regions}
\imageC[0.5]{figure_man/momentum_convergence.png}
\vfill
Convergence rate is the slowest of $\text{max} \{|\sigma_1|, |\sigma_2| \}$.
Each region shows different convergence behavior.
\end{frame2}


\begin{frame2}[fs=footnotesize]{Momentum with complex eigenvalues}
\splitV[0.48]
{\imageC[1]{figure_man/momentum_conv_ripples.png}}
{\imageC[1]{figure_man/momentum_ripples.png}}
\vfill
The eigenvalues of $R$ are complex and we see low frequency ripples.
\end{frame2}


\begin{frame2}[fs=footnotesize]{Momentum with positive eigenvalues}
\splitV[0.48]
{\imageC[1]{figure_man/momentum_conv_mono.png}}
{\imageC[1]{figure_man/momentum_mono.png}}
\vfill
Here, both eigenvalues of $R$ are positive with their norm being less than $1$. This behavior resembles gradient descent.
\end{frame2}


\begin{frame2}[fs=footnotesize]{One-step convergence}
\splitV[0.48]
{\imageC[1]{figure_man/momentum_conv_1step.png}}
{\imageC[1]{figure_man/momentum_1step.png}}
\vfill
The step size is $\alpha = 1/\lambda_i$ and $\varphi = 0$ - we converge in one step.
\end{frame2}


\begin{frame2}[fs=footnotesize]{Oscillating iterates}
\splitV[0.48]
{\imageC[1]{figure_man/momentum_conv_osc.png}}
{\imageC[1]{figure_man/momentum_osc.png}}
\vfill
When $\alpha > 1/\lambda_i$, the iterates flip sign every iteration.
\end{frame2}


\begin{frame2}[fs=footnotesize]{Diverging iterates}
\splitV[0.48]
{\imageC[1]{figure_man/momentum_conv_div.png}}
{\imageC[1]{figure_man/momentum_div.png}}
\vfill
If $\text{max} \{|\sigma_1|, |\sigma_2| \} > 1$, the iterates diverge.
\end{frame2}


\begin{framei}{Convergence conditions}
\item If we combine all conditions for convergence, we can see:
$$
0 < \alpha \lambda_i < 2 + 2 \phi\qquad \text{for}\qquad 0 \leq \phi < 1
$$
\item Comparing this with the results from before ($\phi = 0$), we see that we gain a stepsize factor of 2 before we diverge!
\item Can obtain global convergence rate by optimizing over $\alpha$ and $\phi$
\item More involved, see blogpost for details
\item We get $\alpha = \left(\frac{2}{\sqrt{\lambda_1} + \sqrt{\lambda_n}}\right)^2$
and $\phi = \left(\frac{\sqrt{\lambda_n} - \sqrt{\lambda_1}}{\sqrt{\lambda_n} + \sqrt{\lambda_1}} \right)^2$
\item Results in convergence rate of $\frac{\sqrt{\kappa} - 1}{\sqrt{\kappa} + 1}$
\end{framei}


\begin{framei}{Practical parameter choices}
\item Compared to GD with $\frac{\kappa - 1}{\kappa + 1}$ this is much better as the condition is rooted
\item Of course, this would in principle require knowledge of the EVs $\lambda_i$
\item But we can derive simple rule-of-thumb: for poorly conditioned problems, the stepsize is approximately twice that of GD and $\phi$ close to 1
\item So we want to set $\phi$ to a high value and then still pick the highest $\alpha$ which still converges
\end{framei}

\endlecture
\end{document}
