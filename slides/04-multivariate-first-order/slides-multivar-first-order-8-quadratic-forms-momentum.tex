\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  First order methods
  }{% Lecture title  
  Momentum on quadratic forms
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/momentum_conv_osc.png
  }{
    \item Momentum update in Eigenspace
    \item Effect of $\varphi$
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{vbframe}{Momentum update}

\begin{eqnarray*}
 \boldsymbol{\nu}^{[ t+1 ] } &=& \varphi \boldsymbol{\nu}^{[ t ] } + \alpha \nabla f(\xv^{[ t ] }) \\
 \xv^{[ t+1 ] } &=& \xv^{[ t ] } - \boldsymbol{\nu}^{[ t+1 ] }, 
\end{eqnarray*}

which simplifies to 

\begin{eqnarray*}
	\boldsymbol{\nu}^{[ t+1 ] } &=& \varphi \boldsymbol{\nu}^{[ t ] } + \alpha (\Amat \xv^{[ t ] } - b)  \\
	\xv^{[ t+1 ] } &=& \xv^{[ t ] } - \boldsymbol{\nu}^{[ t+1 ] }, 
\end{eqnarray*}

for the quadratic form. 

\end{vbframe}

\begin{vbframe}{Dynamics of momentum}
\begin{footnotesize}
Change basis as before with $\boldsymbol{w}^{[ t ] } = \bm{V}^\top (\xv^{[ t ] } - \xv^\ast)$ and $\boldsymbol{u}^{[ t ] } = \bm{V} \boldsymbol{\nu}^{[ t ] }$, again each component acts independently, but $w_i^{[ t ] }$ and $u_i^{[ t ] }$ are coupled: 

\begin{eqnarray*}
	u_i^{[ t+1 ] } &=& \varphi u_i^{[ t ] } + \alpha \lambda_i w_i^{[ t ] }, \\
	w_i^{[ t+1 ] } &=& w_i^{[ t ] } - u_i^{[ t+1 ] }
\end{eqnarray*}

We rewrite this: 

\begin{equation*}
\begin{pmatrix}
	1 & 0 \\
	1 & 1 
\end{pmatrix}  \begin{pmatrix}
u_i^{[ t+1 ] } \\
w_i^{[ t+1 ] }
\end{pmatrix} = 
\begin{pmatrix}
	\varphi & \alpha \lambda_i \\
	0 & 1 
\end{pmatrix}  \begin{pmatrix}
	u_i^{[ t ] } \\
	w_i^{[ t ] }
\end{pmatrix}
\end{equation*}

inverting the the matrix on the LHS, and unravel the recursion:


\begin{equation*}
\begin{pmatrix}
	1 & 0 \\
	1 & 1 
\end{pmatrix}^{-1} =\begin{pmatrix}
	1 & 0 \\
	-1 & 1 
\end{pmatrix}  \text{  ;  }
\begin{pmatrix}
		u_i^{[ t+1 ] } \\
		w_i^{[ t+1 ] }
	\end{pmatrix} = 
	\begin{pmatrix}
		\varphi & \alpha \lambda_i \\
		-\varphi & 1 - \alpha \lambda_i
	\end{pmatrix}  \begin{pmatrix}
		u_i^{[ t ] } \\
		w_i^{[ t ] }
	\end{pmatrix} = R^{ t+1 }  \begin{pmatrix}
	u_i^{0} \\
	w_i^{0}
\end{pmatrix}

\end{equation*}

\end{footnotesize}

\framebreak

Taking a $2 \times 2$ matrix to the $t^{th}$ power can be expressed via its eigenvalues, 
$\sigma_1$ and $\sigma_2$, where $R_j = \frac{R - \sigma_j I}{\sigma_1 - \sigma_2}$:

$$
R^t=\begin{cases}
\sigma_1^t R_1 - \sigma_2^t R_2, & \text{if} \, \sigma_1 \neq \sigma_2 \\
\sigma_1^t (t R/\sigma_1 - (t-1) I), & \text{if} \, \sigma_1 = \sigma_2 
\end{cases}
$$





\begin{itemize}
\item Careful, R is not symmetric, so the EVs can be complex
\item In contrast to GD, where we got one geometric series, we have two coupled series with real or complex values
\end{itemize}


\framebreak

The eigenvalues of an arbitrary 2x2 matrix are: 
$$ A =\begin{pmatrix} a & b \\ c & d \end{pmatrix} \qquad
\lambda_{1,2} =  \frac{\trace(A)  \pm \sqrt{ \trace(A)^2 -  4\det(A)}}{2}
$$

For us this is :
$$
\sigma_{1,2} =  \frac{1+\phi-\alpha \lambda_i \pm \sqrt{ (1+\phi-\alpha \lambda_i)^2  -  4\phi }}{2}
$$

\begin{itemize}
 \item We need both $|\sigma_1|,|\sigma_2| \leq 1$ for convergence
 \item For the complex case this reduces to $2 \sqrt{\phi}$, which is surprisingly independent of $\alpha$ and $\lambda_i$
\item For the real case we cannot simplify 

\end{itemize}

\framebreak
\begin{figure}
	\includegraphics[width=0.5\textwidth, keepaspectratio]{figure_man/momentum_convergence.png} \\
	\begin{footnotesize} 
		Convergence rate is the slowest of $\text{max} \{|\sigma_1|, |\sigma_2| \}$.\\
        Each region shows different convergence behavior. 
	\end{footnotesize}
\end{figure}

\framebreak

%Our convergence criterion is $\text{max} \{|\sigma_1|, |\sigma_2| \} < 1$
%\begin{figure}
%	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/momentum_convergence.png} \\
%	\begin{footnotesize} 
%		Depending on $\text{max} \{|\sigma_1|, |\sigma_2| \}$, each region shows different convergence behavior. 
%	\end{footnotesize}
%\end{figure}

\framebreak
\vspace*{1.0cm}
\begin{figure}
	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/momentum_conv_ripples.png} ~~ \includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/momentum_ripples.png} \\
	\begin{footnotesize} 
		The eigenvalues of $R$ are complex and we see low frequency ripples. 
	\end{footnotesize}
\end{figure}

\framebreak
\vspace*{1.0cm}
\begin{figure}
	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/momentum_conv_mono.png} ~~ \includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/momentum_mono.png} \\
	\begin{footnotesize} 
		Here, both eigenvalues of $R$ are positive with their norm being less than $1$. This behavior resembles gradient descent. 
	\end{footnotesize}
\end{figure}

\framebreak
\vspace*{1.0cm}
\begin{figure}
	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/momentum_conv_1step.png} ~~ \includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/momentum_1step.png} \\
	\begin{footnotesize} 
		The step size is $\alpha = 1/\lambda_i$ and $\varphi = 0$ - we converge in one step.
	\end{footnotesize}
\end{figure}
\framebreak
\vspace*{1.0cm}
\begin{figure}
	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/momentum_conv_osc.png} ~~ \includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/momentum_osc.png} \\
	\begin{footnotesize} 
		When $\alpha > 1/\lambda_i$, the iterates flip sign every iteration. 
	\end{footnotesize}
\end{figure}
\framebreak
\vspace*{1.0cm}
\begin{figure}
	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/momentum_conv_div.png} ~~ \includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/momentum_div.png} \\
	\begin{footnotesize} 
		If  $\text{max} \{|\sigma_1|, |\sigma_2| \} > 1$, the iterates diverge. 
	\end{footnotesize}
\end{figure}

\framebreak

\begin{itemize}
\item If we combine all conditions for convergence, we can see: 
$$ 0 < \alpha \lambda_i < 2  + 2\phi  \qquad \text{ for } \qquad 0 \leq \phi < 1$$
\item Comparing this with the results from before ($\phi=0$), we see that we gain a stepsize factor of 2 before we diverge!
\item Can obtain global convergence rate by optimizing over $\alpha$ and $\phi$
\item More involved, see blogpost for details
\item We get $\alpha = \left(\frac{2}{\sqrt{\lambda_1} + \sqrt{\lambda_n}}\right)^2 $
 and $\phi = \left(\frac{\sqrt{\lambda_n} - \sqrt{\lambda_1}}{\sqrt{\lambda_n} + \sqrt{\lambda_1}} \right)^2 $
\item Results in convergence rate of $\frac{\sqrt{\kappa}-1}{\sqrt{\kappa}+1}$
\end{itemize}
\framebreak

\begin{itemize}

\item Compared to GD with $\frac{\kappa-1}{\kappa+1}$ this is much better as the condition is rooted

\item Of course, this would in principle require knowledge of the EVs $\lambda_i$
\item But we can derive simple rule-of-thump: for poorly conditioned problems, the stepsize is approximately twice that of GD and $\phi$ close to 1
\item So we want to set $\phi$ to a high value and then still pick the highest $\alpha$ which still converges
\end{itemize}
\end{vbframe}
% \framebreak
% \begin{itemize}
% 	\item Finally, we investigate the role of $\varphi$. 
% 	\item We can think of gradient descent with momentum as a damped harmonic oscillator: a weight on a spring. We pull the weight down and study the path back to the equilibrium in phase space (looking at the position and the velocity). 
% 	\item Depending on the choice of $\varphi$, the rate of return to the equilibrium position is affected. 
% \end{itemize}

% \begin{figure}
% 	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/phasespace_underdamping.png} 
% \end{figure}

% \framebreak

% \begin{figure}
% 	\includegraphics[height=0.35\textwidth, keepaspectratio]{figure_man/momentum_damping.png} 
% 	\begin{footnotesize}
		 
% 		Left: If $\varphi$ is too large, we are underdamping. The spring oscillates back and forth and misses the optimum. 
		
% 		Middle: The best value of $\varphi$ lies in the middle. 
		
% 		Right: If $\varphi$ is too small, we are overdamping, meaning that the spring experiences too much friction and stops before reaching the equilibrium. 
% 	\end{footnotesize}
% \end{figure}




\endlecture
\end{document}

