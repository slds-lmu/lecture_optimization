\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
First order methods
}{
GD with Momentum
}{
figure_man/momentum_illustration_medium.png
}{
\item Recap of GD problems
\item Momentum definition
\item Unrolling formula
\item Examples
\item Nesterov
}

\begin{framei}{Recap: Weaknesses of Gradient Descent}
\item \textbf{Zig-zagging behavior:} For ill-conditioned problems, GD moves with a zig-zag course to the optimum, since the gradient points approximately orthogonal in the shortest direction to the minimum.
\item \textbf{Slow crawling:} may vanish rapidly close to stationary points (e.g. saddle points) and hence also slows down progress.
\item \textbf{Trapped in stationary points:} In some functions GD converges to stationary points (e.g. saddle points) since gradient on all sides is fairly flat and the step size is too small to pass this flat part.
\spacer
\item[] \textbf{Aim:} More efficient algorithms which quickly reach the minimum.
\end{framei}


\begin{frame2}{GD with Momentum}
\splitV[0.48]
{
\begin{itemize}
\item \textbf{Idea:} ``Velocity'' $\bm{\nu}$: Increasing if successive gradients point in the same direction but decreasing if they point in opposite directions
\item $\bm{\nu}$ is weighted moving average of previous gradients:
\begin{align*}
\bm{\nu}^{[t+1]} &= \varphi \bm{\nu}^{[t]} - \alpha \nabla f(\xv^{[t]}) \\
\xv^{[t+1]} &= \xv^{[t]} + \bm{\nu}^{[t+1]}
\end{align*}
\item $\varphi \in [0,1)$ is additional hyperparameter
\end{itemize}
}
{
\imageC[1]{figure_man/momentum_illustration_medium.png} %SOURCE: Khandewal, GD with Momentum, RMSprop and Adam Optimizer, 2020.
\smallskip
\footnotesize Source: Khandewal, \emph{GD with Momentum, RMSprop and Adam Optimizer}, 2020.
}
\end{frame2}


\begin{framei}{GD with Momentum: Characteristics}
\item Length of a single step depends on how large and aligned a sequence of gradients is
\item Length of a single step grows if many successive gradients point in the same direction
\item $\varphi$ determines how strongly previous gradients are included in $\bm{\nu}$
\item Common values for $\varphi$ are 0.5, 0.9 and even 0.99
\item In general, the larger $\varphi$ is in relation to $\alpha$, the more strongly previous gradients influence the current direction
\item \textbf{Special case} $\varphi = 0$: ``vanilla'' gradient descent
\item \textbf{Intuition:} GD with ``short term memory'' for the direction of motion
\end{framei}


\begin{frame2}[footnotesize]{Momentum: Analysis}
\begin{equation*}
\begin{aligned}
\bm{\nu}^{[1]} &= \textcolor{blue}{\varphi \bm{\nu}^{[0]} - \alpha \nabla f(\xv^{[0]})} \\[0.1cm]
\xv^{[1]} &= \xv^{[0]} + \textcolor{blue}{\varphi \bm{\nu}^{[0]} - \alpha \nabla f(\xv^{[0]})} \\[0.1cm]
\bm{\nu}^{[2]} &= \textcolor{red}{\varphi \bm{\nu}^{[1]} - \alpha \nabla f(\xv^{[1]})} \\
&= \textcolor{red}{\varphi} \textcolor{blue}{(\varphi \bm{\nu}^{[0]} - \alpha \nabla f(\xv^{[0]}))} - \textcolor{red}{\alpha \nabla f(\xv^{[1]})} \\[0.1cm]
\xv^{[2]} &= \xv^{[1]} +\textcolor{red}{\varphi} \textcolor{blue}{(\varphi \bm{\nu}^{[0]} - \alpha \nabla f(\xv^{[0]}))} - \textcolor{red}{\alpha \nabla f(\xv^{[1]})} \\[0.1cm]
\bm{\nu}^{[3]} &= \textcolor{green}{\varphi \bm{\nu}^{[2]} - \alpha \nabla f(\xv^{[2]})} \\
&= \textcolor{green}{\varphi} (\textcolor{red}{\varphi} \textcolor{blue}{(\varphi \bm{\nu}^{[0]} - \alpha \nabla f(\xv^{[0]}))} - \textcolor{red}{\alpha \nabla f(\xv^{[1]})}) - \textcolor{green}{\alpha \nabla f(\xv^{[2]})} \\[0.1cm]
\xv^{[3]} &= \xv^{[2]} + \textcolor{green}{\varphi} (\textcolor{red}{\varphi} \textcolor{blue}{(\varphi \bm{\nu}^{[0]} - \alpha \nabla f(\xv^{[0]}))} - \textcolor{red}{\alpha \nabla f(\xv^{[1]})}) - \textcolor{green}{\alpha \nabla f(\xv^{[2]})}  \\
&= \xv^{[2]} + \varphi^3\bm{\nu}^{[0]} - \varphi^2\alpha\nabla f(\xv^{[0]}) - \varphi \alpha \nabla f(\xv^{[1]}) - \alpha \nabla f(\xv^{[2]}) \\
&= \xv^{[2]} - \alpha(\varphi^2 \nabla f(\xv^{[0]}) + \varphi^1 \nabla f(\xv^{[1]}) + \varphi^0 \nabla f(\xv^{[2]})) + \varphi^3 \bm{\nu}^{[0]} \\
\xv^{[t+1]} &= \xv^{[t]} - \alpha \displaystyle\sum_{j = 0}^{t} \varphi^j \nabla f(\xv^{[t-j]}) + \varphi^{t+1}\bm{\nu}^{[0]}
\end{aligned}
\end{equation*}
\end{frame2}


\begin{frame2}[footnotesize]{Momentum: Intuition}
Suppose momentum always observes the same gradient $\nabla f(\xv^{[t]})$:
\begin{align*}
\xv^{[t+1]} &= \xv^{[t]} - \alpha \displaystyle\sum_{j = 0}^{t} \varphi^j  \nabla f(\xv^{[j]}) + \varphi^{t+1}\bm{\nu}^{[0]} \\
&= \xv^{[t]} - \alpha  \nabla f(\xv^{[t]}) \displaystyle\sum_{j = 0}^{t} \varphi^j + \varphi^{t+1}\bm{\nu}^{[0]} \\
&= \xv^{[t]} - \alpha  \nabla f(\xv^{[t]}) \frac{1 - \varphi^{t+1}}{1 - \varphi} + \varphi^{t+1} \bm{\nu}^{[0]} \\
&\to \xv^{[t]} - \alpha  \nabla f(\xv^{[t]}) \frac{1}{1 - \varphi} \qquad \text{ for } t \to \infty. 
\end{align*}
Momentum accelerates along $- \nabla f(\xv^{[t]})$ to terminal velocity yielding step size $\alpha / (1 - \varphi)$.\\
\spacer
\textbf{Example:} Momentum with $\varphi = 0.9$ corresponds to a tenfold increase in original step size $\alpha$ compared to vanilla gradient descent
\end{frame2}


\begin{frame2}[footnotesize]{Momentum: Intuition (Example)}
Vector $\bm{\nu}^{[3]}$ (for $\bm{\nu}^{[0]} = 0$):
\begin{align*}
\bm{\nu}^{[3]} &= \varphi (\varphi (\varphi \bm{\nu}^{[0]} - \alpha \nabla f(\xv^{[0]})) - \alpha \nabla f(\xv^{[1]})) - \alpha \nabla f(\xv^{[2]}) \\
&= - \varphi^2\alpha\nabla f(\xv^{[0]}) - \varphi \alpha \nabla f(\xv^{[1]}) - \alpha \nabla f(\xv^{[2]})
\end{align*}
\imageC[0.7]{figure_man/momentum_vectors.png}
\spacer
\begin{footnotesize}
Successive gradients pointing in same/different directions increase/decrease velocity.\\
Further geometric intuitions and detailed explanations: \url{https://distill.pub/2017/momentum/}
\end{footnotesize}
\end{frame2}


\begin{frame2}{GD with Momentum: Zig-Zag Behaviour}
Consider a two-dimensional quadratic form $\fx = x_1^2/2 + 10 x_2$.\\
\medskip
Let $\xv^{[0]} = (10, 1)^\top$ and $\alpha = 0.1$.
\imageC[0.8]{figure_man/momentum/compare_gd_momentum.png}
\smallskip
\footnotesize GD shows stronger zig-zag behaviour than GD with momentum.
\end{frame2}


\begin{frame2}{Momentum: Caution}
\textbf{Caution:}
\begin{itemize}
\item If momentum is too high, minimum is possibly missed
\item We might go back and forth around or between local minima
\end{itemize}
\imageC[0.8]{figure_man/momentum/comparison_momentum_overshoot.png}
\end{frame2}


\begin{frame2}{GD with Momentum: Saddle Points}
Consider the two-dimensional quadratic form $f(\xv) = x_{1}^{2} - x_{2}^{2}$ with a saddle point at $(0, 0)^\top$.\\
\medskip
Let $\xv^{[0]} = (-1/2, 10^{-3})^\top$ and $\alpha = 0.1$.
\imageC[0.7]{figure_man/momentum/sgd_momentum_saddlepoint.png}
\spacer
\begin{footnotesize}
GD was slowing down at the saddle point (vanishing gradient).\\
GD with momentum ``breaks out'' of the saddle point and moves on.
\end{footnotesize}
\end{frame2}


\begin{frame2}{ERM for NN with GD}
Let $\D = \Dset$, with $y = x_1^2 + x_2^2$ and minimize
$$
\risket = \sumin \left(\fxt - \yi\right)^2
$$
where $\fxt$ is a neural network with 2 hidden layers (2 units each).\\
\imageC[0.59]{figure_man/gradient_descent_NN_0.pdf}
\end{frame2}


\begin{frame2}{ERM for NN with GD: 10 Iterations}
After $10$ iters of GD:
\splitV[0.55]{
\imageC[1]{figure_man/gradient_descent_NN_10_surface_0.5.pdf}
}{
\imageC[1]{figure_man/gradient_descent_NN_300_history_0.5.pdf}}
\end{frame2}


\begin{frame2}{ERM for NN with GD: 100 Iterations}
After $100$ iters of GD:
\splitV[0.55]{
\imageC[1]{figure_man/gradient_descent_NN_100_surface_0.5.pdf}
}{
\imageC[1]{figure_man/gradient_descent_NN_300_history_0.5.pdf}}
\end{frame2}


\begin{frame2}{ERM for NN with GD: 300 Iterations}
After $300$ iters of GD:
\splitV[0.55]{
\imageC[1]{figure_man/gradient_descent_NN_300_surface_0.5.pdf}
}{
\imageC[1]{figure_man/gradient_descent_NN_300_history_0.5.pdf}}
\end{frame2}


\begin{frame2}{Gradient Descent with and without Momentum}
\centering Gradient Descent with and without momentum
\medskip
\splitVCC[0.5]{
\imageC[1]{figure_man/gradient_descent_NN_300_history_0.5.pdf}
}{
\imageC[1]{figure_man/gradient_descent_NN_300_history_0.pdf}}
\end{frame2}


\begin{framei}{Nesterov Accelerated Gradient}
\item Slightly modified version: \textbf{Nesterov accelerated gradient}
\item Stronger theoretical convergence guarantees for convex functions
\item Avoid moving back and forth near optima
\begin{align*}
\bm{\nu}^{[t+1]} &= \varphi \bm{\nu}^{[t]} - \alpha \nabla f(\textcolor{orange}{\xv^{[t]} + \varphi \bm{\nu}^{[t]}}) \\
\xv^{[t+1]} &= \xv^{[t]} + \bm{\nu}^{[t+1]}
\end{align*}
\imageC[0.8]{figure_man/nesterov.jpeg}
\vfill
\begin{footnotesize}  
Nesterov momentum update evaluates gradient at the ``look-ahead'' position.\\
Source: \url{https://cs231n.github.io/neural-networks-3/}
\end{footnotesize}
\end{framei}


\begin{frame2}[fs=footnotesize]{Momentum vs. Nesterov}
\imageC[0.8]{figure_man/nesterov_momentum.png}
\vfill
GD with momentum (\textbf{left}) vs. GD with Nesterov momentum (\textbf{right}).\\
Near minima, momentum makes a large step due to gradient history.\\
Nesterov mom. ``looks ahead'' and reduces effect of gradient history.\\
Source: Chandra, 2015.
\end{frame2}

\endlecture
\end{document}
