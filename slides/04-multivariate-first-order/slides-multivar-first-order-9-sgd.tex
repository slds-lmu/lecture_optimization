\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
First order methods
}{
SGD
}{
figure_man/SGD_cropped.png
}{
\item Understand stochastic gradient descent
\item Understand stochasticity and variance of SGD
\item Know convergence properties of SGD
\item Understand effect of batch size
}

\begin{framei}{Stochastic gradient descent}
\item NB: We use $g$ instead of $f$ as objective, because $f$ is used as model in ML
\item $g: \R^d \to \R$ objective, $g$ is \textbf{average over functions}:
$$
g(\xv) = \frac{1}{n}\sumin g_i(\xv), \qquad g \text{ and } g_i \text{ smooth}
$$
\item Stochastic gradient descent (SGD) approximates the gradient
\begin{eqnarray*}
\nabla_\xv g(\xv) = \frac{1}{n}\sumin \nabla_{\xv} g_i(\xv) &:=& \mathbf{d} \quad \text{ by }\\
\frac{1}{\textcolor{blue}{|J|}}\sum_{i \in \textcolor{blue}{J}} \nabla_\xv g_i(\xv) &:=& \hat{\mathbf{d}}, 
\end{eqnarray*}
with random subset $J \subset \{1, 2, \ldots, n\}$ of gradients called \textbf{mini-batch}
\item This is done e.g. when computing the true gradient is \textbf{expensive}
\end{framei}


\begin{frame2}{SGD algorithm}
\begin{algorithm}[H]
\footnotesize
\caption{Basic SGD pseudo code}
\begin{algorithmic}[1]
\State Initialize $\xv^{[0]}$, $t = 0$ 
\While{stopping criterion not met}
\State Randomly shuffle indices and partition into minibatches $J_1, \ldots, J_K$ of size $m$
\For{$k\in\{1,\ldots,K\}$} 
\State $t \leftarrow t + 1$ 
\State Compute gradient estimate with $J_k$: $\hat{\mathbf{d}}^{[t]} \leftarrow \frac{1}{m} \sum_{i \in J_k} \nabla_\xv g_i(\xv^{[t - 1]}) $
\State Apply update: $\xv^{[t]} \leftarrow \xv^{[t-1]} - \alpha \cdot \hat{\mathbf{d}}^{[t]}$
\EndFor
\EndWhile
\end{algorithmic}
\end{algorithm}
\begin{itemize}
\item Instead of drawing batches randomly we might want to go through the $g_i$ sequentially (unless $g_i$ are sorted in any way)
\item Updates are computed faster, but also more stochastic:
\begin{itemize}
\item In the simplest case, batch-size $m := |J_k|$ is set to $m = 1$
\item If $n$ is a billion, computation of update is a billion times faster
\item \textbf{But} (later): Convergence rates suffer from stochasticity!
\end{itemize}
\end{itemize}
\end{frame2}


\begin{framei}{SGD in ML}
\item In ML, we perform ERM:
$$
\risk(\thetav) = \frac{1}{n}\sumin \underbrace{\Lxyit}_{g_i(\thetav)}
$$
\item for a data set $\D = \Dset$
\item a loss function $\Lxy$, e.g., L2 loss $\Lxy = (y - \fx)^2$
\item and a model class $f$, e.g., the linear model $\fxit = \thetav^\top \xv$
\end{framei}



\begin{framei}{SGD in ML}
\item For large data sets, computing the exact gradient
$$
\mathbf{d} = \frac{1}{n}\sum_{i=1}^n \nabla_{\thetav} \Lxyit
$$
may be expensive or even infeasible to compute and is approximated by
$$
\hat{\mathbf{d}} = \frac{1}{m}\sum_{i \in J} \nabla_{\thetav} \Lxyit,
$$
for $J \subset \{1, 2, \ldots, n\}$ random subset
\item \textbf{NB:} Often, maximum size of $J$ technically limited by memory size
\end{framei}


\begin{framei}{Stochasticity of SGD}
\item[]
\imageC[0.8][https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf]{figure_man/SGD.png}
\begin{small} \begin{center}
Minimize $g(x_1, x_2) = 1.25(x_1 + 6)^2 + (x_2 - 8)^2$
\textbf{Left:} GD, \textbf{Right:} SGD\\
Black line shows average value across multiple runs
\end{center} \end{small}
\end{framei}


\begin{framei}{Stochasticity of SGD}
\item Assume batch size $m = 1$ (statements also apply for larger batches)
\item \textbf{(Possibly) suboptimal direction:} Approximate gradient $\hat{\mathbf{d}} = \nabla_\xv g_i(\xv)$ might point in suboptimal (possibly not even a descent!) direction
\item \textbf{Unbiased estimate:} If $J$ drawn i.i.d., approximate gradient $\hat{\mathbf{d}}$ is an unbiased estimate of gradient $\mathbf{d} = \nabla_\xv g(\xv) = \sumin \nabla_\xv g_i(\xv)$:
\begin{align*}
\E_{i}\left[\nabla_\xv g_i(\xv)\right] &= \sumin \nabla_\xv g_i(\xv) \cdot \P(i = i) \\
&= \sumin \nabla_\xv g_i(\xv) \cdot \frac{1}{n} = \nabla_\xv g(\xv).
\end{align*}
\item \textbf{Conclusion:} SGD might perform single suboptimal moves, but moves in \enquote{right direction} \textbf{on average}
\end{framei}


\begin{frame}{Erratic behavior of SGD}
\textbf{Example:} $g(\xv) = \sum_{i = 1}^5 g_i(\xv)$, $g_i$ quadratic, batch size $m = 1$
\vfill
\only<1>{\image[0.8]{figure_man/sgd_example_iter_1.png}}
\only<2>{\image[0.8]{figure_man/sgd_example_iter_2.png}}
\only<3>{\image[0.8]{figure_man/sgd_example_iter_3.png}}
\only<4>{\image[0.8]{figure_man/sgd_example_iter_4.png}}
\only<5>{\image[0.8]{figure_man/sgd_example_iter_5.png}\\}
\only<5>{\small In iteration $5$, SGD performs a suboptimal move away from the minimum.}
\end{frame}


\begin{framei}{Erratic behavior of SGD}
\item[]
\imageC[0.7]{figure_man/sgd_example_confusion_areas.png}
\begin{center}\begin{small}
\textbf{Blue area}: Each $-\nabla g_i(\xv)$ points towards minimum\\
\textbf{Red area} (\enquote{confusion area}): $-\nabla g_i(\xv)$ might point away from minimum and perform a suboptimal move
\end{small} \end{center}
\end{framei}


\begin{framei}{Erratic behavior of SGD}
\item At location $\xv$, \enquote{confusion} is captured by variance of gradients
$$
\frac{1}{n}\sumin \|\nabla_\xv g_i(\xv) - \nabla_\xv g(\xv)\|^2
$$
\item If term is $0$, next step goes in gradient direction (for each $i$)
\item If term is small, next step \emph{likely} goes in gradient direction
\item If term is large, next step likely goes in direction different than gradient
\end{framei}


\begin{framei}{Convergence of SGD}
\item As a consequence, SGD has worse convergence properties than GD
\item \textbf{But:} Can be controlled via \textbf{increasing batches} or \textbf{reducing step size}
\vfill
\begin{blocki}{The larger the batch size $m$}
\item the better the approximation to $\nabla_\xv g(\xv)$
\item the lower the variance
\item the lower the risk of performing steps in the wrong direction
\end{blocki}
\vfill
\begin{blocki}{The smaller the step size $\alpha$}
\item the smaller a step in a potentially wrong direction
\item the lower the effect of high variance
\end{blocki}
\vfill
As maximum batch size is usually limited by computational resources (memory), choosing the step size is crucial
\end{framei}


\begin{framei}{Effect of batch size}
\item[]
\imageC[0.9]{figure_man/gradient_descent_NN_SGD_vs_no_SGD.pdf}
\begin{center}\begin{small}
SGD for a NN with batch size $\in \{0.5 \%, 10 \%, 50 \%\}$ of the training data\\
The higher the batch size, the lower the variance
\end{small}\end{center}
\end{framei}

\endlecture
\end{document}
