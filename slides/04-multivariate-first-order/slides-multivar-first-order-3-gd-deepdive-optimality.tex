\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\newcommand{\Hess}{\mathbf{H}}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
  Deep dive
  }{  
  Gradient descent and optimality
  }{
  figure_man/gdes_1.png
  }{
    \item Convergence of GD
    \item Proof strategy and tools
    \item Descent lemma
}

\begin{framei}[sep=L]{Setting}
\item GD is \textbf{greedy}: \textbf{locally optimal} moves in each iteration
\item If $f$ is \textbf{convex}, \textbf{differentiable} and has a \textbf{Lipschitz gradient}, GD converges to global minimum for sufficiently small step sizes.
\vfill
\imageC[0.6]{figure_man/gdes_1.png}
\end{framei}

\begin{framei}{Setting}
\item\textbf{Assumptions:}
\begin{itemizeM}
\item $f$ convex and differentiable
\item Global minimum $\xv^\ast$ exists
\item $f$ has Lipschitz gradient ($\nabla f$ does not change too fast)
\begin{framed}
$$\| \nabla \fx - \nabla f(\tilde{\xv}) \| \le L \|\xv - \tilde{\xv}\| \quad \text{ for all $\xv, \tilde{\xv}$}$$
\end{framed}
\end{itemizeM}
\begin{framed}
\textbf{Theorem \textnormal{(Convergence of GD)}.}
GD with step size $\alpha \leq 1/L$ yields
$$f(\xv^{[k]}) - f(\xv^\ast) \leq \frac{\| \xv^{[0]} - \xv^\ast \|^2}{2\alpha k}.$$
In other words: GD converges with rate $\mathcal{O}(1/k)$.
\end{framed}
\end{framei}

\begin{frame2}{Proof strategy}
\begin{enumerate}
\item Show that $f(\xv^{[t]})$ \textbf{strictly decreases} with each iteration $t$
\begin{framed}
\textbf{Descent lemma:}
$$f(\xv^{[t+1]}) \le f(\xv^{[t]}) - \frac{\alpha}{2} \|\nabla f(\xv^{[t]})\|^2$$
\end{framed}
\item Bound \textbf{error of one step}
\begin{framed}
$$f(\xv^{[t+1]}) - f(\xv^\ast) \leq \frac{1}{2 \alpha} \left( \|\xv^{[t]}-\xv^\ast\|^2 - \| \xv^{[t+1]} - \xv^\ast \|^2 \right)$$
\end{framed}
\item Finalize by \textbf{telescoping} argument
\end{enumerate}
\end{frame2}

\begin{framei}{Main tool}
\item\textbf{Recall:} First order condition of convexity
\begin{framed}\centering
Every tangent line of $f$ is always below $f$.
$$f(\yv) \geq \fx + \nabla \fx^\top (\yv - \xv)$$
\imageC[0.4]{../01-mathematical-concepts/figure_man/conv-first-order-cond.png}
\end{framed}
\end{framei}

\begin{framei}{Descent lemma}
\item\textbf{Recall:} $\nabla f$ Lipschitz $\implies$ $\nabla^2 \fx \preccurlyeq L \cdot \mathbf{I}$ for all $\xv$
\item This gives convexity of $g(\xv) := \frac{L}{2}\|\xv\|^2 - f(\xv)$ since $$\nabla^2 g(\xv) = L \cdot I - \nabla^2 \fx \succcurlyeq 0.$$
\item First order condition of convexity of $g$ yields
\begin{alignat*}{3}
&& g(\xv) &\geq g(\xv^{[t]}) + \nabla g(\xv^{[t]})^\top (\xv - \xv^{[t]}) \\
\Leftrightarrow \quad && \frac{L}{2} \|\xv\|^2 - f(\xv) &\geq \frac{L}{2} \|\xv^{[t]}\|^2 - f(\xv^{[t]}) + (L\xv^{[t]} - \nabla f(\xv^{[t]}))^\top (\xv - \xv^{[t]}) \\
\Leftrightarrow \quad && &\vdotswithin{\geq} \\
\Leftrightarrow \quad && \fx &\leq f(\xv^{[t]}) + \nabla f(\xv^{[t]})^\top (\xv - \xv^{[t]}) + \frac{L}{2} \|\xv - \xv^{[t]}\|^2
\end{alignat*}
\item\textbf{Now:} One GD step with step size $\alpha \leq 1/L$:
$$\xv \leftarrow \xv^{[t+1]} = \xv^{[t]} - \alpha \nabla f\left(\xv^{[t]}\right)$$
\end{framei}

\begin{framei}[fs=small]{Descent lemma}
\item[] \begin{align*}
f(\xv^{[t+1]}) &\leq f(\xv^{[t]}) + \nabla f(\xv^{[t]})^\top(\xv^{[t+1]} - \xv^{[t]}) + \frac{L}{2} \|\xv^{[t+1]} - \xv^{[t]}\|^2 \\
&= f(\xv^{[t]}) + \nabla f(\xv^{[t]})^\top(\xv^{[t]} - \alpha \nabla f(\xv^{[t]}) - \xv^{[t]}) \\
&\qquad + \frac{L}{2} \|\xv^{[t]} - \alpha \nabla f(\xv^{[t]}) - \xv^{[t]}\|^2 \\
&= f(\xv^{[t]}) - \nabla f(\xv^{[t]})^\top \alpha  \nabla f(\xv^{[t]}) + \frac{L}{2} \|\alpha \nabla f(\xv^{[t]})\|^2 \\
&= f(\xv^{[t]}) - \alpha \|\nabla f(\xv^{[t]})\|^2 + \frac{L \alpha^2}{2} \|\nabla f(\xv^{[t]})\|^2 \\
&\le f(\xv^{[t]}) - \frac{\alpha}{2} \|\nabla f(\xv^{[t]})\|^2
\end{align*}
\item\textbf{Note:} $\alpha \leq 1/L$ yields $L \alpha^2 \leq \alpha$
\begin{itemizeS}
\item $\|\nabla f(\xv^{[t]})\|^2 > 0$ unless $\nabla \fx = \mathbf{0}$
\item $f$ \textbf{strictly decreases} with each GD iteration until optimum reached
\item Descent lemma yields bound on \textbf{guaranteed progress} if $\alpha \leq 1/L$ \\
(explains why GD may diverge if step sizes too large)
\end{itemizeS}
\end{framei}

\begin{framei}{One step error bound}
\item Again, first order condition of convexity gives 
$$f(\xv^{[t]}) - f(\xv^\ast) \leq  \nabla f(\xv^{[t]})^\top (\xv^{[t]} - \xv^\ast).$$
\item This and the descent lemma yields
\begin{align*}
f(\xv^{[t+1]}) - f(\xv^\ast) &\leq f(\xv^{[t]})  - \frac{\alpha}{2}\|\nabla f(\xv^{[t]})\|^2  - f(\xv^\ast) \\
&= f(\xv^{[t]})  - f(\xv^\ast)  - \frac{\alpha}{2}\|\nabla f(\xv^{[t]})\|^2 \\
&\leq \nabla f(\xv^{[t]})^\top (\xv^{[t]}-\xv^*) - \frac{\alpha}{2}\|\nabla f(\xv^{[t]})\|^2 \\
&= \frac{1}{2 \alpha} \left( \|\xv^{[t]}-\xv^\ast\|^2 - \| \xv^{[t]} - \xv^\ast - \alpha \nabla f(\xv^{[t]})\|^2 \right) \\
&= \frac{1}{2 \alpha} \left( \|\xv^{[t]}-\xv^\ast\|^2 - \| \xv^{[t+1]} - \xv^\ast \|^2 \right)
\end{align*}
\item\textbf{Note:} Line 3 $\to$ 4 is hard to see (just expand line 4).
\end{framei}

\begin{framei}{Finalization}
\item Summing over iterations yields
\begin{align*}
k ( f(\xv^{[k]}) - f(\xv^\ast)) &\leq \sum_{t=1}^{k} [f(\xv^{[t]}) - f(\xv^\ast)]\\
&\leq \sum_{t=1}^{k} \frac{1}{2 \alpha} \left[ \|\xv^{[t-1]}-\xv^\ast\|^2 - \| \xv^{[t]} - \xv^\ast \|^2 \right] \\
&= \frac{1}{2 \alpha}  \left( \|\xv^{[0]}-\xv^\ast\|^2 - \| \xv^{[k]} - \xv^\ast \|^2 \right) \\
&\leq \frac{1}{2 \alpha} \left( \|\xv^{[0]}-\xv^\ast\|^2 \right).
\end{align*}
\item\textbf{Arguments:} Descent lemma (line~1).\\ Telescoping sum (line~2 $\to$ 3).
\begin{framed}
$$f(\xv^{[t+1]}) - f(\xv^\ast) \leq \frac{\| \xv^{[0]} - \xv^\ast \|^2}{2\alpha k}$$
\end{framed}
\end{framei}	

\endlecture
\end{document}
