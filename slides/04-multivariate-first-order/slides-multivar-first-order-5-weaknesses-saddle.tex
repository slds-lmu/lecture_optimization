\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
First order methods
}{
GD -- Multimodality and Saddle points
}{
figure_man/multimodal.png
}{
\item Multimodality, GD result can be arbitrarily bad
\item Saddle points, major problem in NN error landscapes, GD can get stuck or slow crawling
}

\begin{framei}{Unimodal vs. Multimodal loss surfaces}
\item[]
\splitV[0.52]
{\imageC{figure_man/multimodal.png}\imageC[1][https://arxiv.org/abs/1712.09913]{figure_man/difficult_vs_easy.png}}
{\begin{itemizeM}[small]
\item Snippet of a loss surface with many local optima
\item In deep learning, we often find multimodal loss surfaces.
\item \textbf{Left:} Multimodal loss surface.
\item \textbf{Right:} (Nearly) unimodal loss surface.
\end{itemizeM}}
\end{framei}


\begin{framei}[fs=small]{GD: Only locally optimal moves}
\item GD makes only \textbf{locally} optimal moves
\item It may move away from the global optimum
\imageC[0.7][https://www.deeplearningbook.org]{figure_man/local_hill.png}
\item Initialization on \enquote{wrong} side of the hill results in weak performance
\item In higher dimensions, GD may move around the hill (potentially at the cost of longer trajectory and time to convergence)
\end{framei}


\begin{framei}{Local minima}
\item \textbf{In practice:} Only local minima with high value compared to global minimium are problematic.
\imageC[0.9][https://www.deeplearningbook.org]{figure_man/minima.png}
\end{framei}


\begin{framei}{Local minima: Sensitivity}
\item Small differences in starting point or step size can lead to huge differences in the reached minimum or even to non-convergence
\vfill
\splitV[0.48]{\image{figure_man/nonconv_ackley_plot.jpg}}{\image{figure_man/nonconv_ackley_path.jpg}}
\vfill
\item (Non-)Converging gradient descent for Ackley function
\end{framei}


\begin{frame2}{GD at saddle points}
\splitV[0.52]
{\textbf{Example:}\\
\spacer
$$f(x_1, x_2) = x_1^2 - x_2^2$$
$$\nabla f(x_1, x_2) = (2 x_1, -2 x_2)^T$$
$$\bm{H} = \begin{pmatrix} 2 & 0 \\ 0 & -2 \end{pmatrix}$$
\begin{itemizeM}[small]
\item Along $x_1$, curvature is positive ($\lambda_1 = 2 > 0$).
\item Along $x_2$, curvature is negative ($\lambda_2 = -2 < 0$).
\end{itemizeM}}
{\imageC[0.95]{figure_man/saddlepoint.png}}
\end{frame2}


\frame{
\frametitle{Example: Saddle point with GD}
\begin{itemize}
\item How do saddle points impair optimization?
\item Gradient-based algorithms \textbf{might} get stuck in saddle points
\end{itemize}
\center
\only<1>{\imageC{figure_man/opt1.png}}
\only<2>{\imageC{figure_man/opt2.png}}
\only<3>{\imageC{figure_man/opt3.png}}
\only<4>{\imageC{figure_man/opt10.png}}
\only<5>{\imageC{figure_man/saddle_point_grad_norm.pdf}}
\begin{center}
\only<1>{Red dot: Starting location}
\only<2>{Step 1 ...}
\only<3>{... Step 2 ...}
\only<4,5>{... Step 10 ... got stuck and cannot escape saddle point}
\end{center}
}


\begin{framei}[sep=L]{Saddle points in neural networks}
\item For the empirical risk $\risk : \R^d \rightarrow \R$ of a neural network, the expected ratio of the number of saddle points to local minima typically grows exponentially with $d$
\item In other words: Networks with more parameters (deeper networks or larger layers) exhibit a lot more saddle points than local minima
\item \textbf{Reason:} Hessian at local minimum has only positive eigenvalues.
Hessian at saddle point has positive and negative eigenvalues.
\end{framei}

\endlecture
\end{document}

