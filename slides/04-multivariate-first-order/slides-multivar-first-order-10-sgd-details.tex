\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
First order methods
}{
SGD Further Details
}{
figure_man/SGD_cropped.png
}{
\item Decreasing step size for SGD
\item Stopping rules 
\item SGD with momentum
}

\begin{framei}{SGD with constant step size}
\item Example: SGD with constant step size
\vfill
\imageC[0.9]{figure_man/sgd_example_erratic_behavior.png}
\vfill
\item Fast convergence of SGD initially
\item Erratic behavior later (variance too big)
\end{framei}


\begin{framei}{SGD with decreasing step size}
\item Idea: Decrease step size to reduce magnitude of erratic steps
\item Trade-off:
\begin{itemize}
\item If step size $\alpha^{[t]}$ decreases slowly, large erratic steps
\item If step size decreases too fast, performance is impaired
\end{itemize}
\item SGD converges for sufficiently smooth functions if 
$$
\frac{\sum_{t = 1}^\infty \left(\alpha^{[t]}\right)^2}{\sum_{t = 1}^\infty \alpha^{[t]}} = 0
$$
(``how much noise affects you'' by ``how far you can get'')
\end{framei}
\begin{framei}{SGD with decreasing step size}
\item Popular solution: step size fulfilling $\alpha^{[t]} \in \order(1/t)$
\vfill
\imageC[1]{figure_man/sgd_example_decreasing_step_size.png}
\vfill
\item Example continued: Step size $\alpha^{[t]} = 0.2/t$
\item Often not working well in practice: step size gets small quite fast
\item Alternative: $\alpha^{[t]} \in \order(1/\sqrt{t})$
\end{framei}


\begin{frame2}{Advanced step size control}
\begin{blocki}{Why not Armijo-based step size control?}
\item Backtracking line search or other approaches based on Armijo rule usually not suitable: Armijo condition 
$$
g(\xv + \alpha \mathbf{d}) \le g(\xv) + \gamma_1 \alpha \textcolor{red}{\nabla g(\xv)}^\top \mathbf{d}
$$
requires evaluating full gradient
\item But SGD is used to \emph{avoid} expensive gradient computations
\item Research aims at finding inexact line search methods that provide better convergence behaviour, e.g., Vaswani et al., \emph{Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates}. NeurIPS, 2019
\end{blocki}
\end{frame2}


\begin{framei}{Mini-batches}
\item Reduce noise by increasing batch size $m$ for better approximation
$$
\hat{\mathbf{d}} = \frac{1}{m} \sum_{i \in J} \nabla_\xv g_i(\xv) \approx \frac{1}{n} \sumin \nabla_\xv g_i(\xv) = \mathbf{d} 
$$
\item Usually, the batch size is limited by computational resources (e.g., how much data you can load into the memory)
\vfill
\imageC[1]{figure_man/sgd_example_batch_size.png}
\vfill
\item Example continued: Batch size $m = 1$ vs. $m = 5$
\end{framei}


\begin{framei}{Stopping rules for SGD}
\item For GD: We usually stop when gradient is close to $0$ (i.e., we are close to a stationary point)
\item For SGD: Individual gradients do not necessarily go to zero, and we cannot access full gradient
\item Practicable solution for ML: 
\begin{itemize}
\item Measure the validation set error after $T$ iterations
\item Stop if validation set error is not improving
\end{itemize}
\end{framei}


\begin{frame2}[fs=small]{SGD and ML}
\begin{blocki}{General remarks:}
\item SGD is a variant of GD 
\item SGD particularly suitable for large-scale ML when evaluating gradient is too expensive / restricted by computational resources
\item SGD and variants are the most commonly used methods in modern ML, for example:  
\begin{itemize}
\item Linear models \\
Note that even for the linear model and quadratic loss, where a closed form solution is available, SGD might be used if the size $n$ of the dataset is too large and the design matrix does not fit into memory
\item Neural networks 
\item Support vector machines
\item ...
\end{itemize}
\end{blocki}
\end{frame2}


\begin{framei}[fs=small]{SGD with momentum}
\item SGD is usually used with momentum due to reasons mentioned in previous chapters
\vfill
\begin{algorithm}[H]
\small
\caption{Stochastic gradient descent with momentum}
\begin{algorithmic}[1]
\State \textbf{require} step size $\alpha$ and momentum $\varphi$ \strut
\State \textbf{require} initial parameter $\bm{x}$ and initial velocity $\bm{\nu}$ \strut
\While{stopping criterion not met}
\State Sample mini-batch of $m$ examples
\State Compute gradient estimate $\nabla \hat{g}(\xv)$ using mini-batch
%$\leftarrow  \frac{1}{m} \nabla_{\bm{x}} \sum_{i} \Lxyit$
\State Compute velocity update: $\bm{\nu} \leftarrow \varphi \bm{\nu} - \alpha \nabla \hat{g}(\xv)$
\State Apply update: $\bm{x} \leftarrow \bm{x} + \bm{\nu}$
\EndWhile
\end{algorithmic}
\end{algorithm}
\end{framei}

\endlecture
\end{document}
