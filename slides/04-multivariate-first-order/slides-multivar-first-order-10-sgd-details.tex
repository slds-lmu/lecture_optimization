\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  First order methods
  }{% Lecture title  
  SGD Further Details
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/SGD_cropped.png
  }{
    \item Decreasing step size for SGD
    \item Stopping rules 
    \item SGD with momentum
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{vbframe}{SGD with constant step size}

\textbf{Example}: SGD with constant step size. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\textwidth]{figure_man/sgd_example_erratic_behavior.png}
    \caption*{Fast convergence of SGD initially.
        Erratic behavior later (variance too big).}
\end{figure}

\end{vbframe}


\begin{vbframe}{SGD with decreasing step size}

\begin{itemize}
	\item \textbf{Idea:} Decrease step size to reduce magnitude of erratic steps.
	\item \textbf{Trade-off:}
	\begin{itemize}
		\item if step size $\alpha^{[t]}$ decreases slowly, large erratic steps% variance of $\nabla_\xv g_i(\xv)$ also decreases slowly
		\item if step size decreases too fast, performance is impaired
	\end{itemize}
	\item SGD converges for sufficiently smooth functions if 
	$$
		\frac{\sum_{t = 1}^\infty \left(\alpha^{[t]}\right)^2}{\sum_{t = 1}^\infty \alpha^{[t]}} = 0
	$$
	(\enquote{how much noise affects you} by \enquote{how far you can get}).
\end{itemize}	

\framebreak 

\begin{itemize}
	\item Popular solution: step size fulfilling $\alpha^{[t]} \in \order(1/t)$. 
\end{itemize}

\begin{figure}
    \centering
    \includegraphics[width = 1\textwidth]{figure_man/sgd_example_decreasing_step_size.png}
    \caption*{Example continued.
        Step size $\alpha^{[t]} = 0.2/t$. }
\end{figure}

\vspace{-0.5\baselineskip}

\begin{itemize}
 	\item Often not working well in practice: step size gets small quite fast.
 	\item Alternative: $\alpha^{[t]} \in \order(1/\sqrt{t})$
\end{itemize}

\end{vbframe}

\begin{vbframe}{Advanced step size control} 

\begin{blocki}{Why not Armijo-based step size control? }
    \item Backtracking line search or other approaches based on Armijo rule usually not suitable: Armijo condition 
    $$
          g(\xv + \alpha \mathbf{d}) \le g(\xv) + \gamma_1 \alpha \textcolor{red}{\nabla g(\xv)}^\top \mathbf{d}
    $$
    requires evaluating full gradient.
    \item But SGD is used to \emph{avoid} expensive gradient computations. 
    \item Research aims at finding inexact line search methods that provide better convergence behaviour, e.g., Vaswani et al., \emph{Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates}. NeurIPS, 2019.
\end{blocki}

\end{vbframe}


\begin{vbframe}{Mini-batches}

\begin{itemize}
	\item Reduce noise by increasing batch size $m$ for better approximation
	$$
		\hat{\mathbf{d}} = \frac{1}{m} \sum_{i \in J} \nabla_\xv g_i(\xv) \approx \frac{1}{n} \sumin \nabla_\xv g_i(\xv) = \mathbf{d} 
	$$
	\item Usually, the batch size is limited by computational resources (e.g., how much data you can load into the memory)
\end{itemize}

 	\begin{figure}
 		\vspace{-0.3cm}
 		\centering
 		\includegraphics[width = 1\textwidth]{figure_man/sgd_example_batch_size.png} \newline
		Example continued. Batch size $m = 1$ vs. $m = 5$. 
 	\end{figure}

\end{vbframe}

\begin{vbframe}{Stopping rules for SGD} 

\begin{itemize}
	\item \textbf{For GD}: We usually stop when gradient is close to $0$ (i.e., we are close to a stationary point)
	\item \textbf{For SGD}: individual gradients do not necessarily go to zero, and we cannot access full gradient.
	\item Practicable solution for ML: 
	\begin{itemize}
		\item Measure the validation set error after $T$ iterations
		\item Stop if validation set error is not improving
	\end{itemize}
\end{itemize}


\end{vbframe}


\begin{vbframe}{SGD and ML}

\begin{blocki}{General remarks:}
    \item SGD is a variant of GD 
    \item SGD particularly suitable for large-scale ML when evaluating gradient is too expensive / restricted by computational resources
    \item SGD and variants are the most commonly used methods in modern ML, for example:  
    \begin{itemize}
        \item Linear models \\
            \begin{footnotesize}
                Note that even for the linear model and quadratic loss, where a closed form solution is available, SGD might be used if the size $n$ of the dataset is too large and the design matrix does not fit into memory.
            \end{footnotesize}
        \item Neural networks 
        \item Support vector machines
        \item ...
    \end{itemize}
    % It is worthwhile to test the gradient for robustness.
\end{blocki}

\end{vbframe}

	% \begin{vbframe}{Convergence of SGD}


% 	\begin{blocki}{Convergence:}
% 		% \item Das Robbin-Sigmunds-Theorem zeigt, dass das Verfahren unter
% 		% überraschend milden Vorraussetzungen fast immer sicher
% 		% konvergiert. Auch in Fällen von nicht stetigen Verlustfunktionen.
% 		\item The method has worse convergence properties than the exact gradient descent method 
% 		\item This is due to the approximation of the gradient, which is introduces a source of noise that does not vanish even at the minimum 
% 		\item A crucial parameter is the step-size: 
% 		\begin{itemize}
% 			\item If the step size $\alpha^{[t]}$ decreases too slowly, the variance of $\thetat$ decreases slowly too.
% 			\item If the step size decreases too fast, the parameter vector $\thetat$ reaches its optimum too slowly.
% 		\end{itemize}
% 	\end{blocki}
	

	
% \end{vbframe}



\begin{vbframe}{SGD with momentum}
		
SGD is usually used with momentum due to reasons mentioned in previous chapters.

\begin{algorithm}[H]
    \small
    \caption{Stochastic gradient descent with momentum}
    \begin{algorithmic}[1]
        \State \textbf{require} step size $\alpha$ and momentum $\varphi$ \strut
        \State \textbf{require} initial parameter $\bm{x}$ and initial velocity $\bm{\nu}$ \strut
        \While{stopping criterion not met}
        \State Sample mini-batch of $m$ examples
        \State Compute gradient estimate $\nabla \hat{g}(\xv)$ using mini-batch
        %$\leftarrow  \frac{1}{m} \nabla_{\bm{x}} \sum_{i} \Lxyit$
        \State Compute velocity update: $\bm{\nu} \leftarrow \varphi \bm{\nu} - \alpha \nabla \hat{g}(\xv)$
        \State Apply update: $\bm{x} \leftarrow \bm{x} + \bm{\nu}$
        \EndWhile
    \end{algorithmic}
\end{algorithm}

\end{vbframe}

	% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	%  \begin{vbframe}{SGD with and without momentum}
	%  	\footnotesize The following plot was created by our Shiny App. On the upper left you can explore different predefined examples. \href{https://juliambr.shinyapps.io/shinyapp/}{\beamergotobutton{Click here}}
	%  	\begin{figure}
	%  		\vspace{-0.3cm}
	%  		\centering
	%  		\includegraphics[width = 8cm]{figure_man/momentum2.png} \newline
	%  		\footnotesize{Comparison of SGD with and without momentum on the Styblinkski-Tang function.
	%  			The black dot on the bottom left is the global optimum. We can see that 
	%  			SGD without momentum (red line/points) cannot escape the local minimum, while SGD with momentum (blue line/dots) is able to escape the local minimum and finds the global minimum.}
	%  	\end{figure}
	%  \end{vbframe}





% \frame{
%
% \frametitle{Momentum - motivation}
% \only<1>{\textbf{Iteration 1}}
% \only<2>{\textbf{Iteration 2}}
% \only<3>{\textbf{...Iteration 10}}
%   \center
%   \only<1>{\includegraphics[height = 4.5cm, width=9cm]{figure_man/opt1.png}}%
%   \only<2>{\includegraphics[height = 4.5cm, width=9cm]{figure_man/opt2.png}}%
%   \only<3>{\includegraphics[height = 4.5cm, width=9cm]{figure_man/opt3.png}}%
%   \only<4>{\includegraphics[height = 4.5cm, width=9cm]{figure_man/momentum.png}}%
%
%   \medskip
%   \begin{itemize}
%     \only<1-3>{\item (Stochastic) Gradient Descent not optimal in areas with significantly stronger curvature in one direction than in the others (for example saddle point)}
%     \only<1-3>{\item Algorithm terminates in local minimum, global minimum not found}
%     \only<4>{\item Black line shows the course of the SGD to the minimum.}
%     \only<4>{\item SGD is influenced by strong curvature of the optimization function and moves slowly towards the minimum.}
%     % \only<4>{\item Geschwindigkeit des SGD sehr langsam (Algorithmus daher ineffizient).}
%     \only<4>{\item \textbf{Aim}: More efficient algorithm which quickly reaches the minimum (red line)}
%   \end{itemize}
% }



%
%\begin{vbframe}{Adaptive Moment Estimation (Adam)}
%	
%	\textbf{Adam} combines concept of momentum with adaptive learning rates to converge faster
%	
%	\medskip
%	\textbf{Adaptive learning rates:}
%	\begin{itemize}
%		\item Intuition: start with large steps and finish with small ones
%		\item AdaGrad: change learning rate over time depending on sum of gradients
%		\item RMSprop: change learning rate by exponentially decaying average
%		\item Difference: RMSprop more restricted to recent time steps and hence changes slower than AdaGrad
%		
%	\end{itemize}
%	
%	\framebreak
%	Comparison of GD, GD with momentum and Adam.
%	\begin{center}
%		\includegraphics[width = 0.85\textwidth]{figure_man/momentum/comparison_adam.png}
%	\end{center}
%	
%\end{vbframe}

%\begin{vbframe}{Trust region methods}
%	Besides line search methods
%\end{vbframe}
\endlecture
\end{document}

