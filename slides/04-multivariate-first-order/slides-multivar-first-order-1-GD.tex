\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
First order methods
}{ 
Gradient descent
}{
figure_man/linesearch.png
}{
\item Iterative Descent / Line Search
\item Descent directions
\item GD
\item ERM with GD
\item Pseudoresiduals
}

\begin{framei}{Iterative Descent}
\item Let $f$ be the height of a mountain depending on the geographic coordinates $(x_1, x_2)$
\vspace*{-0.1cm}
$$f: \R^2 \to \R, \quad f(x_1, x_2) = y.$$
\item \textbf{Goal}: Reach the valley
$$\argmin_{\xv} f(\xv)$$
\item \textbf{Central idea}: Repeat
\splitVCC[0.6]{
\begin{enumerate}
\item At current location $\xv \in \R^d$ search for \textbf{descent direction} $\mathbf{d} \in \R^d$ 
\item Move along $\mathbf{d}$ until $f$ \glqq sufficiently\grqq{} reduces (\textbf{step size control}) and update location
\end{enumerate}}{
\imageC{figure_man/linesearch.png}
\begin{footnotesize} "Walking down the hill, towards the valley." \end{footnotesize}}
\end{framei}


\begin{framei}{Iterative Descent}
\item Let $f: \R^d \to \R$ continuously differentiable.
\item \textbf{Definition:} $\mathbf{d}\in \R^d$ is a \textbf{descent direction} in $\xv$ if
$$D_\mathbf{d} f(\xv) = \nabla f(\xv) ^T \mathbf{d} < 0 \text{ (neg. directional derivative)}$$
\begin{figure}
\imageC[0.4]{figure_man/descent_direction.pdf}
\begin{footnotesize}
Angle between $\nabla \fx$ and $\mathbf{d}$ must be $\in (90^{\circ}, 270^{\circ})$. 
\end{footnotesize}
\end{figure}
\end{framei}


\begin{framev}{Iterative Descend}
\begin{algorithm}[H]
\caption{Iterative Descent / Line search}
\begin{algorithmic}[1]
\State Starting point $\xv^{[0]} \in \R^d$
\While {Stopping criterion not met}
\State Calculate a descent direction $\mathbf{d}^{[t]}$ for current $\xv^{[t]}$
\State Find $\alpha^{[t]}$ s.t. $f(\xv^{[t + 1]}) < f(\xv^{[t]})$ for $\xv^{[t + 1]} = \xv^{[t]} + \alpha^{[t]} \mathbf{d}^{[t]}$.
\State Update $\xv^{[t + 1]} = \xv^{[t]} + \alpha^{[t]} \mathbf{d}^{[t]}$
\EndWhile
\end{algorithmic}
\end{algorithm}
\vspace*{-0.2cm}
\begin{tiny}
NB: Terminology is sometimes ambiguous: ``line search'' can refer to Step 4 (selecting the step size that decreases $f(\xv)$) and can mean umbrella term for iterative descent algorithms. \par
\end{tiny}\vfill
Key questions: 
\begin{itemizeM}
\item How to choose $\mathbf{d}^{[t]}$ (now)
\item How to choose $\alpha^{[t]}$ (later)
\end{itemizeM}
\end{framev}


\begin{framei}{Gradient Descent}
\item Properties of gradient: 
\begin{itemizeS}
\item $\nabla \fx$: direction of greatest increase
\item $- \nabla \fx$: direction of greatest decrease
\end{itemizeS}
\vfill
\item Using $\mathbf{d} = - \nabla \fx$ is called \textbf{gradient descent}. 
\splitV{\imageC[0.9]{figure/gradient-descent-example_1.png}}{\imageC{figure/gradient-descent-example_2.png}}
\begin{center}\begin{footnotesize}
GD for $f(x_1, x_2) = - \sin(x_1) \cdot \frac{1}{2\pi} \exp\left( (x_2 - \pi / 2)^2 \right)$ with sensibly chosen step size $\alpha^{[t]}$. 
\end{footnotesize}\end{center}
\end{framei}


\begin{framev}{GD and Multimodal functions}
Outcome will depend on start point.
\vfill
\splitVCC[0.63]{
\imageC{figure/gradient-descent-franke_1.png}}
{\imageC{figure/gradient-descent-franke_2.png}}
\begin{center}\begin{footnotesize}
$100$ iters of GD with const $\alpha = 10^{-4}$.
\end{footnotesize}\end{center}
\end{framev}


\begin{framei}[fs=footnotesize]{Optimize LS linear regression with GD}
\item Let $\D = \Dset$ and minimize 
$$\risket = \sumin (\thetav^\top \xi - \yi)^2$$
\textbf{NB: } For illustration, we use GD even though closed-form solution exists. GD-like (more adv.) approaches like this MIGHT make sense for large data, though.
\item\textbf{Gradient: } $$\nabla_{\thetav}\risket(\thetav) = \frac{\partial \risket(\thetav)}{\partial \thetav} = -2 \sumin (\yi - \thetav^{\top} \xv^{(i)}) \xv^{(i)}$$ 
\imageC[0.8]{figure_man/gradient_descent_lm.pdf}
\end{framei}


\begin{framei}{ERM for NN with GD}
\item Let $\D = \Dset$, with $y = x_1^2 + x_2^2$ and minimize 
$$\risket = \sumin \left(\fxt - \yi\right)^2$$
where $\fxt$ is a neural network with 2 hidden layers (2 units each). 
\vspace{-0.7cm}
\imageC[0.6]{figure_man/gradient_descent_NN_0.pdf}
\end{framei}


\begin{framei}{ERM for NN with GD}
\item[] After $10$ iters of GD:
\splitVCC{
\imageC{figure_man/gradient_descent_NN_10_surface.pdf}}
{\imageC{figure_man/gradient_descent_NN_300_history.pdf}}
\end{framei}


\begin{framei}{ERM for NN with GD}
\item[] After $100$ iters of GD: 
\splitVCC{
\imageC{figure_man/gradient_descent_NN_100_surface.pdf}} 
{\imageC{figure_man/gradient_descent_NN_300_history.pdf}}
\end{framei}


\begin{framei}{ERM for NN with GD}
\item[] After $300$ iters of GD (note the zig-zag-behavior after iter. 200):
\splitVCC{
\imageC{figure_man/gradient_descent_NN_300_surface.pdf}}
{\imageC{figure_man/gradient_descent_NN_300_history.pdf}}
\end{framei}


\begin{framei}[fs=footnotesize]{GD for ERM: Pseudoresiduals}
\item Gradient for ERM problem: 
$$
- \pd{\risket}{\thetav} = - \pd{\sumin \Lxyit}{\thetav}= - \sumin \underbrace{\pd{\Lxyi}{f}}_{\text{pseudo residual } \tilde r^{(i)}(f)} \pd{\fxit}{\thetav}
$$
\item \textbf{pseudo residuals} tell us how to distort $\fxi$ to achieve greatest decrease of $\Lxyi$ (best pointwise update)
\item $\pd{\fxit}{\thetav}$ tells us how to modify $\thetav$ accordingly and wiggle model output
\item GD step sums up these modifications across all observations $i$
\splitVCC{
\textbf{NB:} Pseudo-residuals 
$\tilde{r}\left( f \right)$ 
match usual residuals for L2 loss:
\begin{align*}
\pd{\Lxy}{f} & = \frac{1}{2} \left. \pd{(y - f)^2}{f} \right|_{f = \fx}\\ 
& = y - \fx
\end{align*}
}
{\imageC{figure_man/pseudo_residual_1.png}}
\end{framei}

\endlecture
\end{document}
