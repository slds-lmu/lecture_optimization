\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}


\newcommand{\titlefigure}{figure_man/NSGA2_CS2.png}
\newcommand{\learninggoals}{
\item A-Posteriori and EA
\item NSGA-II
\item SMS-EMOA algorithm
}


%\usepackage{animate} % only use if you want the animation for Taylor2D

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Evolutionary multi-objective optimization algorithms (EMOA)}
\lecture{Optimization in Machine Learning}
\sloppy

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\begin{vbframe}{Notation}

%\begin{itemize}
%\item Admissible set $\Xspace \subset \R^n$
%\item Target region $\R^m$
%\item Multi-criteria objective function $f: \Xspace \to \R^m$
%\item Objective function vector $f(\bm{x}) = \left(f_1(\bm{x}), ..., f_m(\bm{x}\right))^\top \in \R^m$, which maps $\bm{x}$ into the space $\R^m$.
% \item $F:=f(\Xspace) = \left\{f(\bm{x}) ~|~ \bm{x} \in \Xspace\right\}$: Bild einer Funktion (Menge aller möglichen Funktionswerte)
%\end{itemize}

%w.l.o.g. we look at minimization problems.

%\end{vbframe}


% \begin{vbframe}{Skalarisierung}
%
% \textbf{Idee:} Führe mehrkriterielle Optimierung auf monokriterielle Optimierungsprobleme zurück.
%
% \lz
%
% Mögliches Verfahren: Optimiere eine gewichtete Summe
%
% $$
% w_1 \cdot f_1(\bm{x}) + w_2 \cdot f_2(\bm{x}) + ... + w_m \cdot f_m(\bm{x}),
% $$
%
% wobei $w_i \ge 0$ und $\sum_{i = 1}^m w_i = 1$ (Konvexkombination).
%
% \framebreak
%
% \textbf{Beispiel:} Es sei weiterhin $f_1(x) = (x - 1)^2$ und $f_2(x) = 3(x - 2)^2$. Wir lösen das skalare Ersatzproblem
%
% $$
% z = w_1 f_1(x) + w_2 f_2(x) = w_1 y_1 + w_2y_2 \to \min!
% $$
%
% Umformen ergibt hier eine Geradengleichung: $y_2 = -\frac{w_1}{w_2}y_1 + z$.
%
% \lz
%
% Grafisch finden wir das Optimum dieses skalaren Optimierungsproblems, indem wir
%
% \begin{itemize}
% \item alle möglichen Geraden $y_2 = -\frac{w_1}{w_2}y_1 + z$ für unterschiedliche Werte von $z$ einzeichnen und
% \item den Punkt finden, in dem sich die Gerade mit dem Bild der Funktion $F = f(\Xspace)$ berührt.
% \end{itemize}
%
% Beispielhaft für $w_2 = \frac{2}{3}$ und $w_1 = \frac{1}{3}$.
%
%<<echo = FALSE, fig.width = 2, fig.height=2, fig.align='center', warning=FALSE, include = FALSE>>=
%x = seq(-1, 4, length.out = 1000)
%lin = 3 * 0.4 - 2 * x
%fun1 = function(x) (x - 1)^2
%fun2 = function(x) 3 * (x - 2)^2
%
%p2 = ggplot() + geom_point(data = data.frame(f1 = fun1(x), f2 = fun2(x)), aes(x = f1, y = f2), size = 0.7)
%p2 = p2 + geom_line(aes(x = x, y = lin)) + ylim(c(-3, 25))
%p2 = p2 + geom_point(aes(x = 0.36, y = 0.48), colour = "green", size = 3)
%p2 = p2 + theme_bw()
%p2
%@

% \end{vbframe}
%
% \begin{vbframe}{Zusammenfassung: Skalarisierungsmethoden}
%
% Es gibt weitere Möglichkeiten, ein mehrkriterielles Problem durch eines oder mehrere skalare Ersatzprobleme approximativ zu löse (z.B. auch lexikografische Methode).
%
% \lz
%
% \textbf{Aber: }
%
% \begin{itemize}
% \item Aber: Es gibt Lösungen, die diesen Methoden verborgen bleiben (insbesondere: bei einmaliger Anwendung nur eine Lösunge)
% \item Auch monokriterielle Optimierung ist nicht immer \enquote{einfach}
% \end{itemize}
%
% \end{vbframe}
%
% \begin{vbframe}{Mehrkriterielles Abstiegsverfahren}
%
% Die Definition einer Abstiegsrichtung lässt sich auf den multikriteriellen Fall erweitern.
%
% \lz
%
% \textbf{Definition: } Abstiegsrichtung \\
% Es sei $f: \R^n \to \R^m$ stetig differenzierbar.  Eine Richtung $\bm{v} \in \R^n$ heißt \textbf{Abstiegsrichtung} in $\bm{x} \in \R^n$, wenn
%
% $$
% \nabla f_j(\bm{x})^T\bm{v} < 0 \quad \forall ~ j = 1, ..., m.
% $$
%
% \framebreak
%
% Problem: Das Finden einer Abstiegsrichtung ist per se schon ein nicht ganz einfach zu lösendes Problem.

%<<include = FALSE>>=
%f1 = function(x) 0.01 * sum(x^2) - 2
%f2 = function(x) 0.01 * sum(c(0.1, 0.3) * (x - c(-10, 20))^2)
%
%x1 = x2 = seq(-10, 20, length.out = 100)
%grid = expand.grid(x1 = x1, x2 = x2)
%grid$y1 = apply(grid[, 1:2], 1, f1)
%grid$y2 = apply(grid[, 1:2], 1, f2)
%
%melt = reshape2::melt(grid, id.vars = c("x1", "x2"))
%
%p = ggplot(data = melt) + geom_raster(aes(x = x1, y = x2, fill = value))
%p = p + geom_contour(aes(x = x1, y = x2, z = value, colour = variable), bins = 15)
%p = p + ylim(c(-20, 40)) + xlim(c(-20, 40)) + theme_bw()
%p
%@



%\section{Evolutionary multi-objective optimization algorithms (EMOA)}


\begin{vbframe}{A-posteriori methods and EA}

Evolutionary multi-objective algorithms (EMOAs) evolve a diverse population over time to approximate the Pareto front.

\begin{columns}
\begin{column}{0.3\textwidth}
\begin{center}
\begin{figure}
\centering
\begin{tikzpicture}[node distance=-1.8cm, auto,]
%nodes
\begin{footnotesize}
\node (init) {Initialize population};
\node[below = 0.1cm of init](rating1) {Eval population};
\node[below = 0.1cm of rating1](selection1) {Parent selection};
\node[below = 0.1cm of selection1](variation) {Variation};
\node[below = 0.1cm of variation](rating2) {Eval offspring};
\node[below = 0.1cm of rating2](selection2) {Survival selection};
\node[below = 0.1cm of selection2](stop) {Stop};
\node[below = 0.2cm of stop](dummy2) {};
\node[below = 0.2cm of stop](dummy3) {};
\node[right = 0.01cm of dummy3](dummy4) {yes};
\node[left = 0.2cm of rating2](dummy1) {no};
\draw[->] (init) to (rating1) node[midway, above]{};
\draw[->] (rating1) to (selection1) node[midway, above]{};
\draw[->] (selection1) to (variation) node[midway, above]{};
\draw[->] (variation) to (rating2) node[midway, above]{};
\draw[->] (rating2) to (selection2) node[midway, above]{};
\draw[->] (selection2) to (stop) node[midway, above]{};
\draw[->] (stop) to (dummy2) node[midway, above]{};
\draw[->] (stop) to [bend left=90, looseness=2](selection1) node[midway, above]{};
\end{footnotesize}
\end{tikzpicture}
\end{figure}
\end{center}
\begin{footnotesize}
\end{footnotesize}
\end{column}

  
\begin{column}{0.7\textwidth}
\vspace{0.6cm}  
\begin{center}
\includegraphics[width = 0.9\linewidth]{figure_man/EA-steps.png}
\end{center}
\end{column}
\end{columns}

\framebreak

%\begin{algorithm}[H]
 % \begin{center}
  %\caption{Basic EA template loop}
   %   \begin{algorithmic}[1]
    %      \STATE Init and eval population $\mathcal{P}_0 \subset \XX$ with $|\mathcal{P}| = \mu$ 
     % \STATE $t \leftarrow 0$
      %\REPEAT
       % \STATE Select parents and generate offspring $\mathcal{Q}_t$ with $|\mathcal{Q}_t| = \lambda$
        %\STATE Select $\mu$ survivors $\mathcal{P}_{t + 1}$ 
 	%	\STATE $t \leftarrow t + 1$
   %   \UNTIL{Stop criterion fulfilled}
    % \end{algorithmic}
    %\end{center}
%\end{algorithm}


\begin{algorithm}[H]
  \begin{center}
  \caption{Basic EA template loop}
    \begin{algorithmic}[1]
    \State Init and eval population $P_0 \subset \Xspace$ with $|P| = \mu$
    \State $t \leftarrow 0$
      \Repeat
        \State Select parents and generate offspring $Q_t$ with $|Q_t| = \lambda$
        \State Select $\mu$ survivors $P^{[t + 1]}$
        %\State Selection: select survivors $P_{t + 1}$
 		\State $t \leftarrow t + 1$
      \Until{Stop criterion fulfilled}
            \vspace*{-0.3cm}
    \end{algorithmic}
    \end{center}
\end{algorithm}

\begin{itemize}
   % \item Note that (as in the EA lecture unit) we are using somewhat non-standard notation here.
    \item Nearly all steps in the above template work also for EMOAs, but both parent and survival selection are now less obvious. How do we rank under multiple objectives?
\end{itemize}

%The population of solution candidates consists of $\bm{x} \in \Xspace$.

\end{vbframe}


%\begin{vbframe}{Objectives of an evolutionary strategy}

%The aim is to select the evolution strategy in such a way that the algorithm provides an approximation of the Pareto front, where

%\begin{enumerate}
%\item The individuals of the population (or the corresponding functional values in the target function space) \textbf{converge} to the Pareto front.
%\item The individuals of the population provide a \textbf{diverse} as possible approximation of the Pareto front.
%\end{enumerate}

%\vspace*{-0.3cm}

%\begin{center}
%\includegraphics[width = 0.25\linewidth]{figure_man/EMO_goals.png}
%\end{center}

%\vspace*{-0.5cm}

%\begin{footnotesize}
%\textbf{Caution}: in this graphic the objective function values are exceptionally \textbf{maximized}.
%\end{footnotesize}

%\end{vbframe}

\begin{vbframe}{NSGA-II}

The \textbf{non-dominated sorting genetic algorithm (NSGA-II)} was published by {\href{https://www.iitk.ac.in/kangal/Deb_NSGA-II.pdf}{K. Dep et al. 2002}}.

\begin{itemize}
\item Follows a $(\mu + \lambda)$ strategy.
\item All previously discussed variation strategies can be used; 
    the original paper uses tournament selection, polynomial mutation and simulated binary crossover.
\item Parent and survival selection rank candidates by 
\begin{enumerate}
\item \textbf{Non-dominated sorting} as main criterion
\item \textbf{Crowding distance assignment} as tie breaker
\end{enumerate}
\end{itemize}

\end{vbframe}

\begin{vbframe}{NSGA-II: non-dominated sorting}

% \begin{center}
% \includegraphics[width = 0.5\linewidth]{figure_man/NSGA2_1.png}
% \end{center}

% \framebreak

\begin{footnotesize}
NDS partitions an objective space set into fronts $\mathcal{F}_1 \prec \mathcal{F}_2 \prec \mathcal{F}_3 \prec ... $.

\begin{itemize}
    \item $\mathcal{F}_1$ is non-dominated, 
      each $\bm{x} \in \mathcal{F}_2$ is dominated, but only by points in $\mathcal{F}_1$, 
      each $\bm{x} \in \mathcal{F}_3$ is dominated, but only by points in $\mathcal{F}_1$ and $\mathcal{F}_2$, 
      and so on. 
    \item We can easily compute the partitioning by computing all non-dominated points  $\mathcal{F}_1$,
        removing them, then computing the next layer of non-dominated points $\mathcal{F}_2$, and so on.
\end{itemize}
\end{footnotesize}

\begin{center}
\includegraphics[width = 0.5\linewidth]{figure_man/NSGA2_NDS.png}
\end{center}

\framebreak

How does survival selection now work? We fill $\mu$ \textit{places} one by one with $\mathcal{F}_1, \mathcal{F}_2, ...$ until a front can no longer \textbf{fully} survive (here: $\mathcal{F}_3$).

\begin{center}
\includegraphics[width = 0.45\linewidth]{figure_man/NSGA2_2.png}
\end{center}

Which individuals survive from $\mathcal{F}_3$? $\to$ \textbf{crowding sort}
\vspace{0.3cm}

\footnotesize{NB: the same principle to rank individuals is applied in tournament selection in parent selection.}
\end{vbframe}



\begin{vbframe}{NSGA-II: crowding distance}
\textbf{Idea:} Add \textit{good} representatives of front $\mathcal{F}_3$, define this as points of $"$low density$"$ in $f$-space.

\includegraphics[height = 0.5\textheight]{figure_man/NSGA2_CS1.png}

\begin{footnotesize}
The points on the left (marked by a triangle) do not represent the front very well because they are very close together. The front is better represented by the points on the right plot.
\end{footnotesize}

\framebreak

\begin{columns}
\begin{column}{0.6\textwidth}
\begin{footnotesize}
For each objective $f_j$
\begin{itemize}
\item Sort points by $f_j$
\item Normalize scores to [0,1]
\item Assign border points (which have score 0 or 1) a CD of $\infty$ (they should always be selected, if possible)
\item Each point gets a distance score, which is the distance between its 2 next-neighbors w.r.t. the sorting of $f_j$
\end{itemize}
For each point, all of its $m$ distance scores are summed up (or averaged) and points are ranked w.r.t. to this overall score.
\end{footnotesize}
\end{column}

\begin{column}{0.4\textwidth}
\begin{center}
\includegraphics[width = 0.9\linewidth]{figure_man/NSGA2_CS2.png}
\vspace{0.2cm}
\footnotesize{Red: Point with high CD.\\ Blue: Low CD.}
\end{center}
\end{column}
\end{columns}

\end{vbframe}

%\begin{vbframe}
%\begin{algorithm}[H]

 % \begin{center}
  %\caption{NSGA-II}
   % \begin{algorithmic}[1]
   	%\begin{footnotesize}
    %\State Initialize population $P_0$, $t \leftarrow 0$
    %\State $F_1, F_2, F_3, ... \leftarrow \texttt{nondominated-sort}(P_0)$
    %\State Generate $Q_0$ by binary tournament selection, recombination and mutation
     % \Repeat
      %  \State $F_1, F_2, F_3, ... \leftarrow \texttt{nondominated-sort}(P_t \cup Q_t)$
       % \State $i \leftarrow 1$
        %\While{$|P_{t + 1} \cup F_i| < \mu$}
        %	\State $P_{t + 1} = P_{t + 1} \cup F_i$
        %	\State $i \leftarrow i + 1$
    	%\EndWhile
       % \State $\tilde F_i = (\bm{x}_1, \bm{x}_2, ..., \bm{x}_k)= \texttt{SortByCrowdingDistance}(F_i)$
        %\While {$P_{t + 1} < \mu$}
        %	\State $P_{t + 1} = P_{t + 1} \cup \bm{x}_j$
        %	\State $j \leftarrow j + 1$
        %\EndWhile
        %\State Generate $Q_{t + 1}$ by binary tournament selection, recombination and mutation
    %  \Until{Stop criterion fulfilled}
     % \vspace*{-0.3cm}
      %\end{footnotesize}
  %  \end{algorithmic}
   % \end{center}
%\end{algorithm}

%\end{vbframe}




%\textbf{Crowding sort} sorts the individuals based on their crowding distance:

%\begin{itemize}
%\item The crowding distance describes the solution density by one point.
%\item It is calculated from the mean distance to the nearest neighbors around a point in the target function space.
%\item The crowding distance is greater when the neighbors are very far away.
%\item The maximum crowding distance is assigned to the boundary points so that they are always selected.
%\end{itemize}

%\begin{center}
%\includegraphics[width = 0.5\linewidth]{figure_man/NSGA2_CS2.png}
%\end{center}

%\begin{footnotesize}
%One point with high crowding distance (red) and one point with very small crowding distance (blue).
%\end{footnotesize}


\begin{vbframe}{Selection criteria: hypervolumen contribiution}
\begin{columns}
\begin{column}{0.6\textwidth}
The SMS-EMOA (S-Metric-Selection-EMOA) evaluates the fitness of an individual $\bm{x} \in \mathcal{P} \subset \Xspace$ based on its contribution to the dominated hypervolume (S-Metric):
$$
\Delta S(\bm{x}, \mathcal{P}) = S(\mathcal{P}, R) - S(\mathcal{P} \setminus \{ \bm{x}\}, R).
$$

\begin{itemize}
\item Dark rectangles: HV contribution of the black dots.
\item Grey point: reference point.
\item The HVC contribution is the volume of space that is dominated only by $\bm{a}$, and nothing else.
\item $\bm{a}^\star$ has lowest S-metric contribution.
\end{itemize}
\end{column}

\begin{column}{0.4\textwidth}
\begin{center}
%Hypervolume contribution in a 2-dimensional objective space:\\
\includegraphics[width = 1\textwidth]{figure_man/hypervolumenbeitrag.png}
\end{center}
\end{column}
\end{columns}

\end{vbframe}


\begin{vbframe}{SMS-EMOA algorithm}

\vspace*{-0.5cm}

\begin{algorithm}[H]
  \begin{center}
  \caption{SMS-EMOA}
    \begin{algorithmic}[1]
    \begin{footnotesize}
    \State Generate start population $P_0$ of size $\mu$
    \State $t \leftarrow 0$
      \Repeat
        \State Generate \textbf{one} individual $\bm{q} \in \R^d$ by recombination and mutation of $P^{[t]}$
        \State $\{\mathcal{F}_{1},..., \mathcal{F}_k\} \leftarrow\text{NDS}(P^{[t]}\cup \{\bm{q}\})$
        \State $\bm{a}^\star \leftarrow \text{argmin}_{\bm{a} \in \mathcal{F}_{k}}\Delta S(\bm{a}, \mathcal{F}_{k})$
        \State $P^{[t+1]} \leftarrow (P^{[t]} \cup \{\bm{q}\}) \setminus\{\bm{a}^\star\}$
        \State $ t \leftarrow t+1$
      \Until{Termination criterion fulfilled}
    \end{footnotesize}
    \vspace*{-0.3cm}
    \end{algorithmic}
    \end{center}
\end{algorithm}

\vspace*{-0.6cm}
\footnotesize
\begin{itemize}
\item L5: the set of temporary $(\mu + 1)$ individuals is partitioned by NDS into $k$ fronts $\mathcal{F}_{1},...,\mathcal{F}_{k}$.
\item L6-7: In last front, find $\bm{a}^\star \in \mathcal{F}_{k}$ with smallest HV contribution - and kill it.
%\item L7: the individual $\bm{a}^\star$ from the worst front with the smallest contribution to the dominated hypervolume does not survive.
\item Fitness of an individual is mainly the rank of its front and HV contribution as tie-breaker.
\end{itemize}
\end{vbframe}

\endlecture
\end{document}


% \begin{vbframe}{SPEA-2}

% Ebenso im Jahr 2002 wurde der \textbf{Strength Pareto EA} (SPEA-2) von Zitzler et al. veröffentlicht.

% \lz

% \begin{itemize}
% \item Neben der aktuellen Population $P_t$ gibt es auch ein sogenanntes Archiv $A_t$, das lediglich zur Bewertung der aktuellen Population dient.

% \begin{figure}
% 	\centering
% 	\includegraphics[width=0.6\linewidth]{figure_man/SPEA-archive}
% \end{figure}

% \framebreak

% \item Die Bewertung (und damit auch die Selektion) eines Individuums erfolgt anhand von

% $$
% \text{fitness}(x) = \text{raw}(x) + \text{density}(x).
% $$

% Hierbei ist

% \begin{itemize}
% \item $\text{raw}(x)$ die \enquote{Grundfitness} (bzgl. Population und Archiv)
% \vspace*{-0.2cm}
% $$
% \text{raw}(x) = |\{y \in P_t: f(x) \prec f(y)\}| + |\{y \in A_t: f(x) \prec f(y)\}|,
% $$

% \item $\text{density}(x)$ die Dichte des Punktes
% $$
% \text{density}(x) = \frac{1}{\sigma^{(k)}(x) + 2},
% $$
% ($\sigma^{(k)}$ bezeichne den Abstand zum $k$-nächsten Nachbarn).
% \end{itemize}
% \end{itemize}

% \framebreak

% \begin{algorithm}[H]
% \begin{footnotesize}
%   \begin{center}
%   \caption{SPEA-2}
%     \begin{algorithmic}[1]
%     \State Initialisiere Population $P_0$, $|P_0| = \lambda$ und ein leeres Archiv $A_0, |A_0| = \gamma$
%       \Repeat
%         \State Berechne Fitness der Individuen in $P_t$ und $A_t$ anhand der oben definierten Fitnessfunktion
%         \State Fülle $A_{t+1}$ auf mit nichtdominierten Individuen aus $P_t \cup A_t$ $^{(*)}$
%         \State Fülle \enquote{mating pool} durch binäre Turnierselektion mit Zurücklegen auf $A_{t + 1}$
%         \State Generiere $P_{t + 1}$ durch Rekombination und Mutation
%       \Until{Stoppkriterium erfüllt}
%       \State Gib $A_t$ zurück
%     \end{algorithmic}
%     \end{center}
% \end{footnotesize}
% \end{algorithm}

% \vfill
% \begin{footnotesize}
% $^{(*)}$ Wenn $|A_{t + 1}| > \gamma$, dann entferne solange Individuen mit kleinster Distanz zum Nachbarn, bis $|A_{t + 1}| = \gamma$. Sollte $|A_{t + 1}| < \gamma$, füge die besten dominierten Individuen aus $P_t \cup A_t$ hinzu.
% \end{footnotesize}

% \end{vbframe}



% \textbf{Berechnung des Hypervolumens im 2 dimensionalen Fall:}
% \begin{enumerate}
% \item Sortiere die Zielfunktionsvektoren bzgl. eines Kriteriums (z.B. aufsteigend bzgl. $f_1$)\\
% $\Rightarrow$ Da Pareto-Front (kein Punkt dominiert anderen): Punkte sind bzgl. $f_2$ absteigend sortiert .
% \item Für das $j$-te Individuum $a^{(j)}, j\in \{2,..., |F_{\nu}|\}$ in der sortierten Sequenz der Front $F_{\nu}$ berechnet sich der Hypervolumensbeitrag als:
% \medskip

% $$
% \Delta s(\bm{y}^{(j)}, F_{\nu}) = (y_{1}^{(j+1)} - y_{1}^{(j)}) (y_{2}^{(j-1)} - y_{2}^{(j)})
% $$
% \end{enumerate}




% \item Links: Punkte entsprechen Werten der Individuen in 2-dimensionalem Zielraum.
% \item Links: Punkte ohne Füllung zeigen dominierte Lösungen. Gelbe Fläche zeigt Bereich in dem dominierende Lösungen liegen.


% \begin{vbframe}{SMS-EMOA}
% \textbf{Motivation:}
% \begin{itemize}
% \item Pareto-Front bildet Menge von optimalen Parameterkonbinationen ab.
% \item Oft ist die Menge dieser Kombinationen noch sehr groß.
% \item In Praxis ist es meist nicht möglich alle Pareto-Effizienten Lösungen zu prüfen
% \end{itemize}
% $\Rightarrow$ SMS-EMOA soll möglichst guten Kompromiss zwischen Aufwand der Überprüfung der Paretoeffizienten Lösungen bei gleichzeitig umfassender Abdeckung möglicher Kompromisslösungen darstellen.
% \medskip

% $\Rightarrow$ SMS-EMOA ist einfach handhabbar und verzichtet auf Erstellung eines Archivs um den Aufwand zu reduzieren.
% \medskip

% $\Rightarrow$ Optimierung wird allein auf Grundlage der Population durchgeführt.

% \framebreak
% \end{vbframe}




%
% \item Kombiniert Verfahren von EMOA (NSGA-II als Bewertungskriterium von Lösungen) und Archivierungverfahren (welche Lösungen werden beibehalten?) in einem Algorithmus
% \item NSGA-II wird als Bewertungskriterium für Lösungen herangezogen.
% \item
% \item 


% \end{vbframe}




