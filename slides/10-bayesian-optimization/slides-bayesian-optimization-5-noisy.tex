\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-mbo}

\newcommand{\titlefigure}{figure_man/noisy_2.png}
\newcommand{\learninggoals}{
\item Noisy surrogate modeling
\item Noisy acquisition functions
\item Final best point
}

\title{Optimization in Machine Learning}
%\author{Bernd Bischl}
\date{}

\begin{document}

\lecturechapter{Bayesian Optimization:\\ Noisy Bayesian Optimization}
\lecture{Optimization in Machine Learning}


\begin{frame}{Noisy Evaluations}

In many real-life applications, we cannot access the true function values $f(\xv)$ but only a \textbf{noisy} version thereof
$$f(\xv) + \epsilon(\xv)$$
For the sake of simplicity, we assume $\epsilon(\xv) \sim \mathcal{N}\left(0, \sigma_{\epsilon}^{2}\right)$ for now\\
\vfill
\only<1>{
\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/noisy_0.png}
\end{center}
}
\only<2>{
Examples:
\begin{itemize}
    \item HPO (due to non-deterministic learning algorithm and/or resampling technique)
    \item Oil drilling optimization (an oil sample is only an estimate)
    \item Robot gait optimization (velocity of a run of a robot is an estimate of true velocity)
\end{itemize}
}

\end{frame}

\begin{frame}{Noisy Evaluations} 

This raises the following problems: 

\begin{itemize}
  \item \textbf{Surrogate modeling:} So far we used an interpolating GP that is based on noise-free observations; as a consequence, the variance is modeled as $0$
  $$
    s^2(\xvsi) = 0
  $$
  for design points $(\xvsi, \ysi) \in \Dt$. This is problematic. 
  \item \textbf{Acquisition functions:} Most acquisition functions are based on the best observed value $\fmin$ so far. If evaluations are noisy, we do not know this value (it is a random variable).
  \item \textbf{Final best point:} The design point evaluated best is not necessarily the true best point in design (overestimation). 
\end{itemize}


\end{frame}

\begin{vbframe}{Surrogate Model}

\begin{columns}[T]
\begin{column}{0.5\textwidth}
In case of noisy evaluations, a nugget-effect GP (GP regression) should be used instead of an interpolating GP.\\
\vspace{1em}
The posterior predictive distribution for a new test point $\xv \in \mathcal{S}$ under a GP assuming homoscedastic noise ($\sigma_{\epsilon}^{2}$) is:
\end{column}
\begin{column}{0.5\textwidth}
\includegraphics[width = \textwidth]{figure_man/noisy_2.png}
\end{column}
\end{columns}

\vfill
\begin{eqnarray*}
  Y(\xv) ~|~ \xv, \Dt \sim \mathcal{N}\left(\fh(\xv), \sh^2(\xv)\right)
\end{eqnarray*}
with 
\begin{eqnarray*}
  \fh(\xv) &=& k(\xv)^\top (\bm{K} + \sigma_{\epsilon}^{2}\mathbf{I}_{t})^{-1} \bm{y} \\
  \sh^2(\xv) &=& k(\xv, \xv) - k(\xv)^\top (\bm{K} + \sigma_{\epsilon}^{2}\mathbf{I}_{t})^{-1} k(\xv)
\end{eqnarray*}

\end{vbframe}

\begin{vbframe}{Noisy Acquisition Functions: AEI}


\textbf{Augmented Expected Improvement} (\emph{Huang et al., 2006})
$$
  a_{\text{AEI}}(\xv) = a_{\text{EI}_{f_{\min_*}}}(\xv) \Bigg(1- \frac{\sigma_{\epsilon}}{\sqrt{\sh^2(\xv)+ \sigma_{\epsilon}^{2}}}\Bigg).
$$

Here, $a_{\text{EI}_{f_{\min_*}}}$ denotes the \textbf{Expected Improvement with Plugin}.\\
It uses the \textbf{effective best solution} as a plugin for the (unknown) best observed value $\fmin$
$$
  f_{\min_*} = \min_{\xv \in \{\xvsi[1], ..., \xvsi[t]\}} \fh(\xv) + c \sh(\xv),
$$

where $c > 0$ is a constant that controls the risk aversion.\\
\vspace{1em}
$\sigma_{\epsilon}^{2}$ is the nugget-effect as estimated by the GP regression.


\framebreak
\begin{columns}[T]
\begin{column}{0.5\textwidth}
In addition, it takes into account the nugget-effect $\sigma_{\epsilon}^{2}$ by a penalty term:
$$
  \Bigg(1- \frac{\sigma_\epsilon}{\sqrt{\sh^2(\xv)+ \sigma_\epsilon^2}}\Bigg)
$$
The penalty is justified to \enquote{account for the diminishing return of additional replicates as the predictions become more accurate} (\emph{Huang et al., 2006})
\end{column}
\begin{column}{0.5\textwidth}
  \includegraphics[width = \textwidth]{figure_man/noisy_3.png}
\end{column}
\end{columns}
\vfill
\begin{itemize}
  \item Designs with small predictive variance $\sh^2(\xv)$ are penalized in favor of more exploration.
  \item If $\sigma_\epsilon^2 = 0$ (noise-free), the AEI corresponds to the EI with plugin. 
\end{itemize}

\end{vbframe}

\begin{frame}{Reinterpolation}

\only<1> {
Clean noise from the model and then apply a general acquisition function (EI, PI, LCB, ...)\\
\vspace{1em}
The RP suggests to build \textbf{two models}: a nugget-effect GP (regression model; left) and then, on the predictions from the first model (grey), an interpolating GP (right)
}
\only<2> {
\begin{algorithm}[H]
\footnotesize
  \caption{Reinterpolation Procedure}
  \begin{algorithmic}[1]
  \State{Build a nugget-effect GP model based on noisy evaluations}
  \State{Compute predictions for all points in the design $\fh(\xvsi[1]), \ldots, \fh(\xvsi[t])$}
  \State{Train an interpolating GP on $\left\{\left(\xvsi[1], \fh(\xvsi[1])\right), \ldots, \left(\xvsi[t], \fh(\xvsi[t])\right)\right\}$}
  \State{Based on the interpolating model, obtain a new candidate using a noise-free acquisition function}
  \end{algorithmic}
\end{algorithm}
}
\vfill
\begin{minipage}[b]{0.45\textwidth}
  \includegraphics[width = \textwidth]{figure_man/noisy_2.png}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
  \includegraphics[width = \textwidth]{figure_man/noisy_4.png}
\end{minipage}

\end{frame}

\begin{vbframe}{Identification of final best point}

Another problem is the identification of a final best point: 

\begin{itemize}
  \item Assume that all evaluations are noisy
  \item The probability is high that \textbf{by chance}
  \begin{itemize}
    \item bad points get overrated 
    \item good points get overlooked
  \end{itemize}
\end{itemize}


\begin{center}
  \includegraphics[width = 0.5\textwidth]{figure_man/noisy_5.png}
\end{center}

\framebreak 

Possibilities to reduce the risk of falsely returning a bad point: 
\begin{itemize}
  \item Return the best predicted point: $\argmin_{\xv \in \{\xvsi[1], \ldots, \xvsi[t]\}} \fh(\xv)$
  \item Repeated evaluations of the final point: infer guarantees about final point (however if final point is \enquote{bad} unclear how to find a better one)
  \item Repeated evaluations of all design points: reduce noise during optimization and risk of falsely returning a bad point
  \item More advanced replication strategies, e.g. incumbent strategies: also re-evaluate the \enquote{incumbent} in each iteration
\end{itemize}

\end{vbframe}


\endlecture
\end{document}
