\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-mbo}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Bayesian Optimization
  }{% Lecture title  
  Practical Aspects of BO
  }{% Relative path to title page image: Can be empty but must not start with slides/
  }{
    \item Size of the initial design
    \item Optimizing the acquisition function
    \item When to terminate
    \item BO Components and robustness
}


\begin{framei}{Size of the Initial Design}
  \item Should not be too small
  \item Should not be too large
  \item Scale with the dimensionality of the search space
  \item Certain SM may impose restrictions on the lower bound of the size of the initial design
  \item Rule of thumb $4d$
\end{framei}

\begin{framev}{Optimizing the Acquisition Function}
  \begin{itemize}
    \item Optimizing the acquisition function to find the next candidate point is comparably cheap
    \item Still can be a hard optimization problem: non-linear, multimodal
    \item Properly optimizing the acquisition function can be crucial for performance
  \end{itemize}
  \vfill
  \imageC[0.5]{figure_man/practical.png}
  \spacer
  {\footnotesize Example EI landscape for a 2D problem.}
\end{framev}

\begin{framei}{Optimizing the Acquisition Function}
  \item Choice of optimizer depends on the search space
  \begin{itemize}
    \item Numeric: L-BFGS-B with restarts, DIRECT, ...
    \item Mixed: EAs, local search with restarts, ...
    \item A random search can always be used but may not find a good solution (even if the budget is large)
  \end{itemize}
  \item Sometimes, gradient-based optimizers can be used (e.g. when using a GP as SM)
\end{framei}

\begin{framei}{When to terminate}
  \item After a certain number of evaluations
  \begin{itemize}
    \item Potentially scaling the budget with the dimensionality of the search space
  \end{itemize}
  \item After a certain runtime
  \item Specify a target threshold (of the objective or acquisition function)
  \item Based on stagnation (of the objective or acquisition function)
\end{framei}

\begin{framev}{Robustness}
  A good BO implementation should be \textbf{robust}:
  \spacer
  \begin{itemize}
    \item Handle crashing SM
    \begin{itemize}
      \item Especially a GP can result in errors during training (Kernel matrix not invertible, training points too close to each other)
      \item Use a fallback SM (e.g., Random Forest) or catch errors and propose a new candidate uniformly at random
    \end{itemize}
    \item Automatically detect input types (numerical, categorical; hierarchical) and choose an appropriate SM
    \item Have sensible defaults that work well for most scenarios
  \end{itemize}
\end{framev}

\begin{framei}{Choosing the right components}
  \item BO is modular: SM, acquisition function, acquisition function optimizer
  \item Different choices induce different overhead, e.g., GP vs. RF, entropy based acquisition functions vs. cheap to compute ones like EI, thorough acquisition function optimization with a large budget can be expensive
  \item Choosing the right components usually depends on the concrete application at hand and how expensive the evaluation of the black box itself is
\end{framei}

\endlecture
\end{document}
