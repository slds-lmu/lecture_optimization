\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-mbo}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
Bayesian Optimization
}{
Posterior Uncertainty and Acquisition Functions II
}{
figure_man/bayesian_loop_sm_normal_fmin.png
}{
\item Probability of improvement
\item Expected improvement
}

\begin{framev}{Probability of Improvement}
\textbf{Goal}: Find $\xvsi[t+1]$ that maximizes the \textbf{Probability of Improvement} (PI):
$$
a_{\text{PI}}(\xv) = \P(Y(\xv) < \fmin) = \Phi\left(\frac{\fmin - \fh(\xv)}{\sh(\xv)}\right)
$$
where $\Phi(\cdot)$ is the standard normal cdf (assuming Gaussian posterior)
\vfill
\splitV[0.48]{
\imageC[1]{figure_man/bayesian_loop_sm_normal_fmin.png}
}{
\imageC[1]{figure_man/bayesian_loop_pi_0.png}
}
\spacer
\footnotesize
\textbf{Left:} The green vertical line represents $\fmin$.
\textbf{Right:} $a_{\text{PI}}(\xv)$ is given by the black area.
\end{framev}


\begin{framev}{Probability of Improvement}
\textbf{Goal}: Find $\xvsi[t+1]$ that maximizes the \textbf{Probability of Improvement} (PI):
$$
a_{\text{PI}}(\xv) = \P(Y(\xv) < \fmin) = \Phi\left(\frac{\fmin - \fh(\xv)}{\sh(\xv)}\right)
$$
where $\Phi(\cdot)$ is the standard normal cdf (assuming Gaussian posterior)
\vfill
\textbf{Note:} $a_\text{PI}(\xv)=0$ for design points $\xv$, since
\begin{itemize}
\item $\sh(\xv) = 0$,
\item $\fh(\xv) = f(\xv) \ge \fmin \quad \Leftrightarrow \quad \fmin - \fh(\xv) \le 0$.
\end{itemize}
Therefore:
$$
\Phi\left(\frac{\fmin - \fh(\xv)}{\sh(\xv)}\right) = \Phi\left(- \infty\right) = 0
$$
\end{framev}


\begin{framev}{Probability of Improvement}
The PI does not take the size of the improvement into account
Often it will propose points close to the current $\fmin$\\
\spacer
We use the PI (red line) to propose the next point ...
\vfill
\imageC[0.6]{figure_man/bayesian_loop_pi_1.png}
The red point depicts $\argmax_{\xv \in \mathcal{S}} a_{\text{PI}}(\xv)$
\end{framev}


\begin{framev}{Probability of Improvement}
... evaluate that point, refit the SM and propose the next point
\vfill
\imageC[0.7]{figure_man/bayesian_loop_pi_2.png}
(grey point = prev point from last iter)
\end{framev}


\begin{framev}{Probability of Improvement}
...
\vfill
\imageC[0.7]{figure_man/bayesian_loop_pi_3.png}
\end{framev}


\begin{framev}{Probability of Improvement}
In our example, using the PI results in spending plenty of time optimizing the local optimum ...
\vfill
\imageC[0.7]{figure_man/bayesian_loop_pi_4.png}
\end{framev}


\begin{framev}{Probability of Improvement}
...
\vfill
\imageC[0.7]{figure_man/bayesian_loop_pi_5.png}
\end{framev}


\begin{framev}{Probability of Improvement}
...
\vfill
\imageC[0.7]{figure_man/bayesian_loop_pi_6.png}
\end{framev}


\begin{framev}{Probability of Improvement}
...
\vfill
\imageC[0.7]{figure_man/bayesian_loop_pi_7.png}
\end{framev}


\begin{framev}{Probability of Improvement}
... eventually, we explore other regions ...
\vfill
\imageC[0.7]{figure_man/bayesian_loop_pi_8.png}
\end{framev}


\begin{framev}{Probability of Improvement}
...
\vfill
\imageC[0.7]{figure_man/bayesian_loop_pi_9.png}
\end{framev}


\begin{framev}{Expected Improvement}
\textbf{Goal:} Propose $\xvsi[t+1]$ that maximizes the \textbf{Expected Improvement} (EI):
$$
a_{\text{EI}}(\xv) = \E(\max\{\fmin - Y(\xv), 0\})
$$
\vfill
\splitV[0.48]{
\imageC[1]{figure_man/bayesian_loop_sm_normal_fmin.png}
}{
\imageC[1]{figure_man/bayesian_loop_pi_0.png}
}
\vfill
\begin{small}\begin{itemize}
\item We now take the expectation in the tail,
instead of the prob as in PI.
\item Improvement is always assumed $\geq 0$.
\end{itemize}\end{small}
\end{framev}


\begin{framev}{Expected Improvement}
\textbf{Goal:} Propose $\xvsi[t+1]$ that maximizes the \textbf{Expected Improvement} (EI):
$$
a_{\text{EI}}(\xv) = \E(\max\{\fmin - Y(\xv), 0\})
$$
\begin{small}
If $Y(\xv) \sim \normal\left(\fh(\xv), \sh^2(\xv)\right)$, we can express the EI in closed-form as:
$$
a_{\text{EI}}(\xv) = (\fmin-\fh(\xv)) \Phi \Big(\frac{\fmin - \fh(\xv)}{\sh(\xv)}\Big) + \sh(\xv) \phi\Big(\frac{\fmin-\fh(\xv)}{\sh(\xv)}\Big),
$$
\begin{itemize}
\item $a_\text{EI}(\xv) = 0$ at design points $\xv$:
$$
a_{\text{EI}}(\xv) = (\fmin-\fh(\xv)) \underbrace{\Phi \Big(\frac{\fmin - \fh(\xv)}{\sh(\xv)}\Big)}_{= 0, \text{ see PI }} + \underbrace{\sh(\xv)}_{ = 0} \phi\Big(\frac{\fmin-\fh(\xv)}{\sh(\xv)}\Big)
$$
\end{itemize}
\end{small}
\end{framev}


\begin{framev}{Expected Improvement}
We use the EI (red line) to propose the next point ...
\vfill
\imageC[0.7]{figure_man/bayesian_loop_1.png}
\vfill
The red point depicts $\argmax_{\xv \in \mathcal{S}} a_{\text{EI}}(\xv)$
\end{framev}


\begin{framev}{Expected Improvement}
... evaluate that point, refit the SM and propose the next point
\vfill
\imageC[0.7]{figure_man/bayesian_loop_2.png}
\vfill
(grey point = prev point from last iter)
\end{framev}


\begin{framev}{Expected Improvement}
...
\vfill
\imageC[0.7]{figure_man/bayesian_loop_3.png}
\end{framev}


\begin{framev}{Expected Improvement}
...
\vfill
\imageC[0.7]{figure_man/bayesian_loop_4.png}
\end{framev}


\begin{framev}{Expected Improvement}
...
\vfill
\imageC[0.7]{figure_man/bayesian_loop_5.png}
\end{framev}


\begin{framev}{Expected Improvement}
The EI is capable of exploration and quickly proposes promising points in areas we have not visited yet
\imageC[0.7]{figure_man/bayesian_loop_6.png}
Here, also a result of well-calibrated uncertainty $\sh(\xv)$ of our GP.
\end{framev}


\begin{framei}{Discussion}
\item Under some mild conditions: BO with a GP as SM and EI is a \textbf{global optimizer}, i.e., convergence to the \textbf{global} (!) optimum is guaranteed given unlimited budget \furtherreading{BULL2011EGO}
\item Cannot be proven for the PI or the vanilla LCB
\item LCB can be proven to converge in a similar manner if the mean-variance trade-off parameter is chosen adaptively and ``correctly'' \furtherreading{SRINIVAS2010GPUCB}
\item In practice, both LCB and EI work quite well
\vfill
Other ACQFs:
\begin{itemize}
\item Entropy based: Entropy search, predictive entropy search, max value entropy search
\item Knowledge Gradient
\item Thompson Sampling
\item ...
\end{itemize}
\end{framei}

\endlecture
\end{document}
