\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-mbo}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{% Chunk title (example: CART, Forests, Boosting, ...), can be empty
  Bayesian Optimization
  }{% Lecture title  
  Posterior Uncertainty and Acquisition Functions II
  }{% Relative path to title page image: Can be empty but must not start with slides/
  figure_man/bayesian_loop_sm_normal_fmin.png
  }{
    \item Probability of improvement
    \item Expected improvement
}

\begin{frame}{Probability of Improvement}

\textbf{Goal}: Find $\xvsi[t+1]$ that maximizes the \textbf{Probability of Improvement} (PI):
$$
  a_{\text{PI}}(\xv) = \P(Y(\xv) < \fmin) = \Phi\left(\frac{\fmin - \fh(\xv)}{\sh(\xv)}\right)
$$
where $\Phi(\cdot)$ is the standard normal cdf (assuming Gaussian posterior) 

%PI illustration for $\xv = 0$ given a SM yielding a normally distributed posterior predictive distribution\\

%\vspace{0.45em}
\only<1> {
\begin{minipage}[b]{0.45\textwidth}
  \includegraphics[width = \textwidth]{figure_man/bayesian_loop_sm_normal_fmin.png}
\end{minipage}
\hfill
\begin{minipage}[b]{0.45\textwidth}
  \includegraphics[width = \textwidth]{figure_man/bayesian_loop_pi_0.png}
\end{minipage}

\begin{footnotesize}
    \textbf{Left:} The green vertical line represents $\fmin$.
    \textbf{Right:} $a_{\text{PI}}(\xv)$ is given by the black area.
\end{footnotesize}
}
%\vspace{1em}

%PI is more complex than LCB and typically makes assumptions regarding the posterior predictive distribution under a SM

\only<2> {
\vspace{1em}
%\begin{footnotesize}
\textbf{Note:} $a_\text{PI}(\xv)=0$ for design points $\xv$, since
\begin{itemize}
    \item $\sh(\xv) = 0$,
    \item $\fh(\xv) = f(\xv) \ge \fmin \quad \Leftrightarrow \quad \fmin - \fh(\xv) \le 0$.
\end{itemize}
Therefore:
$$
  \Phi\left(\frac{\fmin - \fh(\xv)}{\sh(\xv)}\right) = \Phi\left(- \infty\right) = 0
$$

%\end{footnotesize}
}
%\framebreak

\end{frame}

\begin{vbframe}{Probability of Improvement}

The PI does not take the size of the improvement into account\\
Often it will propose points close to the current $\fmin$\\
\vspace{1em}
We use the PI (red line) to propose the next point ...
\vspace{-1em}
\begin{center}
  \includegraphics[width = 0.6\textwidth]{figure_man/bayesian_loop_pi_1.png}
\end{center}

The red point depicts $\argmax_{\xv \in \mathcal{S}} a_{\text{PI}}(\xv)$

\framebreak

... evaluate that point, refit the SM and propose the next point

\begin{center}
  \includegraphics[width = 0.6\textwidth]{figure_man/bayesian_loop_pi_2.png}
\end{center}

(grey point = prev point from last iter)

\end{vbframe}

\begin{frame}{Probability of Improvement}

\only<1>{
...

\begin{center}
  \includegraphics[width = 0.6\textwidth]{figure_man/bayesian_loop_pi_3.png}
\end{center}
}

\only<2>{
In our example, using the PI results in spending plenty of time optimizing the local optimum ...

\begin{center}
  \includegraphics[width = 0.6\textwidth]{figure_man/bayesian_loop_pi_4.png}
\end{center}
}

\only<3>{
...

\begin{center}
  \includegraphics[width = 0.6\textwidth]{figure_man/bayesian_loop_pi_5.png}
\end{center}
}

\only<4>{
...

\begin{center}
  \includegraphics[width = 0.6\textwidth]{figure_man/bayesian_loop_pi_6.png}
\end{center}
}

\only<5>{
...

\begin{center}
  \includegraphics[width = 0.6\textwidth]{figure_man/bayesian_loop_pi_7.png}
\end{center}
}

\only<6>{
... eventually, we explore other regions ...

\begin{center}
  \includegraphics[width = 0.6\textwidth]{figure_man/bayesian_loop_pi_8.png}
\end{center}
}

\only<7>{
...

\begin{center}
  \includegraphics[width = 0.6\textwidth]{figure_man/bayesian_loop_pi_9.png}
\end{center}
}

\end{frame}

\begin{frame}{Expected Improvement}

\textbf{Goal:} Propose $\xvsi[t+1]$ that maximizes the \textbf{Expected Improvement} (EI): 

\vspace*{-0.5cm}

\begin{eqnarray*}
  % FIXME: Expected value with respect to \fh i.e. the GP predictive posterior distribution
  a_{\text{EI}}(\xv) &=& \E(\max\{\fmin - Y(\xv), 0\}) \\
\end{eqnarray*} 

\vspace*{-0.5cm}

\begin{minipage}[b]{0.40\textwidth}
  \includegraphics[width = \textwidth]{figure_man/bayesian_loop_sm_normal_fmin.png}
\end{minipage}
\hfill
\begin{minipage}[b]{0.40\textwidth}
  \includegraphics[width = \textwidth]{figure_man/bayesian_loop_pi_0.png}
\end{minipage}

\vspace*{-0.1cm}

\footnotesize{
\only<1>{
\begin{itemize}
\item We now take the expectation in the tail, 
instead of the prob as in PI.

\item Improvement is always assumed $\geq 0$. % So in a certain sense, we only use uncertainty optimistically. This enforces exploration.
\end{itemize}
}
%EI is more complex than PI and typically also makes assumptions regarding the posterior predictive distribution under a SM\\


\only<2>{

If $Y(\xv) \sim \mathcal{N}\left(\fh(\xv), \sh^2(\xv)\right)$, we can express the EI in closed-form as: 
$$
a_{\text{EI}}(\xv) = (\fmin-\fh(\xv)) \Phi \Big(\frac{\fmin - \fh(\xv)}{\sh(\xv)}\Big) + \sh(\xv) \phi\Big(\frac{\fmin-\fh(\xv)}{\sh(\xv)}\Big), 
$$

%(where $\Phi(\cdot)$ and $\phi(\cdot)$ are standard normal cdf and df)
\begin{itemize}
\item $a_\text{EI}(\xv) = 0$ at design points $\xv$:
%\begin{tiny}

\vspace{-0.8\baselineskip}

$$
    a_{\text{EI}}(\xv) = (\fmin-\fh(\xv)) \underbrace{ \Phi \Big(\frac{\fmin - \fh(\xv)}{\sh(\xv)}\Big)}_{= 0, \text{ see PI }} + \underbrace{\sh(\xv)}_{ = 0} \phi\Big(\frac{\fmin-\fh(\xv)}{\sh(\xv)}\Big)
$$
%\end{tiny}
\end{itemize}
}
}

\end{frame}

\begin{vbframe}{Expected Improvement}

We use the EI (red line) to propose the next point ...

\begin{center}
  \includegraphics[width = 0.6\textwidth]{figure_man/bayesian_loop_1.png}
\end{center}

The red point depicts $\argmax_{\xv \in \mathcal{S}} a_{\text{EI}}(\xv)$

\framebreak

... evaluate that point, refit the SM and propose the next point

\begin{center}
  \includegraphics[width = 0.6\textwidth]{figure_man/bayesian_loop_2.png}
\end{center}

(grey point = prev point from last iter)

\end{vbframe}

\begin{frame}{Expected Improvement}

\only<1>{
...

\begin{center}
  \includegraphics[width = 0.6\textwidth]{figure_man/bayesian_loop_3.png}
\end{center}
}

\only<2>{
...

\begin{center}
  \includegraphics[width = 0.6\textwidth]{figure_man/bayesian_loop_4.png}
\end{center}
}

\only<3>{
...

\begin{center}
  \includegraphics[width = 0.6\textwidth]{figure_man/bayesian_loop_5.png}
\end{center}
}

\end{frame}

\begin{frame}{Expected Improvement}

The EI is capable of exploration and quickly proposes promising points in areas we have not visited yet

\begin{center}
  \includegraphics[width = 0.6\textwidth]{figure_man/bayesian_loop_6.png}
\end{center}

Here, also a result of well-calibrated uncertainty $\sh(\xv)$ of our GP.

\end{frame}

\begin{frame}{Discussion}

\begin{itemize}
  \item Under some mild conditions: BO with a GP as SM and EI is a \textbf{global optimizer}, i.e., convergence to the \textbf{global} (!) optimum is guaranteed given unlimited budget (\href{https://dl.acm.org/doi/pdf/10.5555/1953048.2078198}{Bull 2011})
  \item Cannot be proven for the PI or the vanilla LCB
  \item LCB can be proven to converge in a similar manner if the mean-variance trade-off parameter is chosen adaptively and ``correctly'' (\href{https://icml.cc/Conferences/2010/papers/422.pdf}{Srinivas et al. 2010})
  \item In practice, both LCB and EI work quite well
\end{itemize}

\vspace*{0.3cm}

Other ACQFs:
\begin{itemize}
  \item Entropy based: Entropy search, predictive entropy search, max value entropy search
  \item Knowledge Gradient
  \item Thompson Sampling
  \item ...
\end{itemize}

\end{frame}

\endlecture

\end{document}
