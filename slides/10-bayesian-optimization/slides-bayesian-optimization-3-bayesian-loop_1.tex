\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/ml-mbo}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
Bayesian Optimization
}{
Posterior Uncertainty and Acquisition Functions I
}{
figure_man/bayesian_loop_sm_normal.png
}{
\item Bayesian surrogate modeling
\item Acquisition functions
\item Lower confidence bound
}

\begin{framev}{Bayesian Surrogate Modeling}
\textbf{Goal:}
Find trade-off between \textbf{exploration} (areas we have not visited yet) and \textbf{exploitation} (search around good design points)
\vfill
\imageC[0.7]{figure_man/bayesian_loop_ee.png}
\end{framev}


\begin{framei}{Bayesian Surrogate Modeling}
\item \textbf{Idea}: Use a \textbf{Bayesian approach} to build SM that yields estimates for the posterior mean $\fh(\xv)$ and the posterior variance $\sh^2(\xv)$
\item $\sh^2(\xv)$ expresses \enquote{confidence}/\enquote{certainty} in prediction
\vfill
\splitV[0.5]{
\imageC[1]{figure_man/bayesian_loop_ee.png}
}{
\imageC[1]{figure_man/bayesian_loop_sm.png}
}
\end{framei}


\begin{framev}{Bayesian Surrogate Modeling}
\begin{itemize}
\item Denote by $Y~|~\xv, \Dt$ the (conditional) RV associated with the posterior predictive distribution of a new point $\xv$ under a SM;\\
will abbreviate it as $Y(\xv)$
\item Most prominent choice for a SM is a \textbf{Gaussian process}, here $Y(\xv) \sim \normal\left(\fh(\xv), \sh^2(\xv)\right)$
\end{itemize}
\vfill
\imageC[0.5]{figure_man/bayesian_loop_sm_normal.png}
\begin{footnotesize}
For now we assume an interpolating SM; $\fh(\xv) = f(\xv)$ and $\sh(\xv) = 0$ for training points
\end{footnotesize}
\end{framev}


\begin{framev}{Acquisition Functions}
To sequentially propose new points based on the SM, we make use of so-called acquisition functions $a: \mathcal{S} \to \R$
\vfill
Let $\fmin \coloneqq \min \left\{f(\xvsi[1]), \ldots, f(\xvsi[t])\right\}$ denote the best observed value so far (visualized in green - we will need this later!)
\imageC[0.5]{figure_man/bayesian_loop_sm_fmin.png}
In the examples before we simply used the posterior mean $a(\xv) = \fh(\xv)$ as acquisition function - ignoring uncertainty
\end{framev}


\begin{framev}{Lower Confidence Bound}
\textbf{Goal}: Find $\xvsi[t+1]$ that minimizes the \textbf{Lower Confidence Bound} (LCB):
$$
a_{\text{LCB}}(\xv) = \fh(\xv) - \tau \sh(\xv)
$$
where $\tau > 0$ is a constant that controls the \enquote{mean vs. uncertainty} trade-off\\
\vfill
The LCB is conceptually very simple and does \textbf{not} rely on distributional assumptions of the posterior predictive distribution under a SM
\end{framev}


\begin{framev}{Lower Confidence Bound}
$\tau = 1$
\vfill
\imageC[0.7]{figure_man/bayesian_loop_lcb_0.png}
\vfill
Top: Design points and SM showing $\fh(\xv)$ (blue) and $\fh(\xv) - \tau \sh(\xv)$ (red)\\
Bottom: the red point depicts $\argmin_{\xv \in \mathcal{S}} a_{\text{LCB}}(\xv)$
\end{framev}


\begin{framev}{Lower Confidence Bound}
$\tau = 5$
\vfill
\imageC[0.7]{figure_man/bayesian_loop_lcb_1.png}
\end{framev}


\begin{framev}{Lower Confidence Bound}
$\tau = 10$
\vfill
\imageC[0.7]{figure_man/bayesian_loop_lcb_2.png}
\end{framev}

\endlecture
\end{document}
