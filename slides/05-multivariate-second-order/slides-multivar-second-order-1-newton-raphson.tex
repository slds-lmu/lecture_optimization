\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/optim-basics}

\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
Second order methods
}{
Newton-Raphson
}{
figure/NR_2.png
}{
\item Newton-Raphson
\item Limitations
}

\begin{framei}[sep=L]{From first to second order methods}
\item So far: \textbf{First order methods} \\
$\Rightarrow$ \textit{Gradient} information, i.e., first derivatives 
\item Now: \textbf{Second order methods} \\
$\Rightarrow$ \textit{Hessian} information, i.e., second derivatives
\end{framei}


\begin{framei}{Newton-Raphson}
\item \textbf{Assumption:} $f \in \CC{2}$
\item \textbf{Aim:} Find stationary point $\xv^\ast$, i.e., $\nabla f(\xv^\ast) = \zero$
\item \textbf{Idea:} Find root of first order Taylor approx of $\nabla f(\xv)$:
$$
\nabla f(\xv) \approx \nabla f(\xv^{[t]}) +
\nabla^2 f(\xv^{[t]})(\xv - \xv^{[t]}) = \zero
$$
$$
\nabla^2 f(\xv^{[t]})(\xv - \xv^{[t]}) = - \nabla f(\xv^{[t]})
$$
$$
\xv^{[t+1]} = \xv^{[t]} - \left(\nabla^2 f(\xv^{[t]})\right)^{-1}\nabla f(\xv^{[t]})
$$
\item \textbf{Update scheme:}
$$
\xv^{[t+1]} = \xv^{[t]} + \mathbf{d}^{[t]}
$$
with $\mathbf{d}^{[t]} = - \left(\nabla^2 f(\xv^{[t]})\right)^{-1}\nabla f(\xv^{[t]})$
\end{framei}


\begin{framei}{Newton-Raphson}
\item \textbf{Note:} In practice, we get $\mathbf{d}^{[t]}$ by solving the linear system
$$
\nabla^2 f(\xv^{[t]})\mathbf{d}^{[t]} = - \nabla f(\xv^{[t]})
$$
with direct (matrix decompositions) or iterative methods
\vfill
\item \textbf{Relaxed/Damped Newton-Raphson:} Use step size $\alpha > 0$ with
$$
\xv^{[t+1]} = \xv^{[t]} + \alpha \mathbf{d}^{[t]}
$$
to satisfy Wolfe conditions (or just Armijo rule)
\end{framei}


\begin{framei}{Analytical example with quadratic form}
\item Consider $f(x_1, x_2) = x_1^2 + \frac{x_2^2}{2}$
\item Update direction: $\mathbf{d}^{[t]} = -\left( \nabla^2 f(x_1^{[t]}, x_2^{[t]}) \right)^{-1} \nabla f(x_1^{[t]}, x_2^{[t]})$
\item Gradient and Hessian:
$$
\nabla f(x_1, x_2) = \begin{pmatrix}2x_1 \\ x_2\end{pmatrix}, \quad
\nabla^2 f(x_1, x_2) = \begin{pmatrix}2 & 0 \\ 0 & 1\end{pmatrix}
$$
\item First step:
$$
\begin{pmatrix}
x_1^{[1]} \\ x_2^{[1]}
\end{pmatrix} = \begin{pmatrix}
x_1^{[0]} \\ x_2^{[0]}
\end{pmatrix} + \mathbf{d}^{[0]} = \begin{pmatrix}
x_1^{[0]} \\ x_2^{[0]}
\end{pmatrix} - \begin{pmatrix}
1/2 &  0 \\ 0 & 1
\end{pmatrix} \begin{pmatrix}
2x_1^{[0]} \\ x_2^{[0]}
\end{pmatrix}
$$
$$
= \begin{pmatrix}
x_1^{[0]} \\ x_2^{[0]}
\end{pmatrix} + \begin{pmatrix}
-x_1^{[0]} \\
-x_2^{[0]}
\end{pmatrix} = \zero
$$
\item \textbf{Note:} Newton-Raphson only needs one iteration for quadratic forms
\end{framei}


\begin{framev}{Newton-Raphson vs. GD on Branin function}
\splitV[0.6]{
\imageC[1]{figure/NR_2.png}
}{
\imageC[1]{figure/NR_1.png}
}
\vfill
Newton-Raphson has much better convergence speed here
\end{framev}


\begin{framei}[sep=L]{Discussion}
\item[] \textbf{Advantage:}
\item For $f$ sufficiently smooth:
\begin{framed}
\centering
Newton-Raphson converges \textit{locally} quadratically \\
(i.e., for starting points close enough to stationary point)
\end{framed}
\vfill
\item[] \textbf{Disadvantage:}
\item For \enquote{bad} starting points:
\begin{framed}
\centering
Newton-Raphson may diverge
\end{framed}
\end{framei}


\begin{framei}[fs=small]{Limitations}
\item \textbf{Problem 1:} In general, $\mathbf{d}^{[t]}$ is not a descent direction
\imageC[0.6]{figure/NR_2.png}
\item \textbf{But}: If Hessian is positive definite, $\mathbf{d}^{[t]}$ is descent direction:
$$
\nabla f(\xv^{[t]})^\top \mathbf{d}^{[t]} = - \nabla f(\xv^{[t]})^\top \left(\nabla^2 f(\xv^{[t]})\right)^{-1} \nabla f(\xv^{[t]}) < 0
$$
\item Near minimum, Hessian is positive definite
\item For initial steps, Hessian is often not positive definite and Newton-Raphson may give non-descending update directions
\end{framei}


\begin{framei}{Limitations}
\item \textbf{Problem 2:} Hessian can be \textbf{computationally expensive} to calculate, since descent direction $\mathbf{d}^{[t]}$ is the solution of the linear system
$$
\nabla^2 f(\xv^{[t]}) \mathbf{d}^{[t]} = - \nabla f(\xv^{[t]})
$$
\vfill
\item \textbf{Aim:} Find quasi-second order methods not relying on exact Hessians
\item Quasi-Newton method
\item Gauss-Newton algorithm (for least squares)
\end{framei}

\endlecture
\end{document}
