\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/optim-basics}

\title{Optimization in Machine Learning}

\begin{document} 

\titlemeta{
Second order methods
}{
Gauss-Newton
}{
figure_man/squares.png
}{
\item Least squares
\item Gauss-Newton
\item Levenberg-Marquardt
}

\begin{framei}{Least squares problem}
\item Consider the problem of minimizing a sum of squares
$$
\min_{\thetav} g(\thetav),
$$
where
$$
g(\thetav) = r(\thetav)^T r(\thetav) = \sum_{i = 1}^n r_i(\thetav)^2
$$
and
$$
\begin{aligned}
r: \R^d &\to \R^n \\
\thetav &\mapsto (r_1(\thetav), \ldots, r_n(\thetav))^T
\end{aligned}
$$
maps parameters $\thetav$ to residuals $r(\thetav)$
\end{framei}


\begin{framei}{Least squares problem}
\item \textbf{Risk minimization with squared loss} $\Lxy = \left(y - \fx\right)^2$
\item \textbf{Least squares regression:}
$$
\risket = \sumin \Lxyit = \sumin \underbrace{\left(\yi - \fxit \right)^2}_{r_i(\thetav)^2}
$$
\item $\fxit$ might be a function that is \textbf{nonlinear in $\thetav$}
\item Residuals: $r_i = y^{(i)} - f(\xv^{(i)} \,|\, \thetav)$
\vfill
\item \textbf{Example:}
\splitV[0.55]{
$$
\begin{aligned}
\D &= \left(\left(\xi, \yi\right)\right)_{i = 1, ..., 5} \\ &= \left((1,3),(2,5),(4,6),(5,13),(8,20)\right)
\end{aligned}
$$
}{
\imageC[1]{figure_man/squares.png}
}
\end{framei}


\begin{framei}{Least squares problem}
\item Suppose, we suspect an \textit{exponential} relationship between $x \in \R$ and $y$
$$
f(x \,|\, \thetav) = \theta_1 \cdot \exp(\theta_2 \cdot x), \quad \theta_1, \theta_2 \in \R
$$
\item \textbf{Residuals:}
$$
r(\thetav) = \mat{
\theta_1 \exp(\theta_2 x^{(1)}) - y^{(1)} \\
\theta_1 \exp(\theta_2 x^{(2)}) - y^{(2)} \\
\theta_1 \exp(\theta_2 x^{(3)}) - y^{(3)} \\
\theta_1 \exp(\theta_2 x^{(4)}) - y^{(4)} \\
\theta_1 \exp(\theta_2 x^{(5)}) - y^{(5)}
} = \mat{
\theta_1 \exp(1 \theta_2) - 3 \\
\theta_1 \exp(2 \theta_2) - 5 \\
\theta_1 \exp(4 \theta_2) - 6 \\
\theta_1 \exp(5 \theta_2) - 13 \\
\theta_1 \exp(8 \theta_2) - 20
}
$$
\item \textbf{Least squares problem:}
$$
\min_{\thetav} g(\thetav) = \min_{\thetav} \sum_{i=1}^5 \left(\yi - \theta_1 \exp\left(\theta_2 x^{(i)}\right)\right)^2
$$
\end{framei}


\begin{framei}{Newton-Raphson idea}
\item \textbf{Approach:} Calculate Newton-Raphson update direction by solving:
$$
\nabla^2 g(\thetat) \mathbf{d}^{[t]} = - \nabla g(\thetat).
$$
\item Gradient is calculated via chain rule
$$
\nabla g(\thetav) = \nabla (r(\thetav)^T r(\thetav)) = 2 \cdot J_r(\thetav)^T r(\thetav),
$$
where $J_r(\thetav)$ is Jacobian of $r(\thetav)$.
\item In our example:
$$
J_r(\thetav) = \mat{
\frac{\partial r_1(\thetav)}{\partial \theta_1} & \frac{\partial r_1(\thetav)}{\partial \theta_2} \\
\frac{\partial r_2(\thetav)}{\partial \theta_1} & \frac{\partial r_2(\thetav)}{\partial \theta_2} \\
\vdots & \vdots \\
\frac{\partial r_5(\thetav)}{\partial \theta_1} & \frac{\partial r_5(\thetav)}{\partial \theta_2}
} = \mat{
\exp(\theta_2 x^{(1)}) & x^{(1)} \theta_1 \exp(\theta_2 x^{(1)}) \\
\exp(\theta_2 x^{(2)}) & x^{(2)} \theta_1 \exp(\theta_2 x^{(2)})\\ \exp(\theta_2 x^{(3)}) & x^{(3)} \theta_1 \exp(\theta_2 x^{(3)}) \\
\exp(\theta_2 x^{(4)}) & x^{(4)} \theta_1 \exp(\theta_2 x^{(4)}) \\ \exp(\theta_2 x^{(5)}) & x^{(5)} \theta_1 \exp(\theta_2 x^{(5)})
}
$$
\end{framei}


\begin{framei}{Newton-Raphson idea}
\item Hessian of $g$, $\H_g = (H_{jk})_{jk}$, is obtained via product rule:
$$
H_{jk} = 2 \sum_{i=1}^n \left(\frac{\partial r_i}{\partial \theta_j}\frac{\partial r_i}{\partial \theta_k} + r_i \frac{\partial^2 r_i}{\partial \theta_j \partial \theta_k}\right)
$$
\vfill
\item \textbf{But:}
\begin{center}
\textbf{Main problem with Newton-Raphson:}
Second derivatives can be computationally expensive.
\end{center}
\end{framei}


\begin{framei}{Gauss-Newton for least squares}
\item Gauss-Newton approximates $\H_g$ by dropping its second order part:
$$
\begin{aligned}
H_{jk} &= 2 \sum_{i=1}^n \left(\frac{\partial r_i}{\partial \theta_j}\frac{\partial r_i}{\partial \theta_k} + r_i \frac{\partial^2 r_i}{\partial \theta_j \partial \theta_k}\right) \\
&\approx  2 \sum_{i=1}^n \frac{\partial r_i}{\partial \theta_j}\frac{\partial r_i}{\partial \theta_k} \\
&= 2 J_r(\thetav)^T J_r(\thetav)
\end{aligned}
$$
\item \textbf{Note}: We assume that
$$
\left|\frac{\partial r_i}{\partial \theta_j}\frac{\partial r_i}{\partial \theta_k}\right| \gg \left|r_i \frac{\partial^2 r_i}{\partial \theta_j \partial \theta_k}\right|.
$$
\item This assumption may be valid if:
Residuals $r_i$ are small in magnitude \underline{\textbf{or}} functions are only \enquote{mildly} nonlinear s.t. $\frac{\partial^2 r_i}{\partial \theta_j \partial \theta_k}$ is small
\end{framei}


\begin{framei}{Gauss-Newton for least squares}
\item If $J_r(\thetav)^T J_r(\thetav)$ is invertible, Gauss-Newton update direction is
$$
\begin{aligned}
\mathbf{d}^{[t]} &= - \left[\nabla^2 g(\thetat)\right]^{-1} \nabla g(\thetat) \\
&\approx - \left[J_r(\thetat)^T J_r(\thetat)\right]^{-1} J_r(\thetat)^T r(\thetav) \\
&= - (J_r^T J_r)^{-1} J_r^T r(\thetav)
\end{aligned}
$$
\vfill
\item \textbf{Advantage}:
\begin{center}
Reduced computational complexity since no Hessian necessary.
\end{center}
\vfill
\item \textbf{Note:} Gauss-Newton can also be derived by starting with
$$
r(\thetav) \approx r(\thetat) + J_r(\thetat)^T (\thetav - \thetat) = \tilde{r}(\thetav)
$$
and $\tilde{g}(\thetav) = \tilde{r}(\thetav)^T \tilde{r}(\thetav)$.
Then, set $\nabla \tilde{g}(\thetav)$ to zero
\end{framei}


\begin{framei}{Levenberg-Marquardt algorithm}
\item \textbf{Problem:} Gauss-Newton may not decrease $g$ in every iteration but may diverge, especially if starting point is far from minimum
\item \textbf{Solution:} Choose step size $\alpha > 0$ s.t.
$$
\xv^{[t+1]} = \xv^{[t]} + \alpha \mathbf{d}^{[t]}
$$
decreases $g$ (e.g., by satisfying Wolfe conditions)
\item However, if $\alpha$ gets too small, an \textbf{alternative} method is the
\begin{center}
\textbf{Levenberg-Marquardt algorithm}
$$
(J_r^T J_r + \lambda D) \mathbf{d}^{[t]} = -J_r^T r(\thetav)
$$
\end{center}
\item $D$ is a positive diagonal matrix
\item $\lambda = \lambda^{[t]} > 0$ is the \emph{Marquardt parameter} and chosen at each step
\end{framei}


\begin{framei}{Levenberg-Marquardt algorithm}
\item \textbf{Interpretation:} Levenberg-Marquardt \emph{rotates} Gauss-Newton update directions towards direction of \emph{steepest descent}
\item Let $D = I$ for simplicity. Then:
$$
\begin{aligned}
\lambda \mathbf{d}^{[t]} &= \lambda (J_r^T J_r + \lambda I)^{-1} (- J_r^T r(\thetav)) \\
&= (I - J_r^T J_r / \lambda + (J_r^T J_r)^2 / \lambda^2 \mp \cdots) (- J_r^T r(\thetav)) \\
&\to - J_r^T r(\thetav) = - \nabla g(\thetav) / 2
\end{aligned}
$$
for $\lambda \to \infty$
\item \textbf{Note:} $(\A + \mathbf{B})^{-1} = \sum_{k=0}^\infty (- \A^{-1} \mathbf{B})^k \A^{-1}$ if $\|\A^{-1} \mathbf{B}\| < 1$
\item Therefore: $\mathbf{d}^{[t]}$ approaches direction of negative gradient of $g$
\item Often: $D = \text{diag}(J_r^T J_r)$ to get scale invariance (\textbf{Recall:} $J_r^T J_r$ is positive semi-definite $\Rightarrow$ non-negative diagonal)
\end{framei}

\endlecture
\end{document}
