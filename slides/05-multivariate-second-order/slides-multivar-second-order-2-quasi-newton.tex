\documentclass[11pt,compress,t,notes=noshow, xcolor=table]{beamer}

\input{../../style/preamble}
\input{../../latex-math/basic-math}
\input{../../latex-math/basic-ml}
\input{../../latex-math/optim-basics}


\title{Optimization in Machine Learning}

\begin{document}

\titlemeta{
Second order methods
}{
Quasi-Newton
}{
figure_man/NR_2.png
}{
\item Newton-Raphson vs. Quasi-Newton
\item SR1
\item BFGS
}

\begin{framei}[fs=small]{Quasi-Newton: Idea}
\item Start point of \textbf{QN method} is (as with NR) a Taylor approximation of the gradient, except that H is replaced by a \textbf{pd} matrix $\A^{[t]}$:
\begin{alignat*}{4}
\nabla f(\xv) &\approx  \nabla f(\xv^{[t]}) + \nabla^2 f(\xv^{[t]}) (\xv - \xv^{[t]}) ~ &=& ~\zero  &\qquad& \text{ NR} \\
\nabla f(\xv) &\approx \nabla f(\xv^{[t]}) + \A^{[t]} (\xv - \xv^{[t]}) ~ &=& ~ \zero &\qquad& \text{ QN}
\end{alignat*}
\item The update direction:
\begin{alignat*}{3}
\bm{d}^{[t]} &= - \nabla^2 f(\xv^{[t]})^{-1} \nabla f(\xv^{[t]}) &\qquad& \text{ NR} \\
\bm{d}^{[t]} &= - (\A^{[t]})^{-1} \nabla f(\xv^{[t]}) &\qquad& \text{ QN}
\end{alignat*}
\end{framei}


\begin{framei}{Quasi-Newton: Idea}
\item Select a starting point $\xv^{[0]}$ and initialize pd matrix $\A^{[0]}$ (can also be a diagonal matrix - a very rough approximation of Hessian)
\item Calculate update direction by solving
$$
\A^{[t]} \bm{d}^{[t]} = - \nabla f(\xv^{[t]})
$$
and set $\xv^{[t+1]} = \xv^{[t]} + \alpha^{[t]} \bm{d}^{[t]}$ (Step size through backtracking)
\item Calculate an efficient update $\A^{[t+1]}$, based on $\xv^{[t]}$, $\xv^{[t+1]}$, $\nabla f(\xv^{[t]})$, $\nabla f(\xv^{[t+1]})$ and $\A^{[t]}$
\end{framei}


\begin{framei}{Quasi-Newton: Idea}
\item Usually the matrices $\A^{[t]}$ are calculated recursively by performing an additive update
$$
\A^{[t+1]} = \A^{[t]} + \bm{B}^{[t]}
$$
\item How $\bm{B}^{[t]}$ is constructed is shown on the next slides
\vfill
\item \textbf{Requirements} for the matrix sequence $\A^{[t]}$:
\begin{itemizeS}
\item Symmetric pd, so that $\bm{d}^{[t]}$ are descent directions
\item Low computational effort when solving LES
$$
\A^{[t]} \bm{d}^{[t]} = - \nabla f(\xv^{[t]})
$$
\item Good approximation of Hessian: The ``modified'' Taylor series for $\nabla f(\xv)$ (especially for $t \to \infty$) should provide a good approximation
$$
\nabla f(\xv) \approx \nabla f(\xv^{[t]}) + \A^{[t]}(\xv - \xv^{[t]})
$$
\end{itemizeS}
\end{framei}


\begin{framei}{Symmetric rank 1 update (SR1)}
\item Simplest approach: symmetric rank 1 updates (\textbf{SR1}) of form
$$
\A^{[t+1]} \leftarrow \A^{[t]} + \bm{B}^{[t]} = \A^{[t]} + \beta \bm{u}^{[t]}(\bm{u}^{[t]})^T
$$
with appropriate vector $\bm{u}^{[t]} \in \R^n$, $\beta \in \R$
\end{framei}


\begin{framei}[fs=small]{Symmetric rank 1 update (SR1)}
\item \textbf{Choice of $\bm{u}^{[t]}$:} Vectors should be chosen so that the ``modified'' Taylor series corresponds to the gradient:
$$
\begin{aligned}
\nabla f(\xv) &\overset{!}{=} \nabla f(\xv^{[t+1]}) + \A^{[t+1]}(\xv - \xv^{[t+1]}) \\
\nabla f(\xv) &= \nabla f(\xv^{[t+1]}) +  \left(\A^{[t]} + \beta \bm{u}^{[t]}(\bm{u}^{[t]})^T\right)\underbrace{(\xv - \xv^{[t+1]})}_{:= \bm{s}^{[t+1]}} \\
\underbrace{\nabla f(\xv) - \nabla f(\xv^{[t+1]})}_{\bm{y}^{[t+1]}} &= \left(\A^{[t]} + \beta \bm{u}^{[t]} (\bm{u}^{[t]})^T\right) \bm{s}^{[t+1]} \\
\bm{y}^{[t+1]} - \A^{[t]} \bm{s}^{[t+1]} &= \left(\beta (\bm{u}^{[t]})^T \bm{s}^{[t+1]}\right) \bm{u}^{[t]}
\end{aligned}
$$
\item For $\bm{u}^{[t]} = \bm{y}^{[t+1]} - \A^{[t]} \bm{s}^{[t+1]}$ and $\beta = \frac{1}{(\bm{y}^{[t+1]} - \A^{[t]}\bm{s}^{[t+1]})^T\bm{s}^{[t+1]}}$ the equation is satisfied
\end{framei}


\begin{framei}{Symmetric rank 1 update (SR1)}
\item[] \textbf{Advantage}
\begin{itemizeS}
\item Provides a sequence of \textbf{symmetric pd} matrices
\item Matrices can be inverted efficiently and stable using Sherman-Morrison:
$$
(\A + \beta \bm{u}\bm{u}^T)^{-1} = \A + \beta \frac{\bm{u}\bm{u}^T}{1 + \beta\bm{u}^T\bm{u}}
$$
\end{itemizeS}
\vfill
\item[] \textbf{Disadvantage}
\begin{itemizeS}
\item The constructed matrices are not necessarily pd, and the update directions $\bm{d}^{[t]}$ are therefore not necessarily descent directions
\end{itemizeS}
\end{framei}


\begin{framei}{BFGS algorithm}
\item Instead of Rank 1 updates, the \textbf{BFGS} procedure (published simultaneously in 1970 by Broyden, Fletcher, Goldfarb and Shanno) uses rank 2 modifications of the form
$$
\A^{[t]} + \beta_1 \bm{u}^{[t]}(\bm{u}^{[t]})^T + \beta_2 \bm{v}^{[t]}(\bm{v}^{[t]})^T
$$
with $\bm{s}^{[t]} := \xv^{[t+1]} - \xv^{[t]}$
\item $\bm{u}^{[t]} = \nabla f(\xv^{[t+1]}) - \nabla f(\xv^{[t]})$
\item $\bm{v}^{[t]} = \A^{[t]} \bm{s}^{[t]}$
\item $\beta_1 = \frac{1}{(\bm{u}^{[t]})^T (\bm{s}^{[t]})}$
\item $\beta_2 = - \frac{1}{(\bm{s}^{[t]})^T \A^{[t]} \bm{s}^{[t]}}$
\vfill
\item The resulting matrices $\A^{[t]}$ are positive definite and the corresponding quasi-Newton update directions $\bm{d}^{[t]}$ are actual descent directions
\end{framei}

\endlecture
\end{document}
